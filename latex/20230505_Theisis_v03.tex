\documentclass[12pt,a4paper,english]{article} % document type and language

\usepackage{layout}
\usepackage{babel}   % multi-language support
\usepackage{float}   % floats
\usepackage{url}     % urls
\usepackage{graphicx}
\usepackage{amsmath} % matrix algebra
\usepackage{multirow} % tables
\usepackage{booktabs} % tables
\usepackage{blindtext}
\usepackage{geometry}
\usepackage[parfill]{parskip} % no indent
\usepackage{amssymb}

% graphics path
\graphicspath{{./graphs/}}	% graphics


% word count

% title slide
\author{Lukas Gahl}
\title{\textbf{\huge Memoire }\\}
\date{\today} 

% user commands
\newcommand{\matr}[1]{\mathbf{#1}} % matrix font
\newcommand{\Lagr}{\mathcal{L}} % likelihood L
\newcommand{\E}{\mathbb{E}} % expectation
% RBC expressions
\newcommand{\Rss}{\frac{1}{\beta} + \delta - 1}
\newcommand{\Ass}{\bar{A}}
\newcommand{\KLss}{\left[ \frac{\alpha \Ass}{\Rss} \right]^{\frac{1}{1-\alpha}}}

% set layout
\let\oldsection\section
\renewcommand\section{\clearpage\oldsection}

\geometry{
	a4paper,
	total={170mm,257mm},
	left=25mm,
	right=30mm,
	top=25mm,
	bottom=25mm,
}
\linespread{1.5}

% bibliography
\bibliographystyle{apalike}

\begin{document}
	
	
	\maketitle
	\pagebreak

	\section*{Abstract}
	\pagebreak

	
	\tableofcontents
	\pagebreak
	
	
	 \section{Introduction}
	 
	 High correlation of hours worked and real wage in the RBC model, an fact that is not matched by reality. The aim of RBC research thus must be finding new kinds of shocks, that allow for a more realistic representation of labour supply ()Christiano Eichenbaum 1991).
	 
	 One such 'new' shock is the the inclusion of petrol as an exogenous shock series \cite{kim_role_1992}
	 
	 
	- The method of solving and approximating the model has significant impact on simulated data, hence is relevant when relating models to real data (Taylor  Uhlig, 1990)
	
	This work is joining 3 strains of literature
	- Literature on the RBC, the origin of all new keynesian models
	- Literature on bayesian estimation of structural macroeconomics models
	- Literature on energy price shocks and their role in the 
		- This refers to the nature of the shock
		- Shocks as theoretical origin on business cycles are at the origine of this research, 
		identifying suitable shocks that explain more thus adds to the literature on business cycles
	
	The Lucas 1976 critique: 
	
	Intertamperol evoluation of macreoconomic variables
	
	Kydland and Prescott (1982) introduced the RBC \\
	- became main model of macroeconomy
	- rational expectation model
	
	The New Keynesian model extends the RBC by inflation
	- 
	
	Rational expectation models build on two corner stones Chair et al
	- Structural parameters which are unaffected by policy changes
	- shocks that have a relevant economic interpretation
	
	Kocherlakota (2007) adjustment principle
	- the better a model fits to data the better it is for policy advise
	- in assessing this fit one needs to be wary of over fitting specific data
	
	\section{Literature Review}
		RBC Criticism
	the correlation of real wage and hours worked does not correspond to reality, this is the lacmus test of RBC models (Christiano Eichenbaum 1992)
	"Robert E. Lucas (1981 p. 226) says that "observed real wages are not constant over the cycle, but neither do they exhibit consistent pro- or countercyclical tendencies. This suggests that any attempt to assign systematic real wage movements a central role in an explanation of business cycles is doomed to failure" \cite{christiano_current_1992}
	
	NK models are suitable to comparing alternative policy regimes without being subject to the Lucas (1976) critique (Gali)	



	\section{DSGE Models}
	
	Before diving into the derivation of each individual model this section provides some general remarks applicable to all models across this work. All models discussed in the below have analytical solutions to their steady state, in which the economy is at its natural optimum. However, the system describing the economy is non-linear due to the multiplicative production relationship arising from the Cobb-Douglas production function \cite{campbell_inspecting_1994}. This gives rise to the need of some for of linear approximation around the steady state in order to be able to analyse the system's behaviour. The most common approach adopted across literature is a log-linear Taylor approximation, this work will follow suit \cite{campbell_inspecting_1994}. Such linearisation is applicable under the existence of a deterministic singular solution, which is the case for the small scale DSGE models discussed in the below \cite{herbst_bayesian_2016}.
	
	The theoretical benchmark model a classic RBC 
	The aim of analysing business cycles
	
	The micro foundation, instead of clear behavioural rules agents optimise their behaviour according to utility optimisation
	
	Conceptual ideas:
	The efficiency of business cycles (Gali, 2008)
	in a world of perfect competition and lack of nominal rigidities business cycles might be efficient
	they are the response to changes in productivity, and are actually the result of a "correcting" force towards an efficient equilibrium
	this raises questions on the desirability of policy interventions
	This goes against (Keynes, 1936) who regarded recession as inefficient, due to under-utilisation of resources
	
	Technology shocks
	Technological shocks for a correctly calibrated model allowed to simulate cycles similar to actual business cycles in first and second order moment (ref). This shed new light on the assumption that technology was solely a driver of long-term growth with neglible impact on business cycles (Gali, 2008, p.3).
	
	RBCs succesfully abstraced from monetary rigidities in their explanation of business cycles
	
	The assumptions of the RBC and their consequences, namely the non-existance of money and monetary rigidities in the economy are greatly contratsted by emperical evidence (Gali, 2008, p 15)
	
	Assumptions
	- perfect compitions
	- flexible prices
	- technology shocks
	- identical technology across firms
	
	
	\subsection{Real Business Cycle model}
	The Real Business Cycle model (RBC) was first introduced by \cite{prescott_theory_1986} in the aim of providing an explanation to business cycles. Departing from Schumpeterian growth driven by advances in technology the RBC model is at its heart a de-trended growth model. It emphasis the role of technology growth and resulting changes in total factor productivity in influencing business cycles. In doing so it makes the following assumptions:
	\begin{itemize}
		\item perfect competition
		\item flexible prices
		\item technology shocks
		\item identical technology across firms
	\end{itemize}
	This leads to the interpretation of business cycles as efficient adjustments of economy towards a new equilibrium. While this idea is antithetical to the New-Keynesian literature considering business cycles as inefficient both models share a the mathematical foundation. The following will therefore derive the RBC model
	
	\subsubsection{The Household}
	The households in the RBC model are infinitely lived agents with homogeneous preferences. They simultaneously solve the same threefold optimisation problem. Faced with an income consisting of labour wage, returns on last periods capital and dividends from firms. The income is divided between consumption, capital investment and leisure. The leisure expenditure is implicitly made by adjusting the hours of work, factoring negatively into household utility. Dividends are drawing from the fact that in the RBC firms are owned by the consumer. However, the assumption of full price flexibility and perfect competition implies zero firm profits at equilibrium, dividend incomes are thus null in the steady state. The following will assume a separable utility function of the form: 
	\begin{equation}
		U(C_t, L_t) = \frac{C_t^{1-\sigma}}{{1-\sigma}} - \frac{L_t^{1+\phi}}{1+\phi} 
	\end{equation}
% check \sigma claim
	In this specification $\sigma$ is a coefficient of risk aversion and $\phi$ is the marginal disutility of labour. While other utility specification have been employed across the RBC literature this work will remain with a separable utility function in order to be consistent with the New-Keynesian literature. The function $U(C_t, L_t)$ exhibits the usual characteristics of well behaved utility function $\frac{\partial U}{\partial C_t} > 0$, $\frac{\partial U}{\partial L_t} \leq 0$ and $\frac{\partial^2 U}{\partial C_t^2} \leq 0$, $\frac{\partial^2 U}{\partial L_t^2} \leq 0$.
	The household maximisation problem then reads as follows
	\begin{equation}
		\begin{aligned}
			\max_{B_t, C_t, L_t} \quad \sum_{\infty}^{t=0} \beta^{t} U(C_{t}, L_{t}) \\
			\textrm{s.t.} \quad C_t + I_t = R_t K_{t-1} + W_t N_t + \Pi_t \\
						  K_t = (1 - delta) K_{t-1} + I_t \\
			\textrm{FOC:} \\
			- \frac{U_{n,t}}{U_{c, t}} = W_t \\
			\E_t \left[ \frac{U_{c,t}}{U_{c,t+1}} \right]^{\sigma} = \beta \left[ 1 - \delta + \E_t[R_{t+1}]\right]
		\end{aligned}
	\end{equation}
	The problem yields two main equations. The Euler equation of intertemporal consumption trade-off and the labour supply equation. 
	\begin{equation}
		\begin{aligned}
			C_t^\sigma L_t^\phi	= W_t \\
			\E_t \left[ \frac{C_{t+1}}{C_t} \right]^\sigma = \beta \left[ 1 - \delta  + \E_t [R_{t+1}] \right] 			
		\end{aligned}
	\end{equation}
	
	\subsubsection{The firm}
	Firms in the RBC are also assumed to be homogenous, producing with the same production technology. As such they solve the same static optimisation problem across its production inputs capital and labour. Due to the assumption of perfect competition and flexible prices firms pay the marginal product of labour and capital to its respective inputs. This implies a zero-profit condition, as mentioned in the above. The two first order conditions thus correspond to capital and labour demand.
	\begin{equation}
		\begin{aligned}
			\max_{N_t} \quad & A_t K_t^\alpha N_t^{1 - \alpha} \\
			\textrm{s.t.} \quad & Y_t = W_t N_t + K_t R_t\\
			\textrm{FOC:} \\
			W_t = (1 - \alpha) A_t K_t^\alpha N_t^{-\alpha}\\
			K_t = \alpha A_t K_t^{\alpha -1} N_t^{1-\alpha}
		\end{aligned}
	\end{equation}

	\subsubsection{Equilibrium}
	Having derived the above optimality condition an analysis of the equilibrium requires two further equations. These are the equilibrium resource constraint arising from the output accounting identity and the exogenous law of motion of technology.
	\begin{equation}
		\begin{aligned}
			Y_t = C_t + I_t \\
			\log(A_t) = (1- \rho_A) \log(A^*) + \rho_A \log(A_{t-1}) + \epsilon_t^A
		\end{aligned}
	\end{equation}
	As the case with all DSGE models the system in its current state is non-linear. This is due to the from multiplicative Cobb-Douglas production and additive law of motion without full depreciation $\delta \leq 1$ \cite{campbell_inspecting_1994}. This raises the need for log-linear approximations which yields the following system of equations. \\
	
	\begin{tabular}{lr}
		\textbf{Equation name} & \textbf{Log-linear expression}\\
		\hline
		Euler equation & $\hat{c}_t = \hat{c}_{t+1} - \hat{r}_{t+1}$ \\ % since \beta = 1 and (\bar{r} + 1 - \delta) = 1 as well
		Capital demand & $\hat{y}_t - \hat{k}_t = \hat{r}_t$ \\
		Capital supply &  $\hat{i}_t = \delta \hat{k}_{t+1} - \frac{1-\delta}{\delta} \hat{k}_t$ \\
		Labour supply & $\hat{c}_t = \hat{w}_t - \frac{\bar{l}}{1-\bar{l}} \hat{l}_t$ \\
		Labour demand & $\hat{y}_t - \hat{l}_t = \hat{w}_t$ \\
		Production function & $\hat{y}_t = \hat{a}_t + \alpha \hat{k}_t + (1-\alpha) \hat{l}_t$ \\
		Equilibrium condition & $\hat{y}_t = \frac{\bar{y}}{\bar{c}} \hat{c}_t + \frac{\bar{i}}{\bar{y}} \hat{i}_t$ \\
		Technology law of motion & $\hat{a}_t = \rho_a \hat{a}_{t-1} + \epsilon_{a,t}$ \\
	\end{tabular}\\
	
	The above linearisation still contains values of the steady state indicated by the $\bar{x}$ notation. In order to express the model as a linear system these have to be replaced with their respective deterministic values. However, the RBC model under separable consumption labour utility, does not possess a deterministic steady state for labour (ref). This issues has often been circumvented by relying on a logarithmic specification of household utility \cite{campbell_inspecting_1994} \footnote{In the work of Campbell the analytical derivation relies on a log-utility function}. However, the purpose of this work is a comparison between an RBC and New-Keynesian models. The latter traditionally rely on separable utility and so will this work will in order to allow for a fair comparison of results.
	The usual solution path for an RBC with separable utility is the computation of the steady state as ratios of labour. These 'great ratios' are deterministic and allow for some analysis \cite{prescott_theory_1986}.\\
	
	\begin{tabular}{lr}
		\textbf{Variable} & \textbf{Deterministic steady state}\\
		\hline 
		Rate of return & $\bar{R} = \Rss$ \\
		Capital labour ratio & $\frac{\bar{K}}{\bar{L}} = \KLss$ \\
		Wage & $\bar{W} = (1 - \alpha) \Ass \left(\KLss\right)^\alpha$ \\
		Investment labour ratio & $\frac{\bar{I}}{\bar{L}} = \delta \KLss$ \\
		Output labour ratio & $\frac{\bar{Y}}{\bar{L}} = \Ass \left(\KLss\right)^\alpha$ \\
		Consumption labour ratio & $\frac{\bar{C}}{\bar{L}} = \frac{\bar{Y}}{\bar{L}} - \frac{\bar{I}}{\bar{L}}$ \\
	\end{tabular}\\

	Obtaining a linear state-space representation of the RBC model requires the numerical approximation of the steady state value of labour $\bar{l}$. This work relies on the software package gEconpy (ref) for this numerical approximation. The specification of the input file, its steady-state values and the state-space transition matrix can be found in the (appendix). 

	
	
	
	
	

	
	

	
	
	\subsection{NK}
	The New-Keynesian model developed in this section is based on the work of \cite{gali_monetary_2008}. It diverges from the RBC model in three key aspects. Firstly, he NK model in its simplest form analyses an economy without capital, wherefore its intertemporal  instrument is the output gap instead of the capital stock (ref). Secondly, the NK model introduces nominal rigidities through monopolistic competition. Since firms are endowed with some market power, they can reset prices above marginal cost, introducing a gap between real and nominal marginal cost. This dynamic results in inflation. This extension has several consequences. For one monetary policy is neutral in the short-run, thus changes in interest are note directly matched by changes in expected inflation (ref). This gap in expectations becomes the driver of short-run fluctuation, or business cycles in the economy. The main driver of business cycles is thus no-longer technological progress through a change in total factor productivity. Instead an adjustment or shock to the interest rates drives business cycles. \\
	This of course has important economic implications. Instead of efficient direct adjustments of the economy to new production technology business cycles no longer exhibit efficiency in the NK model. Instead, they are an inefficient divergence caused by central bank interventions. This is also the reason why the model is called New-Keynesian. The model returns Keyne's perspective, regarding regarded business cycles as inefficient. The the main divergence from RBC literature consists in this change of perspective.
		
	\subsubsection{Consumption bundle}
	The New-Keynesian framework introduces monopolistic competition in order to equip firms with price setting power (ref). The market power arises from the fact that goods are not perfect substitutes. In this setting, the simplified assumption of a unique good of the RBC holds no longer and the households allocation problem becomes a two stage process. First, the household needs to decide on the basket of products that optimises its utility. Only then the household is able to solve for the optimal allocation of income in the consumption leisure trade-off. This requires the consumption basket composition problem to be solved prior prior to the consumption-leisure trade-off. Across the DSGE literature the aggregation of consumption is also sometimes referred to as the output of a good bundling firm. This intermediary party determines the optimal basket of goods according to the household preferences. In a perfectly competitive environment the household buys the optimal bundle at no mark up beyond the producer's mark-up.
	Determining the optimal basket requires some function of aggregation across the continuum of goods. The most commonly employed aggregator is hte Dixit-Stiglitz function. This function has the appealing property of a constant elasticity of substitution (CES). This is important later in deriving marginal cost and its equilibrium value. \\
	The optimisation problem reads as follows where each good $C_{it}$ is matched by its price $P_{it}$ purchased with budget $Z_t$.
	\begin{equation}
		\begin{aligned}
			\max_{C_{it}}
			\quad \left(\int_{0}^{1} C_{it}^{ \frac{\epsilon - 1}{\epsilon} } di 
			\right)^{ \frac{\epsilon}{\epsilon - 1} }\\
			\textrm{s.t.}\\
			\quad \int_{0}^{1} P_{it} C_{it} di \leq Z_t
		\end{aligned}
	\end{equation}
	Solving the above optimisation problem results in three important expression. First, the aggregated price level $P_t$ of the optimal consumption bundle is derived. This corresponds to the consumer price index (CPI).
	\begin{equation} \label{eq:1}
			P_t = \left(
						\int_{0}^{1} P_{it}^{ 1 - \epsilon } di 
					\right)^{ \frac{1}{1 - \epsilon} }
	\end{equation}
	Further manipulation leads to the share of product $C_{it}$ in the consumption basket $C_t$. This share is defined by the goods price relative to the price level $P_t$ and the CES between goods $\epsilon$.
	\begin{equation} \label{eq: cshare}
		C_{it} = \left( \frac{P_{it}}{P_t} \right)^{- \epsilon} C_t
	\end{equation}
	Lastly, using the two above it can be demonstrated that under a binding budget constraint individual goods prices correspond to the CPI times the consumption bundle. This is the case as goods are weighted according to their relative price. This property is important as it allows to solve the household and firm optimisation problem as a representative agent problem.
	\begin{equation}
		\int_{0}^{1} P_{it} C_{it} di = Z_t = P_t C_t
	\end{equation}

	\subsubsection{Household}
	Having found $C_t$ and its price $P_t$ the household's consumption-leisure allocation can be formulated. It follows straightforwardly from the RBC model with one extension, bond holdings. While zero in equilibrium bond holdings introduces the interest rate to the households problem. This is an important extension in order to allow for a monetary authority. The interest rate is contained in the discount factor $Q_t = \frac{1}{1+i_t}$.
	Similar to the RBC households are infinitely lived solving this problem for all future periods. 
% define utility functional form
	\begin{equation}
		\begin{aligned}
			\max_{B_t, C_t, L_t} \quad \sum_{\infty}^{t=0} \beta^{t} U(C_{t}, L_{t}) \\
			\quad P_t C_t + Q_t B_t = B_{t-1} + W_t N_t \\
			\textrm{FOC:} \\
				- \frac{U_{n,t}}{U_{c, t}} = \frac{W_t}{P_t}\\
				Q_t = \beta \E_t \left[ \frac{U_{c, t+1}}{U_{c,t}}^\sigma \frac{P_t}{P_{t+1}} \right] 			
		\end{aligned}
	\end{equation}
	Solving the optimisation problems yields the labour supply, this time explicitly depending on the real wage and the Euler equation. Assuming the function form of $		U(C_t, L_t) = \frac{C_t^{1-\sigma}}{{1-\sigma}} - \frac{L_t^{1+\phi}}{1+\phi}$ and plugging its derivatives into the first order condition yields.
		\begin{equation}
		\begin{aligned}
			C_t^\sigma L_t^\phi	= \frac{W_t}{P_t} \\
			\E_t \left[ \frac{C_{t+1}}{C_t} \right]^\sigma = \beta ( 1 - \delta  + \E_t [R_{t+1}] )
		\end{aligned}
	\end{equation}
		
	\subsubsection{Price setting}
	The main innovation of the NK model lies in modelling inflation. As mentioned in the above inflation arises from the imperfect substitutability consumption goods. This results in monopolistic competition endowing firm's price setting power. Modelling the price setting process is thus at the heart of of the NK model. In doing so the standard model relies on the Calvo-pricing mechanism. Firms are able to reset prices in any period with the probability $\theta$. This is equal to saying that every period a share $\theta$ of firms can reset prices to their optimal price $\hat{P}_t$ while $1-\theta$ firms have to remain with last periods price $P_{t-1}$. This allows to describe the evolution of the price level as an in the following intertemporal equation, which if divided by $P_{t-1}$ yields an expression of inflation.
	\begin{equation}
		P_t = 
		\left[
		\theta P_{t-1}^{1 - \epsilon} + (1 - \theta) \hat{P}_t^{1 - \epsilon}
		\right]^{\frac{1}{1 - \epsilon}}
	\end{equation}

	\begin{equation} \label{eq: inlfation}
		\Pi_t^{1-\epsilon} = \theta + (1 - \theta) \left(\frac{\hat{P}_t}{P_{t-1}} \right)^{1-\epsilon}
	\end{equation}

	The key aspect to price evolution thus lies in the optimal price $\hat{P_t}$. The further it is away from last periods price, the higher is inflation. \\
	
	\subsubsection{The firm}
	In the RBC the firm solved a static optimisation problem providing labour and capital demand. In the NK model they are instead faced with setting the optimal price $\hat{P}_t$. For this the NK model assumes that a continuum of firms each producing one good provide factoring into the continuum of goods as outlined in (section). Setting the optimal price becomes an intertemporal problem as firms trade-off between two opposing forces. 	
	As illustrated by (ref) the share of any good in the overall basket depends on its price relative to the CPI. This implies that setting a price far above $P_t$ leads to a lower demand for a firm's single good $C_{i,t}$. 
	On the other hand, as firms are limited in their price frequency they need to set a price which they would be comfortable with in the future. Under the Calvo-scheme a price once set will be unchanged with probability $\theta^k$ for $k$ periods. Consequently, if the price once set is too low it will at some point allow for no further profits potentially even incur losses. 
	Firms thus try to set a price that allows them an acceptable share in the basket of goods while also guaranteeing profitability across future periods. In this decision the size of $\theta \in [0,1]$ combined with the time discount factor $Q_t \beta$ weights the second force against the first. 
	The problem can be formalised as a future discounted sum of revenue minus total cost. The notation of $Y_{t+k|t}$ hereby refers to the income in period $t+k$ at price of period $t$. The notation of $Q_{t,t+k}$ is the stochastic discount factor of today's discount carried forward into period $t+k$. The budget constraint makes use of the fact that under equilibrium assumption all output is consumed as part of the consumption basket $Y_{it} = C_{it}$. (section) will deliver a more in-depth perspective on this.	
	\begin{equation}
		\begin{aligned}
			\max_{\hat{P}_t}
			\quad
			\sum_{k=0}^{\inf} \theta^k \E_t 
			\left[
			Q_{t, t+k} 
			\left(
			\hat{P}_t Y_{t+k|t} - TC_{t+k|t}^n(Y_{t+k|t})
			\right)
			\right] \\
			\textrm{s.t.}
			\quad
			Y_{it+k|t} = \left(\frac{\hat{P}_t}{P_{t+k}} \right)^{-\epsilon} C_{t+k} \\
			\textrm{FOC:} \quad
			\sum_{k=0}^{\infty} \theta^k \E_t 
			\left[
			Q_{t,t+k} Y_{t+k|t} 
			\left(
			\hat{P}_t - \frac{\epsilon}{1 - \epsilon} MC_{t+k|t}^n
			\right)
			\right]
			= 0
		\end{aligned}
	\end{equation}
	
	Inserting the budget constraint and the discount factor as defined by the Euler condition (ref) $Q_{t,t+k} = \E_t \left[ \frac{C_{t+k}}{C_t} \right]^\sigma \frac{P_t}{P_{t+1}}$ results in:
	\begin{equation}
		\frac{\hat{P}_t}{P_t} = \frac{\epsilon}{\epsilon-1} 
		\frac{
		\E_t \sum_{k=0}^{\infty} \theta^k \beta^k C_{t+k}^{1-\sigma} (\frac{P_{t+k}}{P_t})^\epsilon MC_{t+k}^r
		}{
		\E_t \sum_{k=0}^{\infty} \theta^k \beta^k C_{t+k}^{1-\sigma} (\frac{P_{t+k}}{P_t})^{\epsilon-1}
		}
	\end{equation}
	The above describes the mark-up of the optimal price $\hat{P}_t$ beyond the price level CPI. The difference between the two directly drives inflation as it implies a higher price level as can be seen in \ref{eq: inflation}. 
	In the steady state inflation is zero by definition, implying that no firm resets prices. This is equivalent to saying that $\theta=0$. Multiplying the above by $P_t$ and setting $\theta=0$ results in $\hat{P}_t = \frac{\epsilon}{\epsilon-1} MC_t^n$. This allows to derive an important aspect to the NK model, the price-mark up over nominal marginal cost in the steady state. Firms are endowed with market power, allowing them to set a mark up above marginal cost. The real marginal defined as $MC_t^r \equiv \frac{MC_t^n}{P_t}$ cost can thus be derived. It is important to note that $MC^r$ are the real marginal cost in the steady state, wherefore they are time independent. They are complemented by $MC_t^r$, the time depended marginal cost, which can deviate from their steady state value leading to the gap of $\hat{MC}_t^r \equiv MC_t^r - MC^r$.
	\begin{equation}
		MC^r = \frac{\epsilon}{\epsilon-1}
	\end{equation} 
	
	
	\subsubsection{Equilibrium conditions}
	To this point a number of optimal choices have been derived. The households decides on the consumption basket and consumption-leisure trade-off. The firm sets the optimal price and consequently mark-up over nominal marginal costs, yielding an expression for inflation. Having derived the optimality conditions for all agents the model now requires market clearing condition to describe their interaction. This is based on accounting identity on individual good and production level $Y_{it} = C_{it}$. In order to close the model one needs to demonstrate, that this property on individual goods level translates in the global property $Y_t = C_t$.
	
	Output $Y_t$ is defined as an aggregator function. The accounting identity combined with \ref{eq: cshare} allows to link $Y_t$ to the the price level, allowing to demonstrate that $Y_t = C_t$ holds.
	\begin{equation}
			Y_t = 
			\left( 
				\int_{0}^{1} Y_{it}^{\frac{\epsilon - 1}{\epsilon}} di 
			\right)^{\frac{\epsilon}{\epsilon - 1}}
			=
			\left( 
			\int_{0}^{1} 
			\left[
			\left( \frac{P_{it}}{P_t} \right)^\epsilon C_t
			\right]^{\frac{\epsilon - 1}{\epsilon}} di 
			\right)^{\frac{\epsilon}{\epsilon - 1}}
			=
			P_t^{\epsilon} C_t P_t^{-\epsilon}
			=
			C_t
	\end{equation}
	A similar reasoning is required in order to derive an economy wide production function an expression from the firm's individual production functions $Y_{it} = A_t N_{it}$. This yields the below expression, which will be kept unchanged for now.
	\begin{equation} \label{eq: Nt}
		N_t = 	
		\int_{0}^{1} \left( \frac{Y_t}{N_t} \right)^{\frac{1}{1 - \alpha}}
		\int_{0}^{1} \left( \frac{P_{it}}{P_t} \right)^{-\frac{\epsilon}{1 - \alpha}} di
	\end{equation}

	\subsubsection{Log linearisation}
	
	This completes the number of equations required for the derivation of the New Keynesian model. As the above equations are a non-linear system without a closed form solution some method of linearisation is required. The below will follow the procedure of \cite{gali_monetary_2008} and apply log-linearisation first order Taylor approximations. Throughout the log-linearisation several manipulations are required. The Euler equation relies on reasoning about the non-inflationary steady state so does the expression for $N_t$. Explanation and proofs can be found in \ref{appc}. Lower-case expressions are the log-equivalent to previous upper case variables. \\

	Euler equation
	\begin{equation} \label{eq:lleuler}
		c_t = \E_t[c_{t+1}] - \frac{1}{\sigma} (i_t - \rho - \E_t [\pi_{t+1}])
	\end{equation}
	Labour supply 
	\begin{equation} \label{eq:llslabour}
		w_t - p_t = \sigma c_t + \phi n_t
	\end{equation}
	Inflation
	\begin{equation}\label{eq:llpi}
		\pi_t = (1 - \theta) (\hat{p}_t - p_{t-1})	
	\end{equation}
	Optimal price 
	\begin{equation}\label{eq:llpstar}
		\hat{p}_t = (1 - \theta \beta) \E_t
		\left[
		\sum_{k=0}^{\infty} \theta^k \beta^k \left( mc_{t+k|t}^r - mc^r +p_{t+k}\right)
		\right]		
	\end{equation}
	Production function
	\begin{equation}\label{eq:llprod}
		y_t = a_t + (1 - \alpha) n_t
	\end{equation}
	Equilibrium condition 
	\begin{equation}\label{eq:lleq}
		y_t = c_t
	\end{equation}

	\subsubsection{New Keynesian Phillips and IS curve}
	
	The next step is the derivation of an expression for real marginal cost $mr_{t}^r$ contained in the optimal price setting. In the absence of other production factors the marginal product of labour $MPN_t = \frac{\partial Y}{\partial N} = A_t (1- \alpha) N_t^{-\alpha}$ corresponds to nominal marginal cost $mc_t^n$. Real marginal cost can thus be obtained from $MC_t^r = \frac{W_t}{P_t MCN_t}$. This allows to formulate the below property in logs by including the production function \ref{eq:llprod}.
	\begin{equation}
		\begin{aligned}
			mc_t^r = w_t - p_t - a_t - mpn_t \\
%			mc_t^r = w_t - p_t - a_t - \ln(1 - \alpha) + \alpha n_t \\
			mc_t^r = w_t - p_t - \frac{a_t - \alpha y_t}{1 - \alpha} - \ln(1 - \alpha) \\
		\end{aligned}
	\end{equation}
	This expression is at the provides the basis for two essential conclusions. 
	Firstly, it allows to derive a relation between $mc_t^r$ and output $y_t$. Using labour supply \ref{eq:llslabour} and the production function \ref{eq:llprod} one can rephrase the above as. 
	\begin{equation}
		\begin{aligned}
			mc_t^r = \sigma c_t + \phi n_t - [a_t + \alpha n_t + \ln(1-\alpha)] \\
			mc_t^r = 
			\frac{
				\sigma (1 - \alpha) + \phi + \alpha
			}{
				(1 - \alpha)	
			}	 y_t
			- \frac{
				(1 + \phi)	
			}{
				(1 - \alpha)	
			} a_t
			+ \ln(1-\alpha)
		\end{aligned}
	\end{equation}
	In the steady state marginal cost should be equal to its natural level and $mc_t^r = mc^r$. In this case the above describes natural output $y_t^n$. This fact can be used to derive an expression for the gap between marginal cost and its steady state value in logs $\hat{mc_t}^r \equiv = mc_t^r - mc^r$. In doing so one 
	Reformulating the above by replacing $mc_t^r$ by $mc^r$ yields an expression of the output gap in terms of the marginal cost gap. 
	\begin{equation} \label{eq:llmcrhat}
		\begin{aligned}
			\hat{mc_t}^r = \frac{\sigma (1 - \alpha) + \phi + \alpha}
			{1 - \alpha)} (y_t - y_t^n)
		\end{aligned}
	\end{equation}
	The second insight arising from the real marginal cost is its translation in period $t+k$. Combining this with the share of consumption good $i$ in the overall basket as well as the market clearing condition \ref{eq:lleq} logs gives 
% mc_{t+l}^r = w_{t+k} - p_{t+k} - \frac{a_ {t+k} - \alpha y_{i,t+k|t}}{1 - \alpha} - \ln(1 - \alpha)
% check this
	\begin{equation}
		y_{t+k|t} = -\epsilon(p_{t+k|t} - p_{t+k}) + y_{t+k}
	\end{equation}
	Combining the two yields an expression of marginal cost and the price level.
	\begin{equation}
		mc_{i, t+k|t}^r = mc_{t+k}^r - \frac{\epsilon \alpha}{1 - \alpha}(\hat{p}_t - p_{t+k})
	\end{equation}
	The above combined with the expression for the optimal price level.
% more math??	
	\begin{equation}
		\begin{aligned}
			\hat{p}_t - p_{t-1} =
			\quad
			(1 - \theta \beta) \E_t
			\left[
			\sum_{k=0}^{\infty} \theta^k \beta^k \left( mc_{t+k|t}^r - mc^r +p_{t+k} - p_{t-1}\right)
			\right] \\			
			=
			\quad
			(1 - \theta \beta) \Theta \E_t
			\sum_{k=0}^{\infty} \theta^k \beta^k (mc_{t+k}^r - mc^r) + 
			\sum_{k=0}^{\infty} \theta^k \pi_{t+k} \\
			=
			\quad
			\theta \beta \E_t (\hat{p}_{t+1} - p_{t}) + (1 - \theta \beta) \Theta (mc_{t+k}^r - mc^r) + (1 - \theta) (\hat{p}_t - p_{t-1})
		\end{aligned}		
	\end{equation} 

	This can be rewritten in more compact form as \footnote{where $\Theta \equiv \frac{1 - \alpha}{1 - \alpha + \alpha \epsilon} \leq 1$} \footnote{	where $\lambda \equiv \frac{(1-\theta)(1-\beta\theta)}{\theta} \Theta$}
	\begin{equation}
		\pi_t = \beta E_t [\pi_{t+1}] + \lambda \hat{mc}_{t}
	\end{equation}

	The above combined with $\hat{mc}_t^r$ \ref{eq:llmcrhat} allows to construct the New-Keynesian Phillips curve, on of the two core equations of the NK model.\footnote{where $\kappa = \lambda \frac{\sigma (1 - \alpha) + \phi + \alpha}{1 - \alpha)}$}
	\begin{equation} \label{eq:llnkp}
		\begin{aligned}
			\pi_t = \beta E_t [\pi_{t+1}] + \kappa \tilde{y}_t
		\end{aligned}
	\end{equation}
	
	The second core equation of the New-Keynesian model is derived from the Euler equation \ref{eq:lleuler}, the equilibrium condition \ref{eq:lleq} and the log linear Fisher identity $r_t \equiv i_t - \E_t[\pi_{t+1}]$. Inserting $y_t$ for $c_t$ and introducing the difference between output and its natural level as $\tilde{y}_t \equiv y_t - y_t^n$ allows to write
	\begin{equation}
		\tilde{y}_t = 
		\left[
		\E_t[y_{t+1}] - \frac{1}{\sigma} (i_t - \rho - \E_t[\pi_{t+1}])
		\right]
		-
		\left[
		\E_t[y_{t+1}^n] - \frac{1}{\sigma} (r_t^n - \rho)
		\right]
	\end{equation}
	This provides the New-Keynesian IS curve.
	\begin{equation} \label{eq:llnkis}
		\tilde{y}_t = \E_t[\tilde{y}_{t+1}] - \frac{1}{\sigma} (i_t - r_t^n - \E_t[\pi_{t+1}])
	\end{equation}
% discussion of IS and P curve
	\subsubsection{Monetary policy}
	Closing the model requires an interest rate rule, describing the behaviour of the central bank. This work will rely on a traditional Taylor rule with an exogenous interest rate shock following \cite{gali_monetary_2008}.
	\begin{equation}
		\begin{aligned}
			i_t = \rho + \phi_{\pi} \pi_t + \phi_{y} \tilde{y}_t + v_t \\
			v_t = \rho_v v_{t-1} + \epsilon_t^v
		\end{aligned}
	\end{equation}

	\subsubsection{State Space}
	Having derived the New-Keynesian Phillips curve, New-Keynesian IS equation as well as a monetary policy rule allows to formulate the model as a linear state space. For this purpose 
	\begin{equation}
		\begin{bmatrix}
			\tilde{y}_t \\
			\pi_t
		\end{bmatrix}
		=
		\Omega
		\begin{bmatrix}
			\sigma & 1 - \beta \phi_{\pi} \\
			\sigma \kappa & \kappa + \beta (\sigma + \phi_y)
		\end{bmatrix}
		*
		\begin{bmatrix}
			\tilde{y}_{t-1} \\
			\pi_{t-1}
		\end{bmatrix}
		+
		\Omega
		\begin{bmatrix}
			1 \\
			\kappa
		\end{bmatrix}	
		(\hat{r}_t^n - v_t)
	\end{equation}
	where $\Omega \equiv \frac{1}{\sigma + \phi_y + \kappa \phi_{\pi}}$ and $\hat{r}_t^n \equiv r_t^n - \rho$.
	
%Define stability condition of the linear state-space, Blanchard-Kahn

	\subsection{Petrol}
	The below is an extension of the standard NK model by petrol as a consumption good as well as factor of production. There is no domestic production of petrol, instead it is imported and its price is assumed to be exogenous. Beyond the import of petrol there is no foreign trade, all closed economy assumptions thus hold. 
	Given the exogenous nature of petrol the model is quite similar to the standard NK model. The below will therefore depart from (section) and only outline the changes to the model from petrol production. 	
	
	\subsubsection{Bundler}
	Petrol is considered a consumption good and as such enters into the good basket $C_t$. This inclusion is modelled across the bundling process, deriving the optimal basket across two classes of goods, domestic and foreign. 
	The bundling of domestic goods follows from the same procedure as outlined in (section). Variables $C_{q,t}$ and $P_{q,t}$ are aggregated consumption and price respectively. The optimisation across the continuum of goods results in the same properties as above. These are share of good $C_{i,q,t}$ in the consumption basket $C_{q,t}$ and the aggregated price index $P_{q,t}$ as well as an expression of aggregate consumption. \\
	\begin{equation} \label{eq:o_pindex}
		P_{q,t} = \left( \int_{0}^{1} P_{i,q,t}^{1 - \epsilon} di \right)^{\frac{1}{1-\epsilon}} \\
	\end{equation}
	\begin{equation} \label{eq:o_cshare}
		C_{i,q,t} = \left( \frac{P_{i,q,t}}{P_{q,t}} \right)^{-\epsilon} C_{q,t}
	\end{equation}
	\begin{equation} \label{eq:o_pcon}
		\int_{0}^{1} P_{it} C_{it} di = Z_t = P_t C_t
	\end{equation}
	Foreign goods consumption $C_{m,t}$ exclusively consists of petrol and its share in overall consumption $C_t$ is assumed to be constant. Parameter $\chi$ represents this share. 
	This assumption, though a multiplication, is viable as consumption demand for petrol is relatively inelastic in the short-run. Empirical evidence suggests that due to the inability replace durable goods (e.g. cars) that usually are at heart of consumption demand this demand is inelastic (ref). Overall consumption this is defined as \footnote{where $\Theta = \chi^{-\chi}(1-\chi)^{(\chi-1)}$}
	\begin{equation}
		C_t \equiv \Theta_\chi C_{m,t}^\chi C_{q,t}^{1-\chi}
	\end{equation}
	From this the overall price level of the consumption basket $P_{c,t}$ is derived. Where $S_t \equiv \frac{P_{m,t}}{P_{q,t}}^\chi$ is the real price of oil.
	\begin{equation}
		P_{c,t} \equiv P_{m,t}^\chi P_{q,t}^{1-\chi} 
			= P_{q,t} \frac{P_{m,t}}{P_{q,t}}^\chi
			= P_{q,t} S_t^\chi
	\end{equation}
		
	Following this reasoning and using \ref{eq:o_pcon} overall consumption can be written as 
	\begin{equation}
		C_t P_{c,t} = C_{m,t}P_{m,t} + C_{q,t}P_{q,t}
	\end{equation}

	\subsubsection{Household}
	Having derived expressions for the consumption basket and its price the household's optimisation problem can be solved. As in the standard NK model this reads as follows. 
% log-utility	
	\begin{equation}
		\begin{aligned}
			\max_{B_t, C_t, L_t} \quad \sum_{\infty}^{t=0} \beta^{t} U(C_{t}, L_{t}) \\
			\textrm{s.t.} \quad P_{c,t} C_t + Q_t B_t = B_{t-1} + W_t N_t \\
			\textrm{FOC:} \\
			- \frac{U_{n,t}}{U_{c, t}} = \frac{W_t}{P_{c,t}}\\
			Q_t = \beta \E_t \left[ \left( \frac{U_{c, t+1}}{U_{c,t}} \right)^\sigma \frac{P_{c,t}}{P_{c,t+1}} \right] 			
		\end{aligned}
	\end{equation}
	
	The authors use a log-utilty function $U(C_t,L_t) \equiv \log(C_t) - \frac{N_t^(1+\phi)}{1+\phi}$ which results in the following first order conditions.
	\begin{equation}
		\begin{aligned}
			Q_t = \beta \E_t \left[ \frac{C_t}{C_{t+1}} \frac{P_{c,t}}{P_{c,t+1}} \right] \\
			\frac{W_t}{P_{c,t}} = C_t N_t^\phi
		\end{aligned}
	\end{equation}

	\subsubsection{The Firm}
	Petrol serving as a second factor of production the firm's optimisation problem is extended from the standard NK model. As in the RBC the firm obtains its production factor demand by equalising their marginal cost. 
	\begin{equation}
		\begin{aligned}
%			\max_{M_{i,t}} \quad A_t M_{i,t}^{\alpha_m}
			\max_{M_t} \quad & A_t M_{it}^{\alpha_m} N_{it}^{\alpha_n} \\
			\textrm{s.t.} \quad & Y_{i,q,t} = W_t N_{i,t} + M_{i,t} P_{m,t}\\
			\textrm{FOC:} \\
			\quad & W_{i,t} = \alpha_n A_t M_{i,t}^{\alpha_m} N_{i,t}^{\alpha_n -1}\\
			\quad & K_{i,t} = \alpha_m A_t M_{i,t}^{\alpha_m -1} N_{i,t}^{\alpha_n}
		\end{aligned}
	\end{equation}
	Equalising the marginal cost of labour and petrol leads to the an expression of the firms nominal marginal cost $MC_{t}^n$.
	\begin{equation}
		MC_t^n = \frac{W_{i,t}}{\alpha_n Y_{i,q,t} N_{i,t}^{-1}} = \frac{P_{m,t}}{\alpha_m Y_{i,q,t} M_{i,t}^{-1}}
	\end{equation}
	Having obtained nominal marginal cost an expression for the firm's mark-up can be defined as $\xi_{i,t} \equiv \frac{P_{q,t}}{MC_{i,t}^n}$. With some further manipulation \cite{blanchard_macroeconomic_2007} uses this to obtain the demand for petrol.
	\begin{equation}
		\begin{aligned}
		\xi_{i,t} \frac{P_{m,t}}{P_{q,t}} M_{i,t} = \alpha_m Q_{i,t} \frac{P_{i,q,t}}{P_{q,t}} \\
		M_{i,t} = \alpha_m \frac{Q_{i,t}}{S_t \xi_{i,t}} \frac{P_{i,q,t}}{P_{q,t}}
	\end{aligned}
	\end{equation}

	\subsubsection{Equilibrium}
	The equilibrium conditions follow the standard NK model in assuming the ressource constraint $Y_{i,q,t} = C_{i,q,t}$ on individual firm level. As showed in (section) of the standard NK model overall domestic this property also holds on the overall output level $Y_{q,t} = \left( \int_{0}^{1} Y_{it}^{1-\frac{1}{\epsilon}} di \right)^{\frac{\epsilon}{\epsilon - 1}} = C_{q,t}$. \\
	
	Using this fact the 
	\begin{equation}
		M_t = \frac{\alpha_m Y_{q,t}}{\xi_{t}^n S_t}
	\end{equation}
% show xi_t mathematecally 
	where $\xi_t^n$ is the individual firm mark-up weighted by its share in output $Y_{q,t}$ which is given by the ratio of $\frac{P_{i,q,t}}{P_{q,t}}$. \\
	
	As bond holdings are zero in equilibrium all  allows to derive an expression for the overall price level, factoring out petrol.
% check the reasoning behind P_ct C_t = Pqt Cqt - P_mt M_t	
	\begin{equation}
		P_{c,t}C_t = P_{q,t} C_{q,t} - P_{m,t}M_t = \left(1 - \frac{\alpha_m}{\xi_t^n} \right) P_{q,t} Y_{q,t}
	\end{equation}

	\cite{blanchard_macroeconomic_2007} then go ahead and define the GDP deflator $P_{y,t}$ as
	\begin{equation}
		P_{q,t} \equiv P_{y,t}^{1-\alpha_m} P_{m,t}^{\alpha_m}
	\end{equation}
	Moreover, they define the gross domestic product as 
	\begin{equation}
		P_{y,t} Y_t \equiv P_{q,t} Y_{q,t} - P_{m,t} M_t = \left( 1 - \frac{\alpha_m}{\xi_t^n} \right) P_{q,t} Y_{q,t}
	\end{equation}
	
	This yields:
	and expression for petrol inputs $M_t$
	and expression for time dependent real mark-up $\log(\xi_t) \equiv \mu_t \equiv mc_t^r$
	
	
	\subsubsection{Price setting}
	The price setting problem of the firm is equivalent to the price setting in the standard NK model. It diverges in only one notion, the real marginal cost $MC_t^r$. \\
	
	The expression for inflation is obtained as in the standard model
	\begin{equation}
		P_{q,t} = 
		\left[ 
		\theta P_{q,t-1}^{1 - \epsilon} + (1 - \theta) \hat{P}_t^{1 - \epsilon}
		\right]^{\frac{1}{1 - \epsilon}}
	\end{equation}
	The firm solves the same optimisation problem as in the above. 
		\begin{equation}
		\begin{aligned}
			\max_{\hat{P}_t}
			\sum_{k=0}^{\inf} \theta^k \E_t 
			\left[
			Q_{t, t+k} 
			\left(
			\hat{P}_t Y_{t+k|t} - TC_{t+k|t}^n(Y_{t+k|t})
			\right)
			\right] \\
			\textrm{s.t.}\\
			\quad
			Y_{it+k|t} = \left(\frac{\hat{P}_t}{P_{t+k}} \right)^{-\epsilon} C_{t+k}
		\end{aligned}
	\end{equation}
	with the first order condition 
	\begin{equation}
		\begin{aligned}
			\sum_{k=0}^{\infty} \theta^k \E_t 
			\left[
			Q_{t,t+k} Y_{q,t+k|t} 
			\left(
			\hat{P}_{q,t} - \frac{\epsilon}{1 - \epsilon} MC_{t+k|t}^n
			\right)
			\right]
			= 0
		\end{aligned}
	\end{equation}
	
	\begin{equation}
		\pi_{q,t} = \beta E_t [\pi_{q,t+1}] + \lambda \hat{mc}_{t}
	\end{equation}
	
	The difference in the New-Keynesian Phillips curve lies in $\hat{mc}_t \equiv mc_t^p - mc^p = $. Instead of the output gap the mark-up gap becomes the instrument at the heart of the model.
	
	\subsubsection{Log linearisation}
	The above is then log-linearised to the following form
	
	\begin{tabular}{llr}
		Equation name & Equation & Log-linear Equation \\
		\hline
		 Euler equation & 
		 $Q_t = \beta \E_t\left[ \frac{C_t}{C_{t+1}} \frac{P_t}{P_{t+1}} \right]$ &
		 $c_t = \E_t [c_{t+1}] - (i_t - \E_t[\pi_{t+1}] - \rho)$ \footnotemark \\
		 
		 Labour supply &
		 $\frac{W_t}{P_{c,t}} = C_t N_t^\phi$ &
		 $w_t - p_t = c_t + n_t \phi$ \\
		 
		 Oil factor demand &
		 $M_t = \frac{\alpha_m Y_{q,t}}{\xi_t^n S_t}$ &
		 $m_t = -\mu_t^n - st + y_{q,t}$ \footnotemark \\
		 
		 Production function &
	%A_t M_t^{\alpha_m} N_t^{\alpha_n} = 
		 $Y_{q,t} = A_t N_t^{\alpha_n} \left( \frac{\alpha_m Y_{q,t}}{\xi_t^n S_t} \right)^{\alpha_m}$ &
		 $y_{q,t} = \frac{1}{1-\alpha_m} (a_t + \alpha_n n_t - \alpha_m s_t - \alpha_m \mu_t^n)$ \\
		 
		 Gross output & 
		 $P_{c,t}C_t = (1-\frac{\alpha_m}{\xi_t^n})P_{q,t}Y_{q,t}$ &
		 $c_t = y_{q,t} - \chi s_t + \eta \mu_t^n$ \footnotemark \\
		 
		 GDP deflator &
		 $P_{q,t} \equiv P_{y,t}^{1-\alpha_m} P_{m,t}^{\alpha_m}$ &
		 $p_{y,t} = p_{q,t} - \frac{\alpha_m}{1-\alpha_m}s_t$ \\
		
		GDP &
		$P_{y,t}Y_t = \left(1-\frac{\alpha_m}{\xi_t^n}\right) P_{q,t}Y_{q,t}$ &
		$y_t = y_{q,t} + \frac{\alpha_m}{1-\alpha_m}s_t + \eta \mu_t^n$ \\
		
		\hline
	\end{tabular}
% fix footnotes
	\footnotetext{where $\rho \equiv - \ln(\beta)$ and $i_t = \ln(Q_t)$}
	\footnotetext{where $\mu_t^n \equiv \ln(\xi_t^n)$}
	\footnotetext{where $\eta \equiv \frac{\alpha_m}{\xi_t^n-\alpha_m}$}

	The expressions for gross output and output can be combined into
	\begin{equation}
		y_t = \frac{1}{1-\alpha_m} (a_t + \alpha_n n_t)
	\end{equation}
	which combined with the expression for consumption
	\begin{equation}
		c_t = y_t - (\frac{\alpha_m}{1-\alpha_m} + \chi) s_t
	\end{equation}
	
	

	
	\section{Data}
	
	All data is gathered from St. Louis Federal Reserve data base (ref), the specific data code can be found in the (appendix). As such all data has been seasonally adjusted already and as pointed out by \cite{pfeifer_guide_2021} is if revised in hindsight adjusted in its entirety so that all measurements depart from the same assumption.
	
	The majority of the data used in across this thesis is standard to the DSGE modelling literature. However, some adjustments have been made to account for the inclusion of a petrol sector.
% potential GDP ??
	With regards to inflation $\pi$ this work relies on the median consumer price index for the RBC and the NK model. However, as the aim of the NK energy model is to analyse inflation using the CPI would be counterproductive. The model itself should generate an inflation that corresponds to the CPI as it includes the petrol sector into its dynamics. In order to accurately relfect on the models ability to do so one therefore has to remove energy price inflation from the data. Only doing so allows to asses the models real ability to replicate inflation. The NK energy model will therefore rely on the "Sticky Price Consumer Price Index less Food and Energy". A similar logic applies to consumption expenditure which for the NK energy model does not include expenditure for energy and utilities.
	
	The other variables used for the evaluation of model suitability are rather standard. The nominal interest rate is proxied by the federal funds effective rate. Consumption and investment are measured by household expenditure data. Labour and wage referr to non-fram business total hours worked as well as averge hourly wage as recommended for the US economy (ref). As proxy of energy cost this thesis employs the WTI Crude Oil Index as suggested by (ref) as it is available for a longer time horizon as opposed to the more recent Brent Crude Oil Index.
	
	\subsection{Preprocessing}
	
	Economic data is usually considered in per capita terms, in order to account for different population dynamics (ref). The same procedure will be applied across this work. However, population dynamics are not constantly collected wherefore its records display significant irregularities and not the smooth dynamic of true population growth or decline \cite{pfeifer_guide_2021}. Using these error prone records thus will distort the data signal of other more carefully collected economic data. In order to circumvent this irregularity a smoothing with the Hodrick-Prescott (HP) filter's (at level 10,000) is suggested by (ref) Edge, Gürkaynak, and Kisacikoglu (2013). This work follows this approach and divides trending variables by population data to obtain per capita variables.
	
	However, population dynamics are only one source of trend in variables. Variables measured in monetary terms are also concerned by inflation. \cite{pfeifer_guide_2021} (Grohe, Uribe) recommend de-trending nominal variables by the GDP deflator in order to account for this. This work follows this approach and relies on inflation corrected data so that monetary variables are expressed in the same quantities.
	
% talk about one-sided filter
	As the model studied across this thesis are describing log-deviations of economic variables from their respective steady states further transformation is required. This traditionally involves separating the underlying long-trend of GDP per capita from its cycle variations. A commonly employed method for doing so is the Hodrick-Prescott filter, which if fed with log-transformed variables will output the desired log-deviations from the trend as well as the trend itself (ref). As this work is concerned with quarterly variables the smoothing parameter has been set to 1600 as suggested by the work of \cite{ravn_adjusting_2002}. This procedure by construction yields stationary time series. In order to confirm the stationarity this work has again as commonly the case confirmed stationarity of all variables using the Augmented-Dickey Fuller test (ref).
	
	To this point all trending variables have been transformed into stationary time series of log deviation from their respective natural level. The percentage variables, namely inflation and interest rate require no such treatment. However, as they are stated in annual terms they require translation into quarterly variables. This has been done according to the following formula $(1 + \frac{\matr{X}}{100})^{1/4} - 1)$.
	
	An overview over the transformed variables can be found in the (appendix).
	The descriptives statistics can be found in the (appendix).
	
	\subsection{Analysis of models}
	Compare model covariance matrices, to actual covariance in data
	Show theoretical impulse response functions, differences between NK and RBC
			
	\section{Bayesian Estimation}
		
	As explained in the above the RBC and later NK models are based on structural parameters of the economy. As such they are not subject to the Lucas 1976 critique as their parameters do not  directly depend on the choices of agents (ref). Instead, they represent the deep-rooted dynamics of the dynamic system describing the economy, as such they should exhibit a certain stability over time (ref). 
	
	This perspective makes an implicit but important assumption: DSGE model corresponds to the real dynamic system describing the economy. A far reaching assumption which has been considered at least partially validated based on the ability of early DSGE models to generate data matching 1st and 2nd order moments of real variables such as output, consumption and investment (ref). 
	In an attempt to tweak this capability  early attempts to parameter to identifying the true parameters of the economy were made. In doing so academics first relied on what \cite{prescott_theory_1986} referred to as great ratios of the economy. Calculating for example the real interest rate allowed to derive the discount factor $\beta$. 
	Other parameters had fewer empirical counterfactuals and where thus not estimated by manually calibrated to sensible values. These inaccuracy gave rise to approaches more deeply rooted in econometric analysis. They were undertaken in the aim of reliably estimating the economy's true structural parameters.
	
	Such early attempts of econometric identification relied on standard frequentists statistics such as the maximum-likelihood estimators (MLE) (ref). Another approach pioneered by Blanchard \& Kahn (ref) referred to as simulation method of moments, later progressed into the general method of moments (GMM) attempted to minimize the distance between simulated and real data moments. The GMM allowed to discriminate between several competing specification of the same structural model \cite{christiano_current_1992}. Comparing the estimated parameters of the model with real data values then served as inspection of model fit to data. However, as models grew in size a major short-coming of GMM became apparent. The method of moment conditions were no longer sufficient to estimate model fit across the variety of parameters \cite{guerron-quintana_bayesian_2013}. Likewise, the ML estimator turned out to be limited in its ability to estimated the entirety of parameters, requiring manual calibration of some parameters \cite{guerron-quintana_bayesian_2013}.
	
	While frequentists statistics knew to overcome the identification problem in introducing theoretical moments Smith (1993) it was soon challenged by a different perspective. At the heart of all frequentist statistics lies the assumption that a given model corresponds to the true structural process, it is the 'true' model (ref). If this assumption cannot be met all estimation will be biased and thus need to be corrected for. Again remedies where found (ref) Smith (1993) or Dridi, Guay, and Renault (2007) in varying parameters most relevant to overall model moments and keeping less relevant ones constant throughout econometric estimation. Yet it is argued that this approach at least partly failed in overcoming the conditional bias introduced by non-relevant parameters \cite{guerron-quintana_bayesian_2013}.
	
% explain Baye's law ?
	Consequently, a new philosophy of linking models to data was identified. Bayesian statistics replaces the assumption of a single true model by the convenient formulation of conditional likelihood making it more appealing to the purposes of DSGE modelling \cite{guerron-quintana_bayesian_2013}. In doing so the Bayesian statistics relies on Bayes law linking the conditional likelihood of an event to the prior distribution.
	\[
	P( \Theta | Y_{T}) = L(Y_{T} | \Theta) P(\Theta)
	\]
	A given model specification can be linked to a likelihood evaluated against the backdrop of data the system is meant to generate (ref). This allows to assess a model's suitability. The frequentist's idea of a single true parameter is replaced by distributions assigning different probabilities to parameter values. These prior distribution reflect previous knowledge allowing to narrow down the possible space of parameters. Instead of identifying a true parameter Bayesian statistics, aims at evaluating these believes and at decreasing uncertainty around the parameter space i.e. the variance of the distribution. This process translates the prior distribution into a posterior distribution, not necessarily belonging to the same class. 
	This process of evaluation is referred to as sampling and builds on numerical and stochastic calculus to approximate the true posterior distribution (ref). Furthermore, Bayesian statistics naturally provides great forecasting properties. Once uncertainty around the space of the real system has been reduced it allows for make informed forecasts, naturally providing confidence bans. 
	
	The following section will therefore be concerned with these four concepts, conditional likelihood, prior and posterior distribution, sampling and forecasting. DSGE literature has developed a rich body of literature covering the case of the canonical DSGE model rather well (ref). The state of research is currently concerned with the estimation of larger models and non-linear and not normally distributed posterior distributions (ref). However, given the simplicity of the above discussed models current techniques are more than sufficient for this thesis purposes.
	
	
	\subsection{The log-likelihood}
	
	Bayesian analysis of DSGE models is concerned with the transition from a prior distribution reflecting previous beliefs to the true posterior distribution (ref). This process relies on Bayes law as illustrated above, which requires the prior distribution and the marginal likelihood of data given the representation of the system $L(Y | \Theta)$ (ref). This section is concerned with obtaining the latter.
	
	The marginal likelihood for a linear state system requires a technique allowing to assess the fit of data to the system. In doing so one relies on the fact that DSGE models when expressed as linear dynamic systems are data generating processes. This allows to formulate the question of how likely a given set of data has been generated by the system. A method allowing to do so is the Kalman filter (ref). This filter originates from engineering and has originally been developed to track the position of an air plane off signals from different sensor. As sensor signals are inherently noisy the filter is designed to exploit one's knowledge about the measurement noise in order to reduce the variance on its believe about the actual position or state of the plane. In doing so the filter imposes the important assumption of Gaussian distributed noise, exploiting the fact that any multiple of two Gaussian distributions will again yield a Gaussian distribution. The resulting Gaussian then has a lower variance than its parents, which is equivalent to saying that the resulting distribution has narrowed down the believe on the current state of the system. This assumption is especially important for the purposes of DSGE modelling, as the likelihood function of the Kalman filter relies on the assumption of Gaussian noise. Applied to DSGE models this implies that the joint distribution of shocks must be Gaussian \cite{herbst_bayesian_2016}. 
% check the following statement	
	This is usually satisfied for small scale New Keynesian models if the individual shocks are Gaussian (ref). For larger DSGE models this assumption does not necessarily hold and shocks are highly correlated. As mentioned above the is state of research and demands other methods of evaluation such as the Kalman-Particle filter (ref). As this work is concerned with the canonical model the Kalman filter will be sufficient.
	
	Given such linear Gaussian system the Kalman filter becomes especially desirable as it provides optimal forecast (ref), making any other method of evaluation obsolete. The below will now briefly explain how the Kalman filter is constructed off the linear system and end with deriving its likelihood function.
	
	Given the generic linear system below wit $\matr{F}$ as the transition and $\matr R$ as the shock noise matrix the Kalman filter proceeds in two steps, a predict and an update step:\\
	\[
		\matr{X}_t = \matr F \matr{X}_{t-1} + \matr R \matr{\epsilon}_t
	\]
	\[
	\matr{\epsilon}_t \sim N(0, \sigma_{\epsilon})
	\]	
	\[
			\begin{bmatrix}
				y_t \\
				z_t
			\end{bmatrix}
		=
			\begin{bmatrix}
				\rho_y & \rho_{yz} \\
				\rho_{zy} & \rho_z
			\end{bmatrix}
		+
			\begin{bmatrix}
				y_{t-1} \\
				z_{t-1}
			\end{bmatrix}
		+
			\begin{bmatrix}
				\epsilon_{yt} \\
				\epsilon_{zt}
			\end{bmatrix}				
	\]
	\[
		\begin{bmatrix}
			\epsilon_{y,t} \\
			\epsilon_{z, t}
		\end{bmatrix}
		\sim
		N
		\left(
			\begin{bmatrix}
				0 \\
				0
			\end{bmatrix}
		,
			\begin{bmatrix}
				\sigma_y^2 & 0 \\
				0 & \sigma_z^2
			\end{bmatrix}
		\right)
	\]
		
	\textbf{Predict step}
	
	In the predict step the Kalman filter propagates the system of state variables $\matr X_{t}$ according to the transition matrix $\matr F$. This matrix defines the system and as such is time independent. The propagation then yields an estimate for the $X_{t+1}$ system state. This could be supplemented by directions or external influences on the system that one is aware of. However, this is not the case for this work.
	\[
		\matr{\hat{X}}_{k|k-1} = \matr F \matr X_{k-1|k-1}
	\]
	The second key aspect to the Kalman Filter is the system state variable's covariance matrix $\matr P_{t}$, which is also time dependent. As pointed out in the above, this matrix is assumed to be Guassian, thus semi-definit positive (ref). The covariance matrix and the state $\matr X$ essentially build a Guassian distribution summarising the uncertainty around the actual state.
	\[
		\matr{P}_{k| k-1} = \matr F \matr{P}_{k-1| k-1} \matr{F}^T + \matr Q
	\]
	
	\textbf{Filter step}
		
	Once the predicted state $\matr X$ and covariance matrix $\matr P$ have been determined the Kalman filter proceeds to identifying the most likely system state. For this it takes into account the state variables measurement referred to as $\matr z_t$, which due to measurement noise is not considered to always reflect the actual actual state of the system. The actual state is instead thought of to ly in-between the predicted state $\matr{X}_{k| k-1}$ and the measurement $\matr{z}_k$. The objective of the filtering step is therefore to identify this true state in-between the two. For this purpose the Kalman filter derives the Kalman gain $\matr{K}_{k| k-1}$, which can be though of as a weighting function between $\matr{X}_{k| k-1}$ and $\matr{z}_k$. In building this average the filter takes into account the process noise and the covariance matrix as well as the accuracy of past iterations.\\
	
	$\matr{y}_k$ is the difference between measurement and prediction. The matrix $\matr H$ is the measurement matrix, ensuring that predicted state $\matr{X}_{k| k-1}$ and $\matr{z}_k$ are in the same units.
	\[
		\matr{y}_{k} = \matr{z}_k - \matr H \matr{X}_{k| k-1}
	\]
	The	$\matr{S}_k$ captures the filters uncertainty by combining the covariance around the predicted state $\matr{X}_{k| k-1}$ with the process noise $\matr{R}$. In DSGE terminology the process noise is derived from the shock covariance matrix (ref).
	\[
		\matr{S}_k = \matr H \matr{P}_{k| k-1} \matr{H}^T + \matr{R}
	\]
	The uncertainty term $\matr{S}_{k}$ is then used to calculate the weighting function between measurement and predicted state the Kalman gain. 
	\[
		\matr{K}_k = \matr{P}_{k| k-1} \matr{H}^T \matr{S}_{k}^{-1}
	\]
	The filtered state which will be next iterations start then is calculated off the Kalman gain and $\matr{y}_k$. The covariance matrix is updated in a similar fashion. This yields a new narrower distribution summarising the uncertainty around the true systems state.
	\[
		\matr{X}_{k|k} = \matr{X}_{k| k-1} + \matr{K}_k \matr{y}_k
	\]
	\[
		\matr{P}_{k|k} = (\matr I - \matr{K}_k \matr{H}) \matr{P}_{k|k-1}
	\]
	
	\textbf{Likelihood function}
	
	Having introduced the Kalman filter algorithm the setting of the starting values remain. For the Kalman filter these are less decisive, has the filter is very capable in converging to the true state. However, as the models at hand are describing deviations from the steady state this work has set $\matr{X}_{0|0}$ to zero, assuming that the system is in its steady state to begin with, an approach generally followed by the literature. Secondly, this section has initially been dedicated to the Kalman filter's likelihood function. Having explored the propagation one can now derive the likelihood function. This function is heavily reliant on the assumption of Gaussian system noise, as it draws from the normal distribution in evaluating the likelihood for each iterations. In the below when referring to likelihood this thesis will however be referring not to the individual likelihood of each iteration but to the cumulated likelihood across all iterations.
	
	The probabiltiy desinty function for any normal distribution is given by: 
	\[
	\Lagr = \frac{1}{\sigma \sqrt{2 \pi}} \exp [- \frac{1}{2} \frac{x - \mu}{\sigma}]^2
	\]
	The Kalman filter relies on the multivariate where the standard deviation is given by system uncertainty $\matr{S}_k$ and the difference between mean and measurement are expressed through $\matr{y}_k$.
	\[
		\Lagr_k = \frac{1}{\matr{S}_k \sqrt{2 \pi}} \exp [- \frac{1}{2} \matr{y}_k^{T} \matr{S}_{k}^{-1} \matr{y}_k]
	\]
% get filterpy reference and Kalman Literature ref
	This work has drawn from filterpy in order to perform the above calculations (ref).
	
	\subsection{Sampler}
	
	There are local and global identification problems 
	
	The key aspect of Bayesian estimation across DSGE models is the identification of the true parameter distribution, the posterior. However, mapping a DSGE model's parameters to its posterior is non-linear in the parameter vector $\Theta$. Therefore the posterior distribution cannot be evaluated analytically \cite{herbst_bayesian_2016}. Instead a numerical approximation mechanism is require, this is referred to as the sampler \cite{guerron-quintana_bayesian_2013}. The Metropolis-Hastings Monte Carlo Markov Chain (MH-MCMC) sampler is the predominant sampling method in linear Bayesian estimation literature \cite{guerron-quintana_bayesian_2013}. The MH-MCMC sampler is applicable to small scale New Keynesian models, for which it usually delivers good results as the joint parameter distribution is well-behaved and elliptical \cite{herbst_bayesian_2016}. This reduces the posterior identification problem to a global problem. Larger NK models exhibit no-elliptic parameter distributions and thus require samplers which are able to distinguish local and global likelihood maxima in their identification of the posterior \cite{herbst_bayesian_2016}. As this thesis is concerned with the small scale canonical model the Metropolis-Hastings sampler is sufficiently accurate and will be used.
	
	The MH-MCMC sampler suggests new posterior candidates according to a multivariate random walk. The posterior is extend by a suggestion if it provides an improvement in the likelihood as described in (section). The algorithm generates a stable Markov chain, which is meant to exhibit low autocorrelation and low variance of the MH estimator. If this is fulfilled the resulting chain is equivalent to the posterior distribution \cite{herbst_bayesian_2016}.	
		
	Metropolis Hastings sampling algorithm
	- it is important to note that a draw from the shocks prior, factors into the drawing process
	- procedure also followed by, with 300,000 draws of which 20,000 are considered burn in \cite{chin_bayesian_2019}
		
	The below will be explaining this thesis implementation of the MH sampler and methods of evaluating its convergence to the true posterior distribution. The algorithm proceeds in three main steps.
	First a candidate for the posterior $\matr{\hat{\Theta}}_{k|k-1}$ is suggested based on the random-walk law of motion. The random-walk departs from the last accepted posterior candidate and suggests a new candidate by departing from the $\matr{\Theta}_{k-1|k-1}$ according to the zero-mean Gaussian distribution $\epsilon$. The distribution's variance $\matr{\Sigma}$ can be set to either the identity matrix $\matr{I}$ or to contain the parameter prior variances on its diagonal. The latter is recommended if well-defined priors are at play \cite{herbst_bayesian_2016}.
% However, for the special case of a target distribution which is multivariate normal, Roberts, Gelman, and W.R. (1997) has derived a limit (in the size of parameter vector) optimal acceptance rate of 0.234. Most practitioners target an acceptance rate between 0.20 and 0.40. The scaling c factor can be tuned during the burn-in period or via pre-estimation chains
	 To complete the random-walk the MH sampler also employs a gain parameter $\eta$ that essentially scales the distance of the new posterior candidate away from the last accepted posterior $\matr{\Theta}_{k-1|k-1}$. This value has been calibrated by several studies, which recommend an $\eta = 0.234$ for the special case of a multivariate normal distribution (ref). Generally the value of $\eta$ should correspond to an acceptance rate between 20\% and 40\% of suggested draws \cite{herbst_bayesian_2016}.
	\[
		\matr{\hat{\Theta}}_{k|k-1} = \matr{\Theta}_{k-1|k-1} + \eta \epsilon
	\]
	\[
		\epsilon = N(0, \matr{\Sigma})
	\]
	The posterior candidate  $\matr{\hat{\Theta}}_{k|k-1}$  is th evaluated base on the Kalman Filter likelihood function introduced in (section). This yields the likelihood of posterior candidate given the data $\matr Y$.
	\[
		 \Lagr(Y| \matr{\hat{\Theta}}_{k|k-1})
	\]
	Once the likelihood is obtained the algorithm proceeds into the two step acceptance procedure. First the likelihood of the candidate is compared to last accepted posterior in order to determine whether this draw constitutes an improvement. This is done according to Bayes' law. 
	\[
	\omega_k = \min \{
						\frac{ \Lagr(Y| \matr{\hat{\Theta}}_{k|k-1}) * P(\matr{\hat{\Theta}}_{k|k-1}) } 
						{ \Lagr(Y| \matr{\Theta}_{k-1|k-1}) P(\matr{\Theta}_{k-1|k-1})},
						 1
					\}
	\]
	The likelihood ratio is then fed into the second step the random acceptance. The above obtained $\omega_k$ is compared the uniform variable $\phi \sim U(0, 1)$. If $\omega_k \leq \phi_k$ the draw is accepted and the candidate is assumed into the posterior. This process is repeated $N$ times and thus generates the posterior distribution.
% math expression for the posterior
	As illustrated in the above MH-MCMC is a recursive method and as such requires an initial value for the posterior $\Theta$. This values is usually obtained throughout a so-called burn in period corresponding to 10 \% of total iterations (ref). The total number of iterations depends on the speed of convergence of the chain but usually ranges between 20,000 and 50,000 iterations (ref).\\
	\\
	The following will now describe the evaluation steps to insure convergence of the chain. Moreover, once the algorithm has run one needs to evaluate this performance. A method beyond visual inspection is suggested by (ref).
	
	The MH random walk is highly suceptible to mis specification of the posterior of the prior distribution is far away from the true posterior \cite{herbst_bayesian_2016}
			
	\subsection{Priors}
	
	The assumption for prior selection is that priors are obtained from some past knowledge. If this knowledge is independent of the data used to evluate the liklihood function than the prior holds. This is fullfiled if the data for prior building at least predates the data for likelihood evluation \cite{herbst_bayesian_2016}.
		
	Priors are statistical distributions which either reflect believes about the strucutral mechanism or are derived from data, external to the estimation process \cite{del_negro_forming_2008}
	- Priors thus refelect microeconomic evidence, for example regarding labour supply and elasticity
	- However, they are a simplication of the real joint likelihood function of the strucutural model. Such function is difficult and sometimes impossible to evaluate
	
	Priors \cite{del_negro_forming_2008}
	- Priors are usually assumed to be independent for simplicities sake
	- this is true for shocks in small scale canonical model, as shocks are assumed to be independent, an important assumptio for the bayesian analysis to hold \cite{herbst_bayesian_2016}
	- Priors are often calibrated for the model to match the covariance of real data
		
	\cite{del_negro_forming_2008} suggest two kinds of data sources. Pre-sample data, which is not included in the model evaluation process or in-sample data, that is explicitly excluded from the model evaluating likelihood function.
	As the model parameters are thought of as structural, hence relatively time-invariant, more dated data can be used to estimate them. 
	
	Priors are divided into three groups, following \cite{del_negro_forming_2008}
	
	\subsubsection{Steady state priors}
	The 'great ratios' or long-run relationships as \cite{kydland_time_1982} referred to them pin down the steady state
	\[
		\theta_{ss} = \{\alpha, \beta, \delta, \pi^*, \eta_p\} % eta is calvo
	\]
	When determining the steady-state priors $\theta_{ss}$ and assuming independence it is possible that the resulting joint prior distributions assigns high probability to unrealistic steady state values \cite{del_negro_forming_2008}. To circumvent this issue other method
	
	This priors are usually estimated from pre-sample data \cite{herbst_bayesian_2016}
	this work follows this approach and thus 	
	
	\subsubsection{Exogenous priors}
	The exogenenous propagation paraemeters 
	these are the exogenenous shock parameters, their autoregressive coefficient and their standard deviation
	\[
		\theta_{exog} = \{\rho_a, \rho_s, \sigma_a, \sigma_s \}
	\]
	Exogenous priors can be evaluated from the model specification in keeping the influence of other priors minimal. Del Negro suggests, to set all other parameters to zero where possible. The system of equations then reduces to variables that are depend on the exogenous shocks. This reduced system can then be used to evaluate the likelihood of prior values, using out-of sample data of the later calibration. 
	An evaluation based on the likelihood of the reduced system is not necessary if the variation in observable variables can be directly retraced to individual shocks. In a simple model where shocks do not intersect in their influence on observable variables, e.g. in the case of a singular technology shock the variance of the shock can be directly inferred from the observable variable. In case of intersecting multiple shocks the likelihood of the reduced system needs to be consulted to choose appropriate priors, as their influence on observables cannot be directly inferred. 
	
	In case of exogenous observable shocks, such as energy prices, the above procedure is not required. Other than the level of technology energy price shocks are measurable, wherefore one can rely on the observable first and second order moments (ref).
	
	\subsubsection{Endogenous priors}
	The endogenous propagation parameters, including the shock autoregressive coefficients are parameters involved with the propagation of shocks through the system.
	These parameters heavily rely on micro-evidence from external data sets \cite{del_negro_forming_2008}
	All remaining parameters are stacked into the the following
	\[
		\theta_{endog} = \{\sigma_C, \sigma_L, \}
	\]
	Priors of the remaining variables are chosen in order to reflect believes or micro evidence. This work draws from the choice of \cite{del_negro_forming_2008} in their endogenous priors, given the similarity in models
	
	
	

	
		
	Priors are chosen in order to match first and second order moments individually as well as in a joint distribution across all parameters \cite{del_negro_forming_2008}
	


	
	
%	"Also, rather than imposing priors on the great ratios, C∗/Y ∗, I∗/K∗, K∗/Y ∗, and G∗/Y ∗, we fix the capital share, α, the depreciation rate, δ, and the share of government expenditure, g∗. This follows well established practices that pre-date Bayesian estimation of NKDSGE models." \cite{guerron-quintana_bayesian_2013}
	
	
	\section{Forecast evaluation}
	
	\subsection{Reference Model BVar}
	\cite{schorfheide_loss_2000}
	
	BVar with normally indpendently distributed lag parameters \cite{chin_bayesian_2019}
	
	This work uses a frequentist VAR to determine the optimal number of lags, which are found to be 4. This corresponds to the choice of four lags in \cite{chin_bayesian_2019}
	
	
	\subsection{Model Forecasts}
	Relevant to policy recommendations
	How well can models capture the dynamics of the economy, e.g. as a spectrum of possible outcomes
	
	Including priors into bayesian VAR makes the model better at forecasting \cite{chin_bayesian_2019}
	
	
	To compare models this work follows a bayesian averaging approach as in \cite{chin_bayesian_2019}
	- (Stock Watson 2003) argue that a simple average of models provides the best forecast
	- ()Jacobson and Karlsson (2004) and Wright (2009)) argue in favour of BMA
	
	
	BMA
	- the weights of each model are calculated from its posterior model probability \cite{chin_bayesian_2019}
	- This is traditionally done for structurally similar models in order to discriminate between different priors
	- the posterior probability is thereby obtained from \cite{chin_bayesian_2019}
	\[
		p(M_k | y) = \frac{p(y | M_k) p(M_k)}{\sum_{K}^{k=1} py(y | M_k) p(M_k)}
	\]
	\[
		p(y | M_k) = \int p( y| \theta_k, M_k) p(\theta_k | M_k) d \theta_k
	\]
	where $p(M_k)$ is the prior density, and $p(y | M_k)$ is the model's marginal likelihood
	
	Anderson (2008) suggest to use the predictive likelihood instead of the marginal model likelihood
	
	"Simply speaking, our Bayesian combination forecast (point forecast) is a weighted average of the K individual point forecasts, wherein the weights are determined by their predictive likelihoods" \cite{chin_bayesian_2019}
	
	Comparing forecasting performance
	Diebold-Mariano test for point forecast comparison following \cite{chin_bayesian_2019}
	- MSE based point forecast comparison 
	Amisano–Giacomini test for density forecast performance following  \cite{chin_bayesian_2019}

	
	\section{Conclusion}
	
	
	\section*{Bibliography}
%	\addcontentsline{toc}{chapter}{Bibliography}
	\bibliography{20230505_m1_dsge}
	
	
	\section*{Appendix}
	
	\subsection*{Appendix A}
		
% graphs
	\includegraphics[scale=.3]{mod4_rbc_vanilla_epsilon_A_irfs_quantiles.png}\\
	\includegraphics[scale=.3]{mod4_rbc_vanilla_posterior_hist.png}\\
	\includegraphics[scale=.3]{data_variables_transforemd.png}

	\subsection*{Appendix B}
% tables
	\include{{./graphs/priors_table.tex}}
	\include{{./graphs/fred_variables.tex}}
	\include{{./graphs/data_descriptives.tex}}
	
	\subsection*{Appendix C} \label{appc}
	\subsubsection*{Log-linearisation}
	
	Euler equations
	
	Expression for $N_t$ from $N_{it}$
	It can be shown that $ log \left( \int_{0}^{1} \left( \frac{P_{it}}{P_t} \right)^{-\frac{\epsilon}{1 - \alpha}} di \right) \approx 0$ which then allows to log linearise the production function to $y_t = a_t + (1 - \alpha) n_t$.
	
	
	RBC all equations\\
	\begin{tabular}{lr}
		\textbf{Equation name} & \textbf{Log-linear expression}\\
		\hline
		Euler equation & $\E_t \left[ \frac{C_{t+1}}{C_t} \right]^\sigma = \beta \left[ (1 - \delta)  + \E_t [R_{t+1}] \right]$ \\
		Capital supply & $K_t = (1 - delta) K_{t-1} + I_t$ \\
		Capital demand & $K_t = \alpha A_t K_t^{\alpha -1} N_t^{1-\alpha}$ \\
		Labour supply & $- \frac{U_{n,t}}{U_{c, t}} = W_t$ \\
		Labour demand & $W_t = (1 - \alpha) A_t K_t^\alpha N_t^{-\alpha}$ \\
		Production function & $Y_t = A_t K_t^\alpha N_t^{1 - \alpha}$ \\
		Equilibrium condition & $Y_t = C_t + I_t$ \\
		Technology law of motion & $\log(A_t) = (1- \rho_A) \log(A^*) \rho_A \log(A_{t-1}) + \epsilon_t^A$ \\
	\end{tabular}\\

	\include{{./graphs/rbc_steady_state.tex}}
	\include{{./graphs/rbc_transition_matrix.tex}}
	
	

\end{document}