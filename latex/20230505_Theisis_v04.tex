\documentclass[12pt,a4paper,english]{article} % document type and language

\usepackage{layout}
\usepackage{babel}   % multi-language support
\usepackage{float}   % floats
\usepackage{url}     % urls
\usepackage{graphicx}
\usepackage{amsmath} % matrix algebra
\usepackage{multirow} % tables
\usepackage{booktabs} % tables
\usepackage{blindtext}
\usepackage{geometry}
\usepackage[parfill]{parskip} % no indent
\usepackage{amssymb}
\usepackage{csvsimple}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{verbatimbox}
\usepackage{filecontents,catchfile}
\usepackage{float}
\usepackage{nameref}

% acronyms
\usepackage[printonlyused,withpage]{acronym}
\renewenvironment{description}
{\list{}{\labelwidth0pt\itemindent-\leftmargin
		\parsep0pt\itemsep0pt\let\makelabel\descriptionlabel}}
{\endlist}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}


\lstset{style=mystyle}
% Default fixed font does not support bold face
%\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
%\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% graphics path
\graphicspath{{./graphs/}}	% graphics

% title slide
%\author{Lukas Gahl}
%\title{\textbf{Dynamic Stochastic General Equilibrium: An empirical application to energy prices and inflation}\\}
%\
%\date{\today} 

% user commands
\newcommand{\matr}[1]{\mathbf{#1}} % matrix font
\newcommand{\Lagr}{\mathcal{L}} % likelihood L
\newcommand{\E}{\mathbb{E}} % expectation
% RBC expressions
\newcommand{\Rss}{\frac{1}{\beta} + \delta - 1}
\newcommand{\Ass}{\bar{A}}
\newcommand{\KLss}{\left[ \frac{\alpha \Ass}{\Rss} \right]^{\frac{1}{1-\alpha}}}

% set layout
\let\oldsection\section
\renewcommand\section{\clearpage\oldsection}

\geometry{
	a4paper,
	total={170mm,257mm},
	left=25mm,
	right=30mm,
	top=25mm,
	bottom=25mm,
}
\linespread{1.5}

% bibliography
\bibliographystyle{abbrv}
\begin{document}
	
	
	\begin{titlepage}
		\begin{center}
			\vspace*{1cm}
			
			\Large
			Dynamic Stochastic General Equilibrium: An empirical application to energy prices and inflation
			
			
			\vspace{1.5cm}
			
			Lukas Ambrosius Grahl
			
			\vfill
			
			A thesis presented for the degree of\\
			Master of Science Economics
			
			\vspace{5 cm}
			
%			\includegraphics[width=0.4\textwidth]{university}
%			\Large
		\end{center}
		Centre Economie de la Sorbonne Name\\
		Paris 1 Panthéon-Sorbonne \\
		France\\
		27th of May 2023
		
			
			

	\end{titlepage}

	\pagebreak

	\section*{Abstract}
	\pagebreak

	
	\section*{List of Acronyms}
	\begin{acronym}
		\acro{BVAR}{Bayesian Vector Autogreression}
		\acro{CPI}{Consumer Price Index}
		\acro{DSGE}{Dynamic Stochastic General Equilibrium Model}
		\acro{GDP}{Gross Domestic Product}
		\acro{HP Filter}{Hodrick-Prescott Filter}
		\acro{MH-MCMC}{Metropolis-Hastings Monte Carlo Markov Chain}
		\acro{NK}{New-Keynesian}
		\acro{NKE}{New-Keynesian Energy}
		\acro{RBC}{Real Business Cycle}
		\acro{VAR}{Vector Autoregression}
		\acro{WAIC}{Widely Applicable Information Criterion}
	\end{acronym}
	\listoffigures
	\listoftables
	\pagebreak
	
		
	\tableofcontents
	\pagebreak
	
	
	 \section{Introduction}
	 The recent simultaneous surge in energy prices and inflation has reintroduced questions about a connection between the two. The late 1970s oil shock caused a recession across most western economies. Later oil price shocks in the 90s and 2000s, despite being of similar magnitude, did not match their predecessor's economic impact \cite{blanchard_macroeconomic_2007}. Recent events however, seem to suggest that such link has re-emerged. This calls for a reevaluation of the literature on inflation dynamics. Research in this area has for much of the recent past relied on structural rational expectation models. These models analyse the transition mechanism of inflation, while allowing to take into account confounding factors such as oil prices. Relying on structural models in analysing its effect on the inflation mechanism has thus a broad base in literature. This work draws from this past literature and applies it to recent events. In doing so three strains of literature are merged, namely: \ac{RBC} and \ac{NK} literature, Bayesian estimation of structural models and literature on structural models encompassing petrol. \ac{RBC} and \ac{NK} models are often also labelled \ac{DSGE} models. They emerged as a response to the Lucas critique (1981), providing micro-based models in which parameters were not longer subject households behaviour \cite{lucas_jr_tobin_1981}. Instead parameters describ static agent preferences \cite{gali_monetary_2008}. This property made DSGE models attractive for policy analysis, wherefore they became the predominant modelling approach in macroeconomics. In their success Bayesian estimation of structural models provided an important stepping stone. It linked \ac{DSGE} models to data by providing means of estimating the time independent model parameters. Moreover, it introduced methods of comparing the models' data generating process against real data allowing for further evaluation and forecasting \cite{del_negro_forming_2008}. This provided an new criteria to the evaluation of structural models \cite{kocherlakota_model_2007}. Lastly, a body of literature on the link between oil prices and inflation in structural models has emerged. This work will review this literature and draw from one model in particular \cite{blanchard_macroeconomic_2007}. 
	 
	 The purpose of this work is to analyse the inclusion of petrol prices into the DSGE framework, as well as to evaluate the pertinence of such exercise. In its evaluation this work will employ two standard models of macroeconomic literature, the RBC and NK model. Both will be compared to a model including petrol in their fit to real data as well as forecasting performance. Lastly, a \ac{VAR} will be used as a forecasting benchmark. This work is organised as follows. First a literature review on structural models and their incorporation of oil price inflation will be provided. Three subsequent sections will be dedicated to developing the mathematical reasoning behind the RBC, the standard NK and lastly the \ac{NKE} model. Having demonstrated the assumptions involved in drawing up structural models and their linearisation this work will proceed in discussing the data used model evaluation. A fifth section will then be dedicated to introducing Bayesian estimation and forecasting procedures. The obtained estimation will  be discussed in a seventh section. A final section will outline this work's results and its implications.
	 
	\section{Literature Review}
	The RBC literature was the first to introduce structural models of time-invariant parameters. Its parameters are rooted in the optimising behaviour of representative agents and consequently convey their preferences. Preferences are thought off as time-invariant \cite{prescott_theory_1986}. The RBC literature further contributed in providing an argument on business cycle efficiency. Previously seen as inefficient divergence from the steady state, the RBC model suggests that in absence of nominal rigidities business cycles are driven by technology innovation and consequently are efficient adjustments towards the economy's steady state \cite{christiano_current_1992}. This notion of efficiency was subsequently questioned by New-Keynesian literature demonstrating that under nominal rigidities changes in the interest rate are not matched by immediate changes in expected inflation \cite{gali_monetary_2008}. Following the New-Keynesian argument business cycles are driven by interventionist monetary policy, causing a gap in agent expectation and thus inefficiency\cite{gali_monetary_2008}. Following the oil price induced stagflation crisis of the 1970s a rich body of economic literature has been dedicated to the effects of oil prices on inflation and the economy at large \cite{barsky_oil_2002, bernanke_systematic_1997}. The majority of these approaches were empiric. However, the growing DSGE literature was soon applied to questions of energy prices and inflation. In doing so two dominant approaches were pursued.
	
	Early analysis of oil prices and their interaction with the economy introduced petrol as an exogenous supply to the economy. As such models did not distinguish between crude oil and refined petrol imports. Instead, a unified petrol good enters the consumption basket consumption as well as the production function. The first such model was inspired by large differences in the inflationary response to oil price shocks \cite{blanchard_macroeconomic_2007}. Blanchard \& Galì (2007) extended the New-Keynesian model by petrol in order to reconcile these different responses under one framework \cite{blanchard_macroeconomic_2007}. Across this model imperfectly competitive firms optimise with regards to inputs of petrol and labour to their production. Moreover, petrol is consumed by households as a fixed share of overall consumption. The inflationary impact of exogenous petrol price is consequently propagated through the firm's elasticity of substitution between factors of production, namely petrol and \cite{blanchard_macroeconomic_2007}. As petrol gets more expensive production costs rise and firms substitute petrol for labour. This places the firms elasticity of substitution at the heart of petrol driven inflation. This first model has later been extended in complexity. It was shown that the introduction of capital accumulation amplifies the stagflationary effects of petrol on the economy \cite{acurio_vasconez_what_2015}. Advances in mainstream DSGE literature provided more sophisticated approaches to including wage rigidities, endowing households with wage setting power \cite{smets_shocks_2007}. This mechanism was then used to extend the model by including wage rigidities \cite{leduc_quantitative_2004}. With petrol entering into the consumption basket, households are directly concerned by higher petrol prices. Endowed with negotiation power they to some degree pass on higher prices to firms. This increases petrol's inflationary  pressure by broadening its channel of transmission \cite{leduc_quantitative_2004}.	This first approach to petrol was much focussed on the analysis of short-run shocks. Assumptions such as a fixed petrol share in consumption or an exogenous oil price are reasonable in the short-run, as prices surge exogenously. However, for analysing the long-run effects of petrol, outside of crisis-regimes this modelling approach lacked foundation. This lead to an extensions of the petrol modelling literature.
	
	With the adoption of DSGE modelling across many central banks complex models tailored to the conditions of national economies came about. Soon petrol was introduced as an important aspect to these models. Pioneered by the Bank of England, its small open economy model was extend by oil \cite{harrison_evaluating_2010, harrison_impact_2011}. Alongside this extension endogenous oil production was introduced \cite{harrison_impact_2011}. The model further relies on a distinction between crude oil and refined petrol, with the latter resulting from a separate sector of production. This is complemented by a utility sector, modelling the effect of gas prices on marginal electricity production \cite{harrison_impact_2011}. Both refined petrol and utilities factor into the consumption basket as well as into the domestic production sector. Consequently, the petrol inflation channel assumes a more complex dynamic, operating through production and household wage negotiation \cite{harrison_impact_2011}. Moreover, the possibility to trade has important implications as with rising petrol prices a substitution away from oil then allows for trading non-utilised domestic oil. Substitution thus not only avoids higher cost but turns into a source of revenue, which is an important aspect for long-run dynamics of petrol utilisation, hence inflation \cite{harrison_impact_2011}. Following the BoE much research has been conducted in tailoring models, containing amongst many other factors petrol, to the specific dynamics of national economies \cite{lees_introducing_2009, malakhovskaya_are_2014, hou_oil_2016}. 
	
	As this work's purpose is an analysis of petrol prices and their interaction with inflation it will rely on the model of Blanchard \& Galì (2008) \cite{blanchard_macroeconomic_2007}. This model focusses on the foundation of petrol inflation interaction while later approaches aim at a comprehensive analysis of the economy. In doing so more complex models confound many factors, not allowing for the clear cut analysis of the basic mechanism behind oil price inflation. This work aims at identifying the contribution of that mechanism and thus opts for simplicity. 
	
	\section{Real Business Cycle model}
	The Real Business Cycle model (RBC) was first introduced by Prescott (1986) \cite{prescott_theory_1986} attempting to explain the origin of business cycles. Departing from Schumpeterian technology induced growth, the RBC model is at its heart a de-trended growth model. It emphasises the link between technology growth to real wages and return to capital through increases in total factor productivity. Technology growth thus increases income, leading the economy to a new optimum \cite{prescott_theory_1986}. 
	In making this argument several assumption are required, namely: Perfect competition, flexible prices and the use of an identical production technology across firms. While the argument of efficient business cycles is antithetical to the New-Keynesian literature, who regards business cycles as inefficient, both share the same mathematical foundation. The following will therefore lay the foundation for the standard New-Keynesian model in deriving the RBC model.
	
	\subsection{The Household}
	Households are assumed to have homogeneous preferences and to solve a simultaneous optimisation problem. Given these properties the RBC introduces a representative agent solving the problem for a continuum of households.
	The agent is faced with an income from labour wage $W_t$, returns on last periods capital holdings $K_{t-1}$ and firm dividends $\Pi_t$. Income is divided between consumption, capital investment $I_t$ and leisure, while the leisure expenditure is implicitly made by choosing the number of labour hours $L_t$. The RBC assumes a Robinson-Crusoe economic setting in which the household owns firms and consequently receive profit dividends. However, under assumptions of perfect competition firm profits are zero and so are dividends at equilibrium \cite{prescott_theory_1986}. 
	Moreover, households are infinitely lived, thus solving their problem discounted into all future periods. Moreover, the utility function of households is assumed to take the following form.
	\begin{equation}
		U(C_t, L_t) = \frac{C_t^{1-\sigma}}{{1-\sigma}} - \frac{L_t^{1+\phi}}{1+\phi} 
	\end{equation}
	In this specification $\sigma$ is the inverse elasticity of intertemporal consumption substitution and $\phi$ is the marginal disutility to labour. While other utility specification have been employed across the RBC literature this work will remain with a separable utility function in order to be consistent with the standard New-Keynesian model. The function $U(C_t, L_t)$ exhibits the usual characteristics of a well behaved utility function $\frac{\partial U}{\partial C_t} > 0$, $\frac{\partial U}{\partial L_t} \leq 0$ and $\frac{\partial^2 U}{\partial C_t^2} \leq 0$, $\frac{\partial^2 U}{\partial L_t^2} \leq 0$.
	
	The household maximisation problem reads as follows, where investment factors into the capital stock and $K_t$ evolves over time based on the difference between quarterly inflows and depreciation $\delta \leq 1$.
	\begin{equation}
		\begin{aligned}
			\max_{B_t, C_t, L_t} \quad \sum_{\infty}^{t=0} \beta^{t} U(C_{t}, L_{t}) \\
			\textrm{s.t.} \quad C_t + I_t = R_t K_{t-1} + W_t N_t + \Pi_t \\
						  K_t = (1 - \delta) K_{t-1} + I_t \\
			\textrm{FOC:}\\
							 \quad - \frac{U_{n,t}}{U_{c, t}} = W_t \\
							 \quad	\E_t \left[ \frac{U_{c,t}}{U_{c,t+1}} \right]^{\sigma} = \beta \left( 1 - \delta + \E_t[R_{t+1}]\right)
		\end{aligned}
	\end{equation}
	The household optimisation yields two main conditions. The Euler equation describes the intertemporal consumption trade-off and the labour supply. Replacing the partial derivates of the utility function leads to the two main equations of the household's problem.
	\begin{equation} \label{eq:rbc_hh_foc}
		\begin{aligned}
			C_t^\sigma L_t^\phi	= W_t \\
			\E_t \left[ \frac{C_{t+1}}{C_t} \right]^\sigma = \beta \left(1 - \delta  + \E_t [R_{t+1}] \right)
		\end{aligned}
	\end{equation}

	\subsection{The firm}
	Just as households, firms are assumed to be identical in their production technology, wherefore the continuum of firms is replaced by a representative firm. The firm solves a static optimisation problem in choosing its production inputs capital and labour only for the period $t$. Due to the assumption of perfect competition and flexible prices the firms pays the marginal product to its production factors labour and capital. This is formalised below and yields the capital and labour demand equations.
	\begin{equation} \label{eq:rbc_firm_foc}
		\begin{aligned}
			\max_{N_t} \quad & A_t K_t^\alpha N_t^{1 - \alpha} \\
			\textrm{s.t.} \quad & Y_t = W_t N_t + K_t R_t\\
			\textrm{FOC:} \quad W_t = (1 - \alpha) A_t K_t^\alpha N_t^{-\alpha}\\
							\quad K_t = \alpha A_t K_t^{\alpha -1} N_t^{1-\alpha}
		\end{aligned}
	\end{equation}

	\subsection{Equilibrium}
	Having derived the above optimality condition an analysis of the equilibrium requires two further pieces. These are the equilibrium resource constraint arising from the output accounting identity $Y = I + C$ and an expression for technology $A_t$. The innovation of technology is assumed to be exogenous and path dependent, it is therefore modelled as a stochastic first order autoregressive process AR(1), where $\bar{A}$ represents the equilibrium value of technology.
	\begin{equation} \label{eq:rbc_eqil}
		\begin{aligned}
			Y_t = C_t + I_t \\
			\log(A_t) = (1- \rho_A) \log(\bar{A}) + \rho_A \log(A_{t-1}) + \epsilon_t^A
		\end{aligned}
	\end{equation}

	The derived system of equations \eqref{eq:rbc_eqil} \eqref{eq:rbc_firm_foc}, \eqref{eq:rbc_hh_foc} in its current state is non-linear. This is due to the multiplicative Cobb-Douglas production function and the evolution of capital as a state variable without full depreciation \cite{campbell_inspecting_1994}. In order to be able to analyse the system in its behaviour around the equilibrium as well as to link the system to data some form of linearisation is needed. This method of linearisation has significant impact the when relating models to real data (ref) (Taylor  Uhlig, 1990). The most common approach adopted across literature is a log-linear first order Taylor approximation \cite{campbell_inspecting_1994}. This work will follow suit.
% put into appendix
	Deriving the log-linearisation requires some additional reasoning which is provided in the \nameref{appc}.
	
	\begin{table}[H]
		\caption{RBC Equations}
		\fontsize{10pt}{10pt}\selectfont
		\centering
		\begin{tabular}{llr}
			\textbf{Equation name} & Equation & \textbf{Log-linear expression}\\
			\hline
			Euler equation &
			$\E_t \left[ \frac{C_{t+1}}{C_t} \right]^\sigma = \beta \left[ (1 - \delta)  + \E_t [R_{t+1}] \right]$ &
			$\hat{c}_t = \hat{c}_{t+1} - \hat{r}_{t+1}$ \\
			Capital supply & 
			$K_t = (1 - delta) K_{t-1} + I_t$ &
			$\hat{i}_t = \delta \hat{k}_{t+1} - \frac{1-\delta}{\delta} \hat{k}_t$ \\
			Capital demand &
			$\hat{y}_t - \hat{k}_t = \hat{r}_t$ &
			$K_t = \alpha A_t K_t^{\alpha -1} N_t^{1-\alpha}$ \\
			Labour supply & 
			$- \frac{U_{n,t}}{U_{c, t}} = W_t$ &
			$\hat{c}_t = \hat{w}_t - \frac{\bar{l}}{1-\bar{l}} \hat{l}_t$ \\
			Labour demand &
			 $W_t = (1 - \alpha) A_t K_t^\alpha N_t^{-\alpha}$ & 
			 $\hat{y}_t - \hat{l}_t = \hat{w}_t$ \\
			Production function &
			$Y_t = A_t K_t^\alpha N_t^{1 - \alpha}$  &
			$\hat{y}_t = \hat{a}_t + \alpha \hat{k}_t + (1-\alpha) \hat{l}_t$ \\
			Equilibrium condition &
			$Y_t = C_t + I_t$ &
			 $\hat{y}_t = \frac{\bar{y}}{\bar{c}} \hat{c}_t + \frac{\bar{i}}{\bar{y}} \hat{i}_t$ \\
			Technology &
			 $\log(A_t) = (1- \rho_A) \log(\bar{A}) + \rho_A \log(A_{t-1}) + \epsilon_t^A$ &
			 $\hat{a}_t = \rho_a \hat{a}_{t-1} + \epsilon_{a,t}$ \\
		\end{tabular}
	\end{table}
	
	The above linearisation still contains steady state values indicated as $\bar{x}$. In order to express the model as a linear system these have to be replaced with their respective deterministic values. However, the RBC model under separable consumption labour utility, does not possess a deterministic steady state for labour (ref). This issues has often been circumvented by relying on a logarithmic specification of household utility \cite{campbell_inspecting_1994} \footnote{In the work of Campbell the analytical derivation relies on a log-utility function}. However, the purpose of this work is a comparison between an RBC and New-Keynesian models. The latter traditionally rely on separable utility, wherefore this work will use separable utility for the RBC as well, in order allow for comparison on similar basis. In order to provide further analysis the below will express the steady state values as ratios of labour \cite{prescott_theory_1986}. The deterministic labour ratios for the RBC model are as below.
	
	\begin{table}[H]
		\fontsize{9pt}{9pt}\selectfont
		\centering
		\caption{RBC steady state}
		\begin{tabular}{lr}
			\textbf{Variable} & \textbf{Deterministic steady state}\\
			\hline 
			Rate of return & $\bar{R} = \Rss$ \\
			Capital labour ratio & $\frac{\bar{K}}{\bar{L}} = \KLss$ \\
			Wage & $\bar{W} = (1 - \alpha) \Ass \left(\KLss\right)^\alpha$ \\
			Investment labour ratio & $\frac{\bar{I}}{\bar{L}} = \delta \KLss$ \\
			Output labour ratio & $\frac{\bar{Y}}{\bar{L}} = \Ass \left(\KLss\right)^\alpha$ \\
			Consumption labour ratio & $\frac{\bar{C}}{\bar{L}} = \frac{\bar{Y}}{\bar{L}} - \frac{\bar{I}}{\bar{L}}$ \\
		\end{tabular}
	\end{table}

	In order to obtain a linear state-space representation of the RBC model a numerical approximation of the steady state for the value of labour $\bar{l}$ is required. This work relies on the software package gEconpy (ref) for this numerical approximation. The specification of the input file, can be found in the \nameref{appe}. The resulting steady-state values and the state-space transition matrix can be found in the \nameref{appb}. For each parameter specification the Blanchard-Kahn conditions have been checked using the software \cite{blanchard_solution_1980}.

	\section{NK} \label{sec:NK}
	The New-Keynesian model developed in this section is based on the work of \cite{gali_monetary_2008}. It diverges from the RBC model in three key aspects.
	Firstly, the NK model introduces nominal rigidities through monopolistic competition. Since firms are endowed with some market power, they can reset prices above marginal cost, introducing a gap between real and nominal marginal cost. This dynamic results in inflation. This extension has several consequences. For one monetary policy is neutral in the short-run, thus changes in interest are note directly matched by changes in expected inflation \cite{gali_monetary_2008}. This gap in expectations becomes the driver of short-run fluctuation in the economy. Business cycles is thus no-longer driven by technological progress but result from an adjustment of the interest rates. The standard New-Keynesian model thus presents a perspective in which the intervention of a monetary authority leads to inefficient short-term adjustments of the economy. The NK returns Keyne's perspective, regarding regarded business cycles as inefficient. The the main divergence from RBC literature consists in this change of perspective.
	
	Secondly, the NK model in its simplest form analyses an economy without capital. The capital stock is thus no longer the intertemporal instrument. Instead the NK model uses the gap between actual and nominal output, where nominal refers to the output without price frictions \cite{gali_monetary_2008}.
	
	\subsection{Consumption bundle}
	The New-Keynesian framework introduces imperfectly substitutable goods as source of market power in order to introduce monopolistic competition. This results in the firms ability to set a price beyond marginal cost for their good of production. This implies that there is not one single good but a continuum of differentiated goods. Consequently, the households allocation problem becomes twofold.
	For one households need to decide on the basket of products that optimises their utility. Only once the optimal basket and its price is determined the household is able decide on the optimal allocation of income across consumption and leisure. The consumption basket composition thus needs to be solved prior to the consumption-leisure trade-off. 
	Across the DSGE literature the aggregation of consumption is also sometimes referred to as the output of a good bundling firm. This intermediary party determines the optimal basket of goods according to the household preferences. In a perfectly competitive environment the household buys the optimal bundle at no mark up beyond the initial producer's mark-up.
	Determining the optimal basket requires some function of aggregation across the continuum of goods. The most commonly employed aggregator is the Dixit-Stiglitz function (ref). This aggregator has appealing properties, namely a constant elasticity of substitution (CES), which will become important later in deriving marginal cost.
	
	The bundling optimisation problem reads as follows where each good $C_{it}$ is matched by its price $P_{it}$ purchased with budget $Z_t$.
	\begin{equation}
		\begin{aligned}
			\max_{C_{it}}
			\quad \left(\int_{0}^{1} C_{it}^{ \frac{\epsilon - 1}{\epsilon} } di 
			\right)^{ \frac{\epsilon}{\epsilon - 1} }\\
			\textrm{s.t.}\\
			\quad \int_{0}^{1} P_{it} C_{it} di \leq Z_t
		\end{aligned}
	\end{equation}
	Solving the above optimisation problem results in three important expression. First, the aggregated price level $P_t$ of the optimal consumption bundle is derived. This corresponds to the consumer price index (CPI).
	\begin{equation} \label{eq:1}
			P_t = \left(
						\int_{0}^{1} P_{it}^{ 1 - \epsilon } di 
					\right)^{ \frac{1}{1 - \epsilon} }
	\end{equation}
	Further manipulation leads to the share of product $C_{it}$ in the consumption basket $C_t$. This share is defined by the goods price relative to the price level $P_t$ and the CES between goods $\epsilon$.
	\begin{equation} \label{eq:cshare}
		C_{it} = \left( \frac{P_{it}}{P_t} \right)^{- \epsilon} C_t
	\end{equation}
	Lastly, using the two above it can be demonstrated that under a binding budget constraint individual good prices correspond to the CPI times the optimal consumption bundle.
	This property is important as it allows to solve the household and firm optimisation problem as a representative agent problem.
	\begin{equation}
		\int_{0}^{1} P_{it} C_{it} di = Z_t = P_t C_t
	\end{equation}

	\subsection{Household}
	Having found $C_t$ and its price $P_t$ the household's consumption-leisure allocation can be formulated. The derivation follows from the RBC except for investment expenditure and the extension by bond holdings $B_t$. While $B_t$ is zero in equilibrium they introduce the nominal interest rate into the households problem. The nominal interest rate is contained in the discount factor $Q_t = \frac{1}{1+i_t}$ and the first step to introducing a monetary authority. Similar to the RBC a representative infinitely lived agent solves the optimisation problem for all future periods. 
	\begin{equation}
		\begin{aligned}
			\max_{B_t, C_t, L_t} \quad \sum_{\infty}^{t=0} \beta^{t} U(C_{t}, L_{t}) \\
			\quad P_t C_t + Q_t B_t = B_{t-1} + W_t N_t \\
			\textrm{FOC:} \\
				- \frac{U_{n,t}}{U_{c, t}} = \frac{W_t}{P_t}\\
				Q_t = \beta \E_t \left[ \frac{U_{c, t+1}}{U_{c,t}}^\sigma \frac{P_t}{P_{t+1}} \right] 			
		\end{aligned}
	\end{equation}
	Solving the optimisation problems yields the labour supply. In the NK it explicitly depends on the real wage. Moreover, as in the and the Euler equation is obtained. Assuming the function form of $U(C_t, L_t) = \frac{C_t^{1-\sigma}}{{1-\sigma}} - \frac{L_t^{1+\phi}}{1+\phi}$ and plugging its derivatives into the first order condition yields.
		\begin{equation} \label{eq:nk_hh_foc}
		\begin{aligned}
			C_t^\sigma L_t^\phi	= \frac{W_t}{P_t} \\
			Q_{t} = \E_t \left[ \frac{C_{t+k}}{C_t} \right]^\sigma \frac{P_t}{P_{t+1}}
		\end{aligned}
	\end{equation}
	
	\subsubsection{Price setting}
	The main innovation of the NK model lies in modelling inflation. As mentioned in the above inflation arises from the imperfect substitutability of consumption goods. This results in monopolistic competition endowing firm's price setting power. Modelling the price setting process is thus at the heart of the NK model. In doing so the standard model relies on the Calvo-pricing mechanism (ref). Firms are able to reset prices in any period with the probability $\theta$. This is equal to saying that every period a share $\theta$ of firms can reset prices to their optimal price $\hat{P}_t$ while $1-\theta$ firms have to remain with last periods price $P_{t-1}$. This allows to describe the evolution of the price level as an in the following intertemporal equation, which if divided by $P_{t-1}$ yields an expression of inflation.
	\begin{equation}
		P_t = 
		\left[
		\theta P_{t-1}^{1 - \epsilon} + (1 - \theta) \hat{P}_t^{1 - \epsilon}
		\right]^{\frac{1}{1 - \epsilon}}
	\end{equation}

	\begin{equation} \label{eq: inlfation}
		\Pi_t^{1-\epsilon} = \theta + (1 - \theta) \left(\frac{\hat{P}_t}{P_{t-1}} \right)^{1-\epsilon}
	\end{equation}
	The key aspect to price evolution thus lies in the optimal price $\hat{P_t}$. The further it is away from last periods price, the higher is inflation.
	
	\subsubsection{The firm}
	In the RBC the firm solved a static optimisation problem providing labour and capital demand. In the NK model they are instead faced with setting the optimal price $\hat{P}_t$. For this the NK model assumes that a continuum of firms each producing one good provide factoring into the continuum of goods as outlined in (section). Setting the optimal price becomes an intertemporal problem as firms trade-off between two opposing forces. 	
	As illustrated by (ref) the share of any good in the overall basket depends on its price relative to the CPI. This implies that setting a price far above $P_t$ leads to a lower demand for a firm's single good $C_{i,t}$. 
	On the other hand, as firms are limited in their price frequency they need to set a price which they would be comfortable with in the future. Under the Calvo-scheme a price once set will be unchanged with probability $\theta^k$ for $k$ periods. Consequently, if the price once set is too low it will at some point allow for no further profits potentially even incur losses. 
	Firms thus try to set a price that allows them an acceptable share in the basket of goods while also guaranteeing profitability across future periods. In this decision the size of $\theta \in [0,1]$ combined with the time discount factor $Q_t \beta$ weights the second force against the first. 
	The problem can be formalised as a future discounted sum of revenue minus total cost. The notation of $Y_{t+k|t}$ hereby refers to the income in period $t+k$ at price of period $t$. The notation of $Q_{t,t+k}$ is the stochastic discount factor of today's discount carried forward into period $t+k$. The budget constraint makes use of the fact that under equilibrium assumption all output is consumed as part of the consumption basket $Y_{it} = C_{it}$. (section) will deliver a more in-depth perspective on this.	
	\begin{equation}
		\begin{aligned}
			\max_{\hat{P}_t}
			\quad
			\sum_{k=0}^{\inf} \theta^k \E_t 
			\left[
			Q_{t, t+k} 
			\left(
			\hat{P}_t Y_{t+k|t} - TC_{t+k|t}^n(Y_{t+k|t})
			\right)
			\right] \\
			\textrm{s.t.}
			\quad
			Y_{it+k|t} = \left(\frac{\hat{P}_t}{P_{t+k}} \right)^{-\epsilon} C_{t+k} \\
			\textrm{FOC:} \quad
			\sum_{k=0}^{\infty} \theta^k \E_t 
			\left[
			Q_{t,t+k} Y_{t+k|t} 
			\left(
			\hat{P}_t - \frac{\epsilon}{1 - \epsilon} MC_{t+k|t}^n
			\right)
			\right]
			= 0
		\end{aligned}
	\end{equation}
	
	Inserting the budget constraint and the discount factor as defined by the Euler condition \eqref{eq:nk_hh_foc} results in:
	\begin{equation}
		\frac{\hat{P}_t}{P_t} = \frac{\epsilon}{\epsilon-1} 
		\frac{
		\E_t \sum_{k=0}^{\infty} \theta^k \beta^k C_{t+k}^{1-\sigma} (\frac{P_{t+k}}{P_t})^\epsilon MC_{t+k}^r
		}{
		\E_t \sum_{k=0}^{\infty} \theta^k \beta^k C_{t+k}^{1-\sigma} (\frac{P_{t+k}}{P_t})^{\epsilon-1}
		}
	\end{equation}
	The above describes the mark-up of the optimal price $\hat{P}_t$ beyond the price level CPI. The difference between the two directly drives inflation as it implies a higher price level as can be seen in \eqref{eq: inflation}. 
	In the steady state inflation is zero by definition, implying that no firm resets prices. This is equivalent to saying that $\theta=0$. Multiplying the above by $P_t$ and setting $\theta=0$ results in $\hat{P}_t = \frac{\epsilon}{\epsilon-1} MC_t^n$. This allows to derive an important aspect to the NK model, the price-mark up over nominal marginal cost in the steady state. Firms are endowed with market power, allowing them to set a mark up above marginal cost. The real marginal defined as $MC_t^r \equiv \frac{MC_t^n}{P_t}$ cost can thus be derived. It is important to note that $MC^r$ are the real marginal cost in the steady state, wherefore they are time independent. They are complemented by $MC_t^r$, the time depended marginal cost, which can deviate from their steady state value leading to the gap of $\hat{MC}_t^r \equiv MC_t^r - MC^r$.
	\begin{equation}
		MC^r = \frac{\epsilon}{\epsilon-1}
	\end{equation} 
	
	\subsubsection{Equilibrium conditions}
	To this point a number of optimal choices have been derived. The households decides on the consumption basket and consumption-leisure trade-off. The firm sets the optimal price and consequently mark-up over nominal marginal costs, yielding an expression for inflation. Having derived the optimality conditions for all agents the model now requires market clearing condition to describe their interaction. This is based on accounting identity on individual good and production level $Y_{it} = C_{it}$. In order to close the model one needs to demonstrate, that this property on individual goods level translates in the global property $Y_t = C_t$.
	
	Output $Y_t$ is defined as an aggregator function. The accounting identity combined with \eqref{eq: cshare} allows to link $Y_t$ to the the price level, allowing to demonstrate that $Y_t = C_t$ holds.
	\begin{equation}
			Y_t = 
			\left( 
				\int_{0}^{1} Y_{it}^{\frac{\epsilon - 1}{\epsilon}} di 
			\right)^{\frac{\epsilon}{\epsilon - 1}}
			=
			\left( 
			\int_{0}^{1} 
			\left[
			\left( \frac{P_{it}}{P_t} \right)^\epsilon C_t
			\right]^{\frac{\epsilon - 1}{\epsilon}} di 
			\right)^{\frac{\epsilon}{\epsilon - 1}}
			=
			P_t^{\epsilon} C_t P_t^{-\epsilon}
			=
			C_t
	\end{equation}
	A similar reasoning is required in order to derive an economy wide production function an expression from the firm's individual production functions $Y_{it} = A_t N_{it}$. This yields the below expression:
	\begin{equation}
		N_t = 	
		\int_{0}^{1} \left( \frac{Y_t}{N_{it}} \right)^{\frac{1}{1 - \alpha}}
		\int_{0}^{1} \left( \frac{P_{it}}{P_t} \right)^{-\frac{\epsilon}{1 - \alpha}} di
	\end{equation}
	For this expression it can be shown that up to a second order approximation $\int_{0}^{1} \left( \frac{P_{it}}{P_t} \right)^{-\frac{\epsilon}{1 - \alpha}} \approx 1$, wherefore labour can be expressed as \cite{gali_monetary_2008}:
	\begin{equation}\label{eq: Nt}
		N_t = \int_{0}^{1} \left( \frac{Y_t}{N_{it}} \right)^{\frac{1}{1 - \alpha}}
	\end{equation}
	
	\subsubsection{Log linearisation}
	This completes the number of equations required for the derivation of the New Keynesian model. As the above equations are a non-linear system without a closed form solution some method of linearisation is required. The below will follow the procedure of \cite{gali_monetary_2008} and apply log-linearisation first order Taylor approximations. Throughout the log-linearisation some further reasoning about is needed fro the Euler equation, which can be found in \nameref{appc}. Lower-case expressions are the log-equivalent to previous upper case variables. \\
	
	Euler equation
	\begin{equation} \label{eq:lleuler}
		c_t = \E_t[c_{t+1}] - \frac{1}{\sigma} (i_t - \rho - \E_t [\pi_{t+1}])
	\end{equation}
	Labour supply 
	\begin{equation} \label{eq:llslabour}
		w_t - p_t = \sigma c_t + \phi n_t
	\end{equation}
	Inflation
	\begin{equation}\label{eq:llpi}
		\pi_t = (1 - \theta) (\hat{p}_t - p_{t-1})	
	\end{equation}
	Optimal price 
	\begin{equation}\label{eq:llpstar}
		\hat{p}_t = (1 - \theta \beta) \E_t
		\left[
		\sum_{k=0}^{\infty} \theta^k \beta^k \left( mc_{t+k|t}^r - mc^r +p_{t+k}\right)
		\right]		
	\end{equation}
	Production function
	\begin{equation}\label{eq:llprod}
		y_t = a_t + (1 - \alpha) n_t
	\end{equation}
	Equilibrium condition 
	\begin{equation}\label{eq:lleq}
		y_t = c_t
	\end{equation}

	\subsubsection{New Keynesian Phillips and IS curve}
	
	The next step is the derivation of an expression for real marginal cost $mr_{t}^r$ contained in the optimal price setting. In the absence of other production factors the marginal product of labour $MPN_t = \frac{\partial Y}{\partial N} = A_t (1- \alpha) N_t^{-\alpha}$ corresponds to nominal marginal cost $mc_t^n$. Real marginal cost can thus be obtained from $MC_t^r = \frac{W_t}{P_t MCN_t}$. This allows to formulate the below property in logs by including the production function \eqref{eq:llprod}.
	\begin{equation}
		\begin{aligned}
			mc_t^r = w_t - p_t - a_t - mpn_t \\
%			mc_t^r = w_t - p_t - a_t - \ln(1 - \alpha) + \alpha n_t \\
			mc_t^r = w_t - p_t - \frac{a_t - \alpha y_t}{1 - \alpha} - \ln(1 - \alpha) \\
		\end{aligned}
	\end{equation}
	This expression is at the provides the basis for two essential conclusions. 
	Firstly, it allows to derive a relation between $mc_t^r$ and output $y_t$. Using labour supply \eqref{eq:llslabour} and the production function \eqref{eq:llprod} one can rephrase the above as. 
	\begin{equation}
		\begin{aligned}
			mc_t^r = \sigma c_t + \phi n_t - [a_t + \alpha n_t + \ln(1-\alpha)] \\
			mc_t^r = 
			\frac{
				\sigma (1 - \alpha) + \phi + \alpha
			}{
				(1 - \alpha)	
			}	 y_t
			- \frac{
				(1 + \phi)	
			}{
				(1 - \alpha)	
			} a_t
			+ \ln(1-\alpha)
		\end{aligned}
	\end{equation}
	In the steady state marginal cost should be equal to its natural level and $mc_t^r = mc^r$. In this case the above describes natural output $y_t^n$. This fact can be used to derive an expression for the gap between marginal cost and its steady state value in logs $\hat{mc_t}^r \equiv = mc_t^r - mc^r$. In doing so one 
	Reformulating the above by replacing $mc_t^r$ by $mc^r$ yields an expression of the output gap in terms of the marginal cost gap. 
	\begin{equation} \label{eq:llmcrhat}
		\begin{aligned}
			\hat{mc_t}^r = \frac{\sigma (1 - \alpha) + \phi + \alpha}
			{1 - \alpha)} (y_t - y_t^n)
		\end{aligned}
	\end{equation}
	The second insight arising from the real marginal cost is its translation in period $t+k$. Combining this with the share of consumption good $i$ in the overall basket as well as the market clearing condition \eqref{eq:lleq} logs gives 
% mc_{t+l}^r = w_{t+k} - p_{t+k} - \frac{a_ {t+k} - \alpha y_{i,t+k|t}}{1 - \alpha} - \ln(1 - \alpha)
% check this
	\begin{equation}
		y_{t+k|t} = -\epsilon(p_{t+k|t} - p_{t+k}) + y_{t+k}
	\end{equation}
	Combining the two yields an expression of marginal cost and the price level.
	\begin{equation}
		mc_{i, t+k|t}^r = mc_{t+k}^r - \frac{\epsilon \alpha}{1 - \alpha}(\hat{p}_t - p_{t+k})
	\end{equation}
	The above combined with the expression for the optimal price level.
% more math??	
	\begin{equation}
		\begin{aligned}
			\hat{p}_t - p_{t-1} =
			\quad
			(1 - \theta \beta) \E_t
			\left[
			\sum_{k=0}^{\infty} \theta^k \beta^k \left( mc_{t+k|t}^r - mc^r +p_{t+k} - p_{t-1}\right)
			\right] \\			
			=
			\quad
			(1 - \theta \beta) \Theta \E_t
			\sum_{k=0}^{\infty} \theta^k \beta^k (mc_{t+k}^r - mc^r) + 
			\sum_{k=0}^{\infty} \theta^k \pi_{t+k} \\
			=
			\quad
			\theta \beta \E_t (\hat{p}_{t+1} - p_{t}) + (1 - \theta \beta) \Theta (mc_{t+k}^r - mc^r) + (1 - \theta) (\hat{p}_t - p_{t-1})
		\end{aligned}		
	\end{equation} 

	This can be rewritten in more compact form as \footnote{where $\Theta \equiv \frac{1 - \alpha}{1 - \alpha + \alpha \epsilon} \leq 1$} \footnote{	where $\lambda \equiv \frac{(1-\theta)(1-\beta\theta)}{\theta} \Theta$}
	\begin{equation}
		\pi_t = \beta E_t [\pi_{t+1}] + \lambda \hat{mc}_{t}
	\end{equation}

	The above combined with \eqref{eq:llmcrhat} allows to construct the New-Keynesian Phillips curve, on of the two core equations of the NK model.\footnote{where $\kappa = \lambda \frac{\sigma (1 - \alpha) + \phi + \alpha}{1 - \alpha)}$}
	\begin{equation} \label{eq:llnkp}
		\begin{aligned}
			\pi_t = \beta E_t [\pi_{t+1}] + \kappa \tilde{y}_t
		\end{aligned}
	\end{equation}
	
	The second core equation of the New-Keynesian model is derived from the Euler equation \eqref{eq:lleuler}, the equilibrium condition \eqref{eq:lleq} and the log linear Fisher identity $r_t \equiv i_t - \E_t[\pi_{t+1}]$. Inserting $y_t$ for $c_t$ and introducing the difference between output and its natural level as $\tilde{y}_t \equiv y_t - y_t^n$ allows to write
	\begin{equation}
		\tilde{y}_t = 
		\left[
		\E_t[y_{t+1}] - \frac{1}{\sigma} (i_t - \rho - \E_t[\pi_{t+1}])
		\right]
		-
		\left[
		\E_t[y_{t+1}^n] - \frac{1}{\sigma} (r_t^n - \rho)
		\right]
	\end{equation}
	This provides the New-Keynesian IS curve.
	\begin{equation} \label{eq:llnkis}
		\tilde{y}_t = \E_t[\tilde{y}_{t+1}] - \frac{1}{\sigma} (i_t - r_t^n - \E_t[\pi_{t+1}])
	\end{equation}
% discussion of IS and P curve
	\subsubsection{Monetary policy}
	Closing the model requires an interest rate rule, describing the behaviour of the central bank. This work will rely on a traditional Taylor rule with an exogenous interest rate shock following \cite{gali_monetary_2008}.
	\begin{equation}
		\begin{aligned}
			i_t = \rho + \phi_{\pi} \pi_t + \phi_{y} \tilde{y}_t + v_t \\
			v_t = \rho_v v_{t-1} + \epsilon_t^v
		\end{aligned}
	\end{equation}

	This model has been implemented using gEconpy, where the log-linear model as provided to the simulation programme can be found in \nameref{appe}. The system's stability has been assessed according to the Blanchard-Kahn conditions of stability, based on the eigenvalues of the transition matrix \cite{blanchard_solution_1980}. These have been checked for each parameter specification.

	\section{NKE}
	The below is an extension of the standard NK model by petrol as a consumption good as well as factor of production. There is no domestic production of petrol, instead it is imported and its price is assumed to be exogenous. Beyond the import of petrol there is no foreign trade, all closed economy assumptions thus hold. 
	Given the exogenous nature of petrol the model is quite similar to the standard NK model. The below will therefore depart from (section) and only outline the changes to the model from petrol production. 	
	
	\subsection{Bundler}
	Petrol is considered a consumption good and as such enters into the good basket $C_t$. This inclusion is modelled across the bundling process, deriving the optimal basket across two classes of goods, domestic and foreign. 
	The bundling of domestic goods follows from the same procedure as outlined in (section). Variables $C_{q,t}$ and $P_{q,t}$ are aggregated consumption and price respectively. The optimisation across the continuum of goods results in the same properties as above. These are share of good $C_{i,q,t}$ in the consumption basket $C_{q,t}$ and the aggregated price index $P_{q,t}$ as well as an expression of aggregate consumption. \\
	\begin{equation} \label{eq:o_pindex}
		P_{q,t} = \left( \int_{0}^{1} P_{i,q,t}^{1 - \epsilon} di \right)^{\frac{1}{1-\epsilon}} \\
	\end{equation}
	\begin{equation} \label{eq:o_cshare}
		C_{i,q,t} = \left( \frac{P_{i,q,t}}{P_{q,t}} \right)^{-\epsilon} C_{q,t}
	\end{equation}
	\begin{equation} \label{eq:o_pcon}
		\int_{0}^{1} P_{it} C_{it} di = Z_t = P_t C_t
	\end{equation}
	Foreign goods consumption $C_{m,t}$ exclusively consists of petrol and its share in overall consumption $C_t$ is assumed to be constant. Parameter $\chi$ represents this share. 
	This assumption, though a multiplication, is viable as consumption demand for petrol is relatively inelastic in the short-run. Empirical evidence suggests that due to the inability replace durable goods (e.g. cars) that usually are at heart of consumption demand this demand is inelastic (ref). Overall consumption this is defined as \footnote{where $\Theta = \chi^{-\chi}(1-\chi)^{(\chi-1)}$}
	\begin{equation}
		C_t \equiv \Theta_\chi C_{m,t}^\chi C_{q,t}^{1-\chi}
	\end{equation}
	From this the overall price level of the consumption basket $P_{c,t}$ is derived, introducing $S_t \equiv \frac{P_{m,t}}{P_{q,t}}^\chi$ the real price of oil.
	\begin{equation}
		P_{c,t} \equiv P_{m,t}^\chi P_{q,t}^{1-\chi} 
			= P_{q,t} \frac{P_{m,t}}{P_{q,t}}^\chi
			= P_{q,t} S_t^\chi
	\end{equation}
		
	Following this reasoning and using \eqref{eq:o_pcon} overall consumption can be written as 
	\begin{equation}
		C_t P_{c,t} = C_{m,t}P_{m,t} + C_{q,t}P_{q,t}
	\end{equation}

	\subsection{Household}
	Having derived expressions for the consumption basket and its price the household's optimisation problem can be solved. As in the standard NK model this reads as follows. 
% log-utility	
	\begin{equation}
		\begin{aligned}
			\max_{B_t, C_t, L_t} \quad \sum_{\infty}^{t=0} \beta^{t} U(C_{t}, L_{t}) \\
			\textrm{s.t.} \quad P_{c,t} C_t + Q_t B_t = B_{t-1} + W_t N_t \\
			\textrm{FOC:} \\
			- \frac{U_{n,t}}{U_{c, t}} = \frac{W_t}{P_{c,t}}\\
			Q_t = \beta \E_t \left[ \left( \frac{U_{c, t+1}}{U_{c,t}} \right)^\sigma \frac{P_{c,t}}{P_{c,t+1}} \right] 			
		\end{aligned}
	\end{equation}
	
	The authors use a log-utilty function $U(C_t,L_t) \equiv \log(C_t) - \frac{N_t^(1+\phi)}{1+\phi}$ which results in the following first order conditions.
	\begin{equation}
		\begin{aligned}
			Q_t = \beta \E_t \left[ \frac{C_t}{C_{t+1}} \frac{P_{c,t}}{P_{c,t+1}} \right] \\
			\frac{W_t}{P_{c,t}} = C_t N_t^\phi
		\end{aligned}
	\end{equation}

	\subsection{The Firm}
	Petrol serving as a second factor of production the firm's optimisation problem is extended from the standard NK model. As in the RBC the firm obtains its production factor demand by equalising their marginal cost.  
	\begin{equation}
		\begin{aligned}
%			\max_{M_{i,t}} \quad A_t M_{i,t}^{\alpha_m}
			\max_{M_t} \quad & A_t M_{it}^{\alpha_m} N_{it}^{\alpha_n} \\
			\textrm{s.t.} \quad & Y_{i,q,t} = W_t N_{i,t} + M_{i,t} P_{m,t}\\
			\textrm{FOC:} \\
			\quad & W_{i,t} = \alpha_n A_t M_{i,t}^{\alpha_m} N_{i,t}^{\alpha_n -1}\\
			\quad & K_{i,t} = \alpha_m A_t M_{i,t}^{\alpha_m -1} N_{i,t}^{\alpha_n}
		\end{aligned}
	\end{equation}
	The real marginal cost can thus be expressed at the equality of marginal input cost of labour and petrol. This constitutes a deviation from the standard NK model,  The the firms nominal marginal cost $MC_{t}^n$ is therefore.
	\begin{equation}
		MC_t^n = \frac{W_{i,t}}{\alpha_n Y_{i,q,t} N_{i,t}^{-1}} = \frac{P_{m,t}}{\alpha_m Y_{i,q,t} M_{i,t}^{-1}}
	\end{equation}
	Having obtained nominal marginal cost an expression for the firm's mark-up can be defined as $MC_{i,t}^r \equiv \frac{P_{q,t}}{MC_{i,t}^n}$. With some further manipulation \cite{blanchard_macroeconomic_2007} uses this to obtain the demand for petrol.
	\begin{equation}
		\begin{aligned}
		MC_{i,t}^r \frac{P_{m,t}}{P_{q,t}} M_{i,t} = \alpha_m Q_{i,t} \frac{P_{i,q,t}}{P_{q,t}} \\
		M_{i,t} = \alpha_m \frac{Q_{i,t}}{S_t MC_{i,t}^r} \frac{P_{i,q,t}}{P_{q,t}}
	\end{aligned}
	\end{equation}

	\subsection{Price setting}
	The price setting problem of the firm is equivalent to the price setting in the standard NK model. It diverges in only one notion, the real marginal cost $MC_t^r$. \\
	
	The expression for inflation is obtained as in the standard model where the ratio $\Pi_{q,t} = \frac{P_{q,t}}{P_{q,t-1}}$ provides \ac{CPI} inflation.
	\begin{equation}
		P_{q,t} = 
		\left[ 
		\theta P_{q,t-1}^{1 - \epsilon} + (1 - \theta) \hat{P}_t^{1 - \epsilon}
		\right]^{\frac{1}{1 - \epsilon}}
	\end{equation}
	The firm solves the same optimisation problem as in the above. 
	\begin{equation}
		\begin{aligned}
			\max_{\hat{P}_t}
			\sum_{k=0}^{\inf} \theta^k \E_t 
			\left[
			Q_{t, t+k} 
			\left(
			\hat{P}_t Y_{t+k|t} - TC_{t+k|t}^n(Y_{t+k|t})
			\right)
			\right] \\
			\textrm{s.t.}\\
			\quad
			Y_{it+k|t} = \left(\frac{\hat{P}_t}{P_{t+k}} \right)^{-\epsilon} C_{t+k}
		\end{aligned}
	\end{equation}
	with the first order condition 
	\begin{equation}
		\begin{aligned}
			\sum_{k=0}^{\infty} \theta^k \E_t 
			\left[
			Q_{t,t+k} Y_{q,t+k|t} 
			\left(
			\hat{P}_{q,t} - \frac{\epsilon}{1 - \epsilon} MC_{t+k|t}^n
			\right)
			\right]
			= 0
		\end{aligned}
	\end{equation}

	\subsection{Equilibrium}
	The equilibrium conditions follow the standard NK model in assuming the resource constraint $Y_{i,q,t} = C_{i,q,t}$ on individual firm level. As showed in (section) of the standard NK model overall domestic output can be linked to consumption of domestic goods: 
	\begin{equation}
		Y_{q,t} = \left( \int_{0}^{1} Y_{it}^{1-\frac{1}{\epsilon}} di \right)^{\frac{\epsilon}{\epsilon - 1}} = C_{q,t}
	\end{equation}
	Using this fact and the resource constraint casted into the \eqref{eq:o_cshare}, resulting in $Y_{q,t} = \frac{P_{i,q,t}}{P_{q,t}}$ allows to derive an expression for oil as a production factor solely depending on domestic output.
	\begin{equation} \label{eq:oil_prod}
		M_t = \frac{\alpha_m Y_{q,t}}{MC_{t}^r S_t}
	\end{equation}
	Using \eqref{eq:oil_prod} and the fact that bond holdings are zero in equilibrium allows to derive an expression for the overall price level, factoring out petrol:
	\begin{equation}
		P_{c,t}C_t = P_{q,t} C_{q,t} - P_{m,t}M_t = \left(1 - \frac{\alpha_m}{\xi_t^n} \right) P_{q,t} Y_{q,t}
	\end{equation}
	To this point the model is closed and one could proceed to log-linearisation. However, domestic output $Y_{q,t}$ does not correspond to overall output as it does not account for the petrol consumption by the economy. The model in its current form can thus not be linked to data as the case with the NK model. The solution proposed by Blanchard \& Galì (2007)  is to define a GDP deflator $P_{y,t}$ \cite{blanchard_macroeconomic_2007}, allowing them to obtain overall GDP $Y_t$ from domestic output.
	\begin{equation}
		\begin{aligned}
		P_{q,t} \equiv P_{y,t}^{1-\alpha_m} P_{m,t}^{\alpha_m}
		P_{y,t} Y_t \equiv P_{q,t} Y_{q,t} - P_{m,t} M_t = \left( 1 - \frac{\alpha_m}{\xi_t^n} \right) P_{q,t} Y_{q,t}
		\end{aligned}
	\end{equation}
		
	\subsubsection{Log linearisation}
	The equations derived in the above can be log-linearise into the following form. The procedure largely follows that of the standards NK model. 	
	\begin{table}
		\centering
		\fontsize{8pt}{8pt}
		\caption{New-Keynesian Energy model log-linear equations}
		\begin{tabular}{llr}
			Equation name & Equation & Log-linear Equation \\
			\hline
			 Euler equation & 
			 $Q_t = \beta \E_t\left[ \frac{C_{q,t}}{C_{q,t+1}} \frac{P_{q,t}}{P_{q,t+1}} \right]$ &
			 $c_t = \E_t [c_{t+1}] - (i_t - \E_t[\pi_{q,t+1}] - \rho)$ \footnotemark \\
			 
			 Labour supply &
			 $\frac{W_t}{P_{c,t}} = C_t N_t^\phi$ &
			 $w_t - p_t = c_t + n_t \phi$ \\
			 
			 Oil demand &
			 $M_t = \frac{\alpha_m Y_{q,t}}{\xi_t^n S_t}$ &
			 $m_t = -\mu_t^n - st + y_{q,t}$ \footnotemark \\
			 
			 Production &
		%A_t M_t^{\alpha_m} N_t^{\alpha_n} = 
			 $Y_{q,t} = A_t N_t^{\alpha_n} \left( \frac{\alpha_m Y_{q,t}}{\xi_t^n S_t} \right)^{\alpha_m}$ &
			 $y_{q,t} = \frac{1}{1-\alpha_m} (a_t + \alpha_n n_t - \alpha_m s_t - \alpha_m \mu_t^n)$ \\
			 
			 Gross output & 
			 $P_{c,t}C_t = (1-\frac{\alpha_m}{\xi_t^n})P_{q,t}Y_{q,t}$ &
			 $c_t = y_{q,t} - \chi s_t + \eta \mu_t^n$ \footnotemark \\
			 
			 GDP deflator &
			 $P_{q,t} \equiv P_{y,t}^{1-\alpha_m} P_{m,t}^{\alpha_m}$ &
			 $p_{y,t} = p_{q,t} - \frac{\alpha_m}{1-\alpha_m}s_t$ \\
			
			GDP &
			$P_{y,t}Y_t = \left(1-\frac{\alpha_m}{\xi_t^n}\right) P_{q,t}Y_{q,t}$ &
			$y_t = y_{q,t} + \frac{\alpha_m}{1-\alpha_m}s_t + \eta \mu_t^n$ \\
			
			Inflation & 
			$\left[\frac{P_{q,t}}{P_{q,t-1}}\right]^{-\epsilon}= \theta + (1 - \theta) \pi_t$ &
			$\pi_t = (1-\theta)(\hat{p}_t - p_{t-1})$ \\
			
			Price setting &
			$\sum_{k=0}^{\infty} \theta^k \E_t 
			\left[
			Q_{t,t+k} Y_{q,t+k|t} 
			\left(
			\hat{P}_{q,t} - \frac{\epsilon}{1 - \epsilon} MC_{t+k|t}^n
			\right)
			\right]
			= 0$ &			
			$\pi_{q,t} = \beta E_t [\pi_{q,t+1}] + \lambda \hat{mc}_{t}$ \\
			\hline
		\end{tabular}
	\end{table}
% fix footnotes
	\footnotetext{where $\rho \equiv - \ln(\beta)$ and $i_t = \ln(Q_t)$}
	\footnotetext{where $\mu_t^n \equiv \ln(\xi_t^n)$}
	\footnotetext{where $\eta \equiv \frac{\alpha_m}{\xi_t^n-\alpha_m}$}

	Combining the expressions for $y_t$, $c_t$ and $m_t$ through the mark-up $\mu_t^n$ a simplified expression for output and consumption can be derived, In doing so Blanchard \& Galì (2008) make the assumption that steady-state mark-up $mc^r$ is small enough so that $\frac{alpha_m}{mc^r - \alpha_m}-\frac{alpha_m}{a - \alpha_m} mc_t^r \approx 0$. This yields the below expressions for output and consumption.
	\begin{equation}
		\begin{aligned}
			y_t = \frac{1}{1-\alpha_m} (a_t + \alpha_n n_t) \\
			c_t = y_t - (\frac{\alpha_m}{1-\alpha_m} + \chi) s_t \\
		\end{aligned}
	\end{equation}
	In order to close the model an exogenous law of motion for the oil price is required, and defined as follows: 
	\begin{equation}
		\begin{aligned}
			s_t = s_{t-1} + \epsilon_{s,t} \\
			\epsilon_{s,t} \sim N(0, \sigma_{\epsilon_{s}})
		\end{aligned}
	\end{equation}
	Moreover, the monetary authority's behaviour is characterised by the following interest rate rule, assuming $\phi_{\pi}>1$. 
	\begin{equation}
		\begin{aligned}
			i_t = \phi_{\pi} \pi_{q,t} \\
		\end{aligned}
	\end{equation}

	This model has been implemented using gEconpy, where the log-linear model as provided to the simulation programme can be found in \nameref{appe}. The system's stability has been assessed according to the Blanchard-Kahn conditions of stability, based on the eigenvalues of the transition matrix \cite{blanchard_solution_1980}. These have been checked for each parameter specification.	
	
	\section{Data}
	This work relies quarterly macroeconomic variables from 1975 to 2020. All data is obtained from the St. Louis Federal Reserve data base, where it has been seasonally adjusted and deflated \cite{noauthor_federal_nodate}. The specific data codes can be found in the  Table~\ref{tab:fred_variables} in the \nameref{appb}. Most data used across this work is standard to the DSGE literature. However, some extensions are necessary to allow for the inclusion of a petrol sector. The below will briefly outline the use of standard data, while providing more detail on the data specific to this work.
	The common variables for DSGE model evaluation are nominal interest rates, consumption, wage and labour hours. The nominal interest rate is proxied by the federal funds effective rate as suggested Herbst \& Schorfheide 2016 \cite{herbst_bayesian_2016}. Consumption is measured by household expenditure data. Labour hours and wage refer to non-farm business total hours worked and average hourly wage. The exclusion of the agricultural sector is generally recommended for the US economy \cite{guerron-quintana_bayesian_2013}. Moreover, this work employs the WTI Crude Oil Index as proxy for the price of petrol. The WTI dates back longer than the more recent Brent Crude Oil Index, wherefore this work will rely on it.
	With regards to inflation $\pi$ this work relies on the median consumer price index for the RBC and the NK model. The NK energy model's aim is to analyse inflation under the inclusion of petrol in the economy. Relying on the CPI, which includes petrol prices, would thus be counterproductive as the model should generate inflation resulting from petrol endogenously. In order to accurately reflect the models ability energy price need to be removed from inflation data. In the NK energy model therefore relies on the "Sticky Price Consumer Price Index less Food and Energy". Similar reasoning applies to consumption expenditure which for the NK energy model specifically excludes expenditure for energy and utilities.
	
	\subsection{Preprocessing}
	Economic data is usually considered in per capita terms in order to account for different population dynamics (ref). Population dynamics are not continuously collected and usually a mix of actual measurement and imputation. As a result population data exhibits significant irregularities \cite{pfeifer_guide_2021}. Using such noisy records would distort the data of more carefully collected economic variables. Population data thus requires some preprocessing on its own. Smoothing the data with the Hodrick-Prescott (HP) filter's at a smoothing factor of 10,000 is recommended in order to de-noise the data \cite{edge_judging_2013}. The smoothed time series captures the real population dynamics and can then be used to obtain per capita variables. 
	Having removed population dynamic and inflation the next step is to separate the cyclicality off trending variables. As DSGE models are defined in terms of log-deviations from the steady state, as such the data need to be transformed accordingly. This requires an identification of the steady state and its evolution over time. Separating the long-term trend of GDP from its cyclical deviations commonly relies on the HP filter. The filter separates trend and deviation. In the case of quarterly variables a smoothing parameter of 1600 is recommended \cite{ravn_adjusting_2002}. As DSGE models are log-linearised they describe deviation in terms of logarithm, which for small values approximates percentage deviations. In order to transform data into log-deviations from their underlying trend the HP filter is fed with log-transformed time series. This procedure by construction yields stationary time series of cyclicality. However, to comply with common standards stationarity is formally confirmed by the Augmented-Dickey Fuller test (ref). Having transformed all trending variables have into stationary log-deviation from their natural level only the percentage variables remain, namely inflation and interest rate. They require no such treatment, however need translation from annual to quarterly rates. This has been done according to the following formula $(1 + \frac{\matr{X}}{100})^{1/4} - 1$.
	
	\input{{./graphs/data_descriptives.tex}}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.9\textwidth]{data_variables_transformed.png}
			\caption{Log transformed HP filtered stationary data series}
			\label{fig:data}
		\end{center}
	\end{figure}
	
	\subsection{Analysis of models}
	The data used for the analysis spans from 1985 to 2020. Moreover, data from 1975 to 1985 have been used to derive parameter prior distribution from historical averages. 
	As can be seen across the transformed GDP series \ref{fig:data} this period spans across three major economic crisis, the 2000 dot.com bubble, the 2008 financial crisis and most recently the Covid-19 pandemic. These are similarly present across consumption, labour hours and investment. When turning to the interest rate great differences across the sample period become apparent. The mid-80s still influenced by the Volker-shock have a comparability high interest rates when compared to the more recent low-interest environment starting as of 2009.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.9\textwidth]{bvar_forecast1.png}
			\caption{Bayesian VAR 2 lags}
			\label{fig:bvar_2lags}
		\end{center}
	\end{figure}
	Running a first two-lag \ac{BVAR} on output $y$, consumption $c$ and labour $l$ reveals the difficulty of forecasting. Depicted in blue in Figure~\ref{fig:bvar_2lags} systematic bias across output and consumption forecasts becomes apparent. Similarly labour hours are forecasted to be much more volatile than actually the case. A more comprehensive \ac{VAR} model will be presented later in \nameref{forecast_bvar}. The initial success of RBC literature was based on the fact that a model was able to replicate data with the same first and second order moments as real data. For this purpose covariance matrices this work includes the covariance matrix for each of the above models as well as the data covariance matrix in \nameref{appa}.
			
	\section{Bayesian Estimation}
	As explained in the above the RBC and NK models are based on structural parameters of the economy. As such they are not subject to the Lucas 1981 critique, as their parameters do not depend on behaviour of agents but describe their preferences \cite{lucas_jr_tobin_1981}. Parameters thus describe the time invariant dynamic system underling the economy. Evaluating the accuracy of such system is thus tied to evaluating its parameters in capturing the true dynamics \cite{herbst_bayesian_2016}. In this aim two main approaches of evaluation have been drawn up by research over time, frequentist or Bayesian methods. 
	The frequentist approach first relied on the simulation method of moments, which latter developed into the general method of moments (GMM) \cite{blanchard_solution_1980}. This approach attempts to minimize the distance between simulated and real data moments, allowing to discriminate between several specification of the same structural model \cite{christiano_current_1992}. However, as the number of parameters grew GMM conditions were no longer sufficient to estimate model fit \cite{guerron-quintana_bayesian_2013}. Instead, maximum-likelihood (ML) estimation was used but likewise unable to estimate the growing number of parameters \cite{guerron-quintana_bayesian_2013}. Bayesian statistics offered remedy in the convenient formulation of conditional likelihood \cite{guerron-quintana_bayesian_2013}. Based on Bayes's law the parameters' posterior distribution can be found through the definition of a prior distribution and the conditional likelihood. 
	Bayes' law suggests that the posterior of parameter vector $\Theta$ can be obtained by multiplying the prior $P(\Theta)$ with the conditional likelihood of the data $Y_t$ given the parameters $\Theta$. 
	\begin{equation} \label{eq:blaw}
		P( \Theta | Y_{T}) = P(\Theta) L(Y_{T} | \Theta)
	\end{equation}
	Based on Bayes law parameters and its dynamic system can be evaluated using the data the system is meant to generate \cite{herbst_bayesian_2016}. For that three ingredients are required, a distribution reflecting prior believes about the parameters, a likelihood function and an iterative procedure allowing to sample from the posterior. The following section will introduce these three concepts. Moreover, it will introduce the forecasting properties that naturally arise from joining the three. 

%	 DSGE literature has developed a rich body of literature covering the case of the canonical DSGE model rather well (ref). The state of research is currently concerned with the estimation of larger models and non-linear and not normally distributed posterior distributions (ref). However, given the simplicity of the above discussed models current techniques are more than sufficient for this thesis purposes.
		
	\subsection{Kalman Filter}
	Bayesian analysis of DSGE models is concerned with the transition from a prior distribution reflecting previous held knowledge to the posterior \cite{herbst_bayesian_2016}. In this an expression of conditional likelihood is needed \eqref{eq:blaw}. In obtaining the likelihood of a linear system one can make use of the fact that the system is itself a data generating process \cite{andrews_kalman_2008}. The likelihood function is obtained through evaluating the probability of a given data series being generated by the system. 
	A method allowing to do so is the Kalman filter, a method originally developed for tracking moving objects across several sensors \cite{kalman_new_1960}. In order to track any object the filter requires the object's position and velocity to be described in a linear system. The filter then uses this system and sensor data to form a believe about the object's position, also referred to as the system's state \cite{andrews_kalman_2008}. Applied to DSGE modelling the system is the above derived model and its state are the magnitude to which the variables deviate from their respective steady state.	
	In forming a believe about the system states the filter takes into account that sensor signals are inherently noisy. The Kalman filter exploits its knowledge about the nature of this measurement noise in using it to reduce the variance of its believe about the current system state \cite{andrews_kalman_2008}. In doing so the Kalman filter imposes the assumption of Gaussian distributed noise \cite{kalman_new_1960}. This assumption makes use of the fact that multiplying Gaussian distributions results in another Gaussian distribution with lower or equal variance. Decreasing a distribution's variance is equivalent to narrowing down the believe about the current state of the tracked object. Applied to DSGE modelling the assumption of Gaussian noise implies that the joint distribution of model shocks must be Gaussian \cite{herbst_bayesian_2016}. This is satisfied for small scale New Keynesian models if the individual shocks are Gaussian \cite{herbst_bayesian_2016}. 	
	The Kalman filter is especially desirable for two properties. For one it translates its believe about a system's state into a likelihood function \cite{kalman_new_1960}. Moreover, it is the optimal forecast for a linear system \cite{kalman_new_1960}. The below will briefly explain how the filter operates as well as its construction of the likelihood function. In implementing the below in code this has drawn from filterpy, a library for Bayesian filtering (ref).
	
	In a generic linear system with transition matrix $\matr{F}$, the state variables $\matr{X}_{k-1}$ is propagated to $\matr{X}_k$. The shock noise matrix $\matr N$ describes the effect of shocks on the transition process. In forming a believe about the current state $\matr{X}_k$ the Kalman filter proceeds in two steps, a forecast and an update step.
	\begin{equation}
		\begin{aligned}
			\matr{X}_k = \matr F \matr{X}_{k-1} + \matr N \matr{\epsilon}_k \\
			\matr{\epsilon}_k \sim N(0, \sigma_{\epsilon})
		\end{aligned}
	\end{equation}
	
	\subsubsection{Kalman forecast step} \label{kalman_forecast_step}
	In the forecast step the Kalman filter propagates system $\matr X_{k}$ according to the transition matrix $\matr F$. The propagation then yields a believe about the state given $\matr{\hat{X}}_{k|k-1}$ state given information from period $k-1$.
	\[
		\matr{\hat{X}}_{k|k-1} = \matr F \matr X_{k-1|k-1}
	\]
	The second key aspect to the Kalman Filter is the system state variable's covariance matrix $\matr P_{k}$, which is also time dependent. As pointed out in the above, this matrix is assumed to be Gaussian, thus semi-definite positive \cite{andrews_kalman_2008}. The covariance matrix $\matr P_{k}$ is propagated in similar fashion. In doing so measurement noise $\matr Q$ is added to reflect the uncertainty around measurements. This results in $\matr{P}_{k| k-1}$ another Gaussian distribution, reflecting the uncertainty the predicted state $matr{\hat{X}}_{k|k-1}$.
	This procedure without the filter step is used to obtain the three period ahead forecasts used in \nameref{forecast_eval}. The filter's forecast is repeatedly applied without taking reference to a measurement. The confidence intervals of this forecast are derived from the multivariate distribution $\matr{P}_{k| k-1}$ representing the certainty of the believe about state $\matr{\hat{X}}_{k|k-1}$. It's diagonal contains the state variable's standard deviation and is used to construct confidence interval around the predicted state. 
	\[
		\matr{P}_{k| k-1} = \matr F \matr{P}_{k-1| k-1} \matr{F}^T + \matr Q
	\]
	\subsubsection{Kalman filter step}
	Once the predicted state $\matr{\hat{X}}$ and its covariance matrix $\matr{P}_{k| k-1}$ have been obtained Kalman filter proceeds to identifying the most likely system state. For this it takes into account the state measurement $\matr{z}_t$ and its measurement noise. 
	Due to measurement noise the system's real state is in-between its predicted state $\matr{X}_{k| k-1}$ and its measurement $\matr{z}_k$. The objective of filtering is to identify the exact point in-between the two \cite{andrews_kalman_2008}. For this purpose the Kalman filter derives the Kalman gain $\matr{K}_{k| k-1}$, a weighting function between $\matr{X}_{k| k-1}$ and $\matr{z}_k$. In building $\matr{K}_{k| k-1}$ the filter takes into account the process noise, the covariance matrix and the accuracy of past iterations.	
	This requires $\matr{y}_k$ a measure of distance between measurement and prediction is derived. In calculating $\matr{y}_k$ the measurement matrix $\matr H$ translats measurement $\matr{z}_t$ into the same units as the predicted state $\matr{X}_{k| k-1}$. For the purposes of this work all unit transformation has been done prior to filtering, wherefore the measurement matrix corresponds to the identity matrix $\matr{I}$.
	\[
		\matr{y}_{k} = \matr{z}_k - \matr H \matr{X}_{k| k-1}
	\]
	Filter uncertainty $\matr{S}_k$ is obtained by combining the covariance $\matr{P}_{k| k-1}$ of the predicted state $\matr{X}_{k| k-1}$ with the process noise $\matr{R}$. In the context of DSGE filtering the process noise is derived from the covariance matrix $\matr{N}$ as defined above \cite{guerron-quintana_bayesian_2013}.
	\[
		\matr{S}_k = \matr H \matr{P}_{k| k-1} \matr{H}^T + \matr{R}
	\]
	The uncertainty term $\matr{S}_{k}$ is then used to calculate the weighting function between measurement and predicted state, the Kalman gain. 
	\[
		\matr{K}_k = \matr{P}_{k| k-1} \matr{H}^T \matr{S}_{k}^{-1}
	\]
	The filtered state, which will be at the beginning of the next iterations, is then obtained from the Kalman gain and distance between measurement and predicted state $\matr{y}_k$. The covariance matrix $\matr{P}_{k|k-1}$ is similarly updated. This yields a new narrower Gaussian distribution $\matr{P}_{k|k}$, summarising the uncertainty around the system's true state.
	\begin{equation}
		\begin{aligned}
			\matr{X}_{k|k} = \matr{X}_{k| k-1} + \matr{K}_k \matr{y}_k
			\matr{P}_{k|k} = (\matr I - \matr{K}_k \matr{H}) \matr{P}_{k|k-1}		
		\end{aligned}
	\end{equation}
	Having introduced the Kalman filter algorithm the question of its initial condition remains. As DSGE models describe deviations from the steady state $\matr{X}_{0|0}$ is set to zero, assuming the system to be in steady state. This approach is generally followed by filtering literature \cite{schorfheide_loss_2000}. 
	
	\subsubsection{Likelihood function} \label{kalman_ll}
	The likelihood function for each iteration is obtained from the assumption of Gaussian system noise \cite{kalman_new_1960}. The likelihood is defined as a multivariate normal distribution, which s generically given the below where $\mu$ is the distribution's mean, $\sigma$ its standard deviation and $x$ the measurement.
	\begin{equation}
		\Lagr = \frac{1}{\sigma \sqrt{2 \pi}} \exp [- \frac{1}{2} \frac{x - \mu}{\sigma}]^2
	\end{equation}
	The below will refer to the Kalman likelihood as the cumulated likelihood across the entire data set with periods $T$. The Kalman likelihood used in the sampler is therefore expressed as the below.
	\begin{equation}
		\Lagr_T = \sum_{k=0}^{T} \left[ \quad \frac{1}{\matr{S}_k \sqrt{2 \pi}} \exp [- \frac{1}{2} \matr{y}_k^{T} \matr{S}_{k}^{-1} \matr{y}_k] \right]
	\end{equation}
		
	\subsection{Metropolis-Hastings Sampler}
	The aim of Bayesian estimation is the identification of the true parameter distribution, the posterior. However, mapping a DSGE model's parameters to its posterior is non-linear in the parameter vector $\Theta$. Therefore the posterior distribution cannot be evaluated analytically \cite{herbst_bayesian_2016}. Instead a numerical approximation mechanism is require, this is referred to as the sampler \cite{guerron-quintana_bayesian_2013}. The Metropolis-Hastings Monte Carlo Markov Chain (MH-MCMC) sampler is the predominant sampling method in linear Bayesian estimation literature \cite{guerron-quintana_bayesian_2013}. The MH-MCMC sampler is applicable to small scale New Keynesian models. It usually delivers good results for small models as their joint parameter distribution is well-behaved and elliptic \cite{herbst_bayesian_2016}. This reduces the posterior identification problem to a global problem. Larger models usually exhibit no-elliptic parameter distributions, requiring samplers which are able to distinguish local and global likelihood maxima \cite{herbst_bayesian_2016}. However, as this thesis is concerned with the canonical model the MH-MCMC sampler is sufficiently accurate.	
	The MH-MCMC sampler suggests new posterior candidates according to a multivariate random walk. The posterior is extend by a suggestion if it provides an improvement in the likelihood as described in \nameref{kalman_ll}. The algorithm generates a stable Markov chain, which is meant to exhibit low autocorrelation and low variance of the MH estimator. Under such conditions the resulting chain is equivalent to the posterior distribution \cite{herbst_bayesian_2016}. The below will outline this thesis implementation of the MH-MCMC sampler for which the code can be found in \nameref{appd}. \\
%and methods of evaluating its convergence to the true posterior distribution

	The algorithm proceeds in three main steps, first a candidate for the posterior $\matr{\hat{\Theta}}_{k|k-1}$ is suggested based on the random-walk law of motion. The random-walk departs from the last accepted posterior candidate and suggests a new candidate by departing from the $\matr{\Theta}_{k-1|k-1}$ according to the zero-mean Gaussian distribution $\epsilon$. The distribution's variance $\matr{\Sigma}$ can be set to either the identity matrix $\matr{I}$ or to contain the parameter prior variances on its diagonal. The latter is recommended if well-defined priors are at play \cite{herbst_bayesian_2016}.
	\begin{equation}
		\begin{aligned}
			\matr{\hat{\Theta}}_{k|k-1} = \matr{\Theta}_{k-1|k-1} + \eta \epsilon \\
			\epsilon = N(0, \matr{\Sigma})
		\end{aligned}
	\end{equation}
	Across the law of motion the \ac{MH-MCMC} sampler employs the gain parameter $\eta$ which essentially scales the distance of the new posterior candidate away from the last accepted candidate $\matr{\Theta}_{k-1|k-1}$. This value has been calibrated to be $\eta \in [0.2, 04.]$ \cite{gelman_weak_1997}. Generally the value of $\eta$ should correspond to an acceptance rate between 20\% and 40\% of suggested draws \cite{herbst_bayesian_2016}. This work puts $\eta$ at 0.25 which yields an acceptance rate of around 30\% for each model.
	A new posterior candidate $\matr{\hat{\Theta}}_{k|k-1}$ is once obtained evaluated through the Kalman likelihood. This yields the likelihood of the candidate given the data $\matr{Y}_T$.
	\[
		 \Lagr(Y| \matr{\hat{\Theta}}_{k|k-1})
	\]
	Once the likelihood is obtained the algorithm proceeds into the two step acceptance procedure \cite{guerron-quintana_bayesian_2013}. First the likelihood of candidate $\hat{\Theta}_{k|k-1}$ is compared to the most recently accepted posterior. According to Bayes' law this allows to determine whether this draw constitutes an improvement in likelihood.	
	\begin{equation}
		\omega_k = \min \quad \left\{ \frac{ \Lagr(Y|\matr{\hat{\Theta}}_{k|k-1}) P(\matr{\hat{\Theta}}_{k|k-1})}{\Lagr(Y| \matr{\Theta}_{k-1|k-1})  P(\matr{\Theta}_{k-1|k-1})}, 1 \right\}
	\end{equation}
	In a second step the likelihood ratio $\omega_k$ is randomly accepted if above uniform random variable $\phi \sim U(0, 1)$. If $\omega_k \leq \phi_k$ the draw is accepted and the candidate is assumed into the posterior. This process is repeated $N$ times generating the a posterior sample. 
% check number of sampler runs
	As an recursive method the MH-MCMC sampler requires an initial condition. This initial condition is usually obtained throughout a so-called burn-in period corresponding to 50\% of total iterations. Moreover, a number of at least 20,000 is recommended for the sampler \cite{herbst_bayesian_2016}.
%	The following will now describe the evaluation steps to insure convergence of the chain. Moreover, once the algorithm has run one needs to evaluate this performance. A method beyond visual inspection is suggested by (ref).
	The MH random walk is highly susceptible to miss-specification of the posterior of the prior distribution is far away from the true posterior \cite{herbst_bayesian_2016}.
			
	\subsection{Priors}
	In response to the Lucas critique \ac{DSGE} models were build around structural time invariant parameters \cite{lucas_jr_tobin_1981}. Consequently, parameters can be expressed as a joint distribution of preferences, which in its entirety is usually impossible to evaluate \cite{del_negro_forming_2008}. Instead parameters are analysed in building individual prior distributions reflecting independent and prior knowledge about them \cite{del_negro_forming_2008}. Here the notion of independence is important in tow ways. Firstly, only if the data used to construct the priors is independent of the likelihood mechanism the resulting posterior is unbiased \cite{del_negro_forming_2008}. This condition is fulfilled if the data for prior formation at least pre-dates the of the likelihood evaluation \cite{herbst_bayesian_2016}. Secondly, assuming independence between parameters is only reasonable within limits, as shocks do not necessarily occur independently \cite{herbst_bayesian_2016}. However, the latter only poses a problem for large multi-shock models. This work analyses the canonical singular shock model and is thus unconcerned by this second issues. The below will therefore focus on identifying priors based on independent information. In doing so the approach of Del Negro \& Schorfheide (2008), dividing priors into three groups for analysis, will be followed \cite{del_negro_forming_2008}. Graph summarising the prior to posterior transition can be found here [\ref{fig:rbcpp}, \ref{fig:nk5pp}, \ref{fig:nk6pp}]. Across the following sections each prior group will be briefly outlined and its posterior transition will be analysed. An overview of all priors distributions can be found in the appendix \ref{appb}. 
	
	\subsubsection{Steady state priors}
	The group of steady state priors can be easily identified from long-run relationship of model variables. These relationships, such as the labour or capital share are also sometimes referred to as the 'great ratios' \cite{kydland_time_1982}. For the models in question the steady state parameters are: 
	\begin{equation}
		\begin{aligned}
			\Theta_{ss}^{RBC} = \{ \alpha, \beta, \delta \} \\
			\Theta_{ss}^{NK} = \{\alpha, \beta, \theta \} \\
			\Theta_{ss}^{NKE} = \{\alpha_m, \alpha_n, \beta, \theta, \chi \} \\
		\end{aligned}
	\end{equation}
	The time discount factor $\beta$ is usually not calibrated, as it results directly from the inverse of the interest rate \cite{guerron-quintana_bayesian_2013}. The pre-1985 data used to estimate prior distributions puts $\beta$ to 0.995.	
	The capital share can be derived from the long-run share in industrialised nations. For smaller models using a uniform distribution covering the possible parameter interval $\alpha \in [0,1[$ is suggested \cite{del_negro_forming_2008}. In the RBC $\alpha$'s posterior converges from a uniform to a centred distribution \ref{fig:rbcpp}. While this is still rather broad and far off the empirical value of 33\% it is indication of convergence towards the true posterior \ref{fig:rbcpp}.
	For both New-Keynesian models a more targeted distribution has been used to ensure steady state existence \cite{guerron-quintana_bayesian_2013}. Consequently the values for $\alpha$ are more left skewed \ref{fig:nk5pp}. Interestingly enough the calibration of the NKE greatly overestimates the share of oil in production which by \cite{blanchard_macroeconomic_2007} is reckoned at below 5\% \ref{fig:nk6pp}. The labour share in production is more reasonable but also off the empirical estimate of 0.7 \cite{blanchard_macroeconomic_2007}.
	The depreciation rate $\delta$ while rather high in the posterior has yet assumed reasonable values in limiting its posterior to below 0.8 \ref{fig:rbcpp}. This indicates some convergence to the real prior, while still being far from empirically estimated values of 2.5\% per quarter \cite{campbell_inspecting_1994}.
	The price setting probability $\theta \in [0,1]$ has reasonable estimates in the NK model \ref{fig:nk5pp}. Its mean is around 0.8 which corresponds to an average price duration of roughly four month, close to the empirical estimate \cite{blanchard_macroeconomic_2007}. This is not the case for the NKE model where $\theta$ exceeds one, which as a probability is impossible. The NKE estimates are much more accurate for the share of petrol in consumption, whose posterior remains close to the empirical mean of 1.7\% \cite{blanchard_macroeconomic_2007}.

	\subsubsection{Exogenous prior}
	Exogenous priors contain the exogenous law of motion and standard deviation of shocks. In a simple model with singular shocks they can be identified from the first and second order moments of the economic variables affected by the shocks \cite{del_negro_forming_2008}. This work thus calibrated the shock standard deviation based on the variance of pre-1985 output cyclicality amounting to 0.0243. Moreover, in setting the autoregressive parameter this work draws from the distributions suggested by \cite{vasconez_what_nodate}. Looking across the posterior all estimates for $\rho$ are bounded at one. While this might appear to be a successful identification this is merely due to the stability requirement of the dynamic system. If $\rho \in \quad ]-1,1[ $ is violated the dynamic system is not stable and thus cannot be solved, leading the sampler to discard any value $|\rho| > 1$. The high variance of the posterior thus indicates problems of identification. However, its skewness towards higher values seems to confirm the prior assumption of relatively time persistent shocks across all models \ref{fig:nk5pp}
	\begin{equation}
		\begin{aligned}
			\Theta_{exog}^{RBC} = \{\rho_A \} \\
			\Theta_{exog}^{NK} = \{\rho_v \} \\
			\Theta_{exog}^{NKE} = \{\rho_s \}
		\end{aligned}
	\end{equation}

	\subsubsection{Endogenous priors}
	All remaining parameters make up the group of endogenous priors. These parameters cannot be measured directly through economic variables' long-term relationships. Instead they rely on micro-evidence from external data sets \cite{del_negro_forming_2008}. 
		\begin{equation}
		\begin{aligned}
			\Theta_{endog}^{RBC} = \{\sigma_C, \sigma_L, \} \\
			\Theta_{endog}^{NK} = \{\sigma_C, \sigma_L, \epsilon, \phi_{y}, \phi_{\pi} \} \\
			\Theta_{endog}^{NKE} = \{\epsilon, \phi_{y}\}
		\end{aligned}
	\end{equation}
		
	The elasticity of substitution $\sigma_C$ and $\sigma_L$ belong in this category. In setting their prior distribution this work relied on the recommendation from \cite{del_negro_forming_2008}. For the RBC the posterior values demonstrate large spreads indicating that the MH sampler did not identify the posterior correctly \ref{fig:rbcpp}. Even larger dispersion can be observed for the NK model \ref{fig:nk5pp} \footnote{ In the \ac{NKE} model the labour and consumption elasticities do not enter into the log-linear model specification due to the log-utility function}. This occurs frequently as the consumption and labour elasticity are parameters with low influence on the transition dynamics \cite{guerron-quintana_bayesian_2013}. Consequently they are dominated by stronger parameters in the likelihood function and the posterior identification fails for these variables.
	The elasticity of substitution $\epsilon$ originates from the Dixit-Stiglitz aggregator and factors into the steady-state mark-up of firms. As such it is one of the driving parameters behind inflation and strong in its influence on the transition dynamics. In setting the prior distribution one can rely on information on the steady-state mark-up $mc^r = \frac{\epsilon}{1-\epsilon}$. Following this a value of $\epsilon$ slightly above 0.5 appears realistic. Consulting the posterior both put the mean around 0.5 \ref{fig:nk5pp} \ref{fig:nk6pp}. While the standard deviation is still large the posterior specification does appear to be within reasonable bounds. 
	
	The central bank's preference parameters $\phi_y$ and $\phi_{\pi}$ also appear within reasonable bounds. As suggested by \cite{gali_monetary_2008} the central bank has a stronger response to inflation that to to the output gap. This has been incorporated into priors and the resulting posterior seems to confirm this assumption in the NK model \ref{fig:nk5pp}. The NKE model relies on output and not the output gap as intertemporal instrument, wherefore $\phi_{\pi}$ appears. However, the posterior estimate appear to confirm the same reasoning \ref{fig:nk6pp}.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.5\textheight]{mod4_rbc_vanilla_posterior_hist.png}
			\caption{RBC model prior posterior comparison.}\label{fig:rbcpp}
			\includegraphics[width=.5\textheight]{mod5_nk_vanilla_lin2_posterior_hist.png}
			\caption{New Keynesian model prior posterior comparison.}\label{fig:nk5pp}
			\includegraphics[width=.5\textheight]{mod6_nk_energy_lin2_posterior_hist.png}
			\caption{New Keynesian Energy model prior posterior comparison.}\label{fig:nk6pp}
		\end{center}
	\end{figure}
	\pagebreak
		
	\section{Forecast evaluation} \label{forecast_eval}
	Having identified the parameter posterior distribution this section uses the Kalman forecasting ability as outlined in Section~\ref{kalman_forecast_step}. In order to obtain a three quarter ahead forecast each model will be specified using the mean posterior parameters, as this it corresponds to the most likely model. The individual forecasts will then be analysed across the next three section. A fourth section will introduce a \ac{BVAR} forecast, while a last section will establish a comparison between DSGE models and the \ac{BVAR} forecast.	In order to accommodate for the Kalman filter convergence the following will be analysing data after 2003. The variables revealed as observables to the filter vary across models in order to reflect the differences in model specification such as investment expenditure, inflation and petrol prices. This is indicated in each section.
	
	\subsection{Real Business Cycle model}	
	The Kalman filter closely traces output and investment as can be seen in Figure~\ref{fig:rbc_kfil}. The real data lies within the filter's 95\% confidence interval for all post 2010 observations which is an important criteria of model fit. This is also true for the recession in early 2020, which emphasises the filter's fit to real data. Consumption, while not contained as an observable in the filter is also closely traced and contained within the confidence bands. This points to even better model accuracy, as the dynamic system is able to replicate a non-included variable to a good extend. The interest rate is less closely traced and predicted to be much more volatile than actually the case. While this indicates problems in the model specification it is important to note that the US Federal Reserve (FED) pursued a notorious low interest rate policy for much of the sample period. It thus followed a different dynamic than in the RBC where the rate of return is dictated by changes in capital demand. A lesser fit is thus not surprising that. Labour is the only variable continuously outside of the 95\% confidence interval. The model even proposes countercyclical movements in labour hours during crisis. While peculiar dynamics where at play for the Covid-19 pandemic the 2008 crisis was certainly more standard in-terms of labour response. This points to a structural misspecification of the RBC model, unable to replicated recessionary unemployment.
	Generally, pre-2010 dynamics are captured less well by the model. Output and consumption data escapes the filter's 95\% confidence interval before and after the 2008 financial crisis. The fit to more recent data is significantly improved. This is possibly circumstantial or related to the expanded period of growth and relative economic stability.\\
				
	The RBC forecasts, as depicted in Figure~\ref{fig:rbc_kfor}, seem reasonable in direction and confidence. The 95\% intervals naturally increase with distance to the point of prediction. However, especially at the one period ahead forecast they exhibit reasonable bounds. Moreover, the point forecasts, with exception of labour, closely trace real variables with no systematic bias in either direction. Labour as explained in the above appears to be incorrectly specified in the model's functional form. Yet, this does not impede the model in its forecasting of other variables. 
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.5\textheight]{mod4_rbc_vanilla_y_I_kalman_filter_00.png}
			\caption{RBC model filtered data with observables $Y$ and $C$}\label{fig:rbc_kfil}
			
			\includegraphics[width=.5\textheight]{mod4_rbc_vanilla_y_I_kalman_forecast.png}
			\caption{RBC model forecasted data with observables $Y$ and $C$}\label{fig:rbc_kfor}
		\end{center}
	\end{figure}
	
	\subsubsection{New-Keynesian model}
	At first glance it is evident from Figure~\ref{fig:nk_kfil} that the New-Keynesian model traces data less accurately. As with the RBC output is closely traced after 2010 while confidence intervals exclude actual data around the 2008 crisis. However, other than with the RBC the NK does not manage to track the 2020 crisis. The slump in output is even deemed outside the 95\% confidence interval. A similar dynamic can be observed with consumption. Again filter fit is much better after 2010, yet does not capture the 2020 crisis. Due to absence of by investment in the NK output is formally equal to consumption. The filter therefore essentially remodels output data, hence consumption can be seen as lesser of a performance indicator. Labour on the other hand is more closely traced and differently from the RBC no longer exhibits counter-cyclicality during recessions. This is indication of better fit by the NK's functional form. However, the main contribution of the New-Keynesian literature is the inclusion of inflation. Looking at the actual dynamics against filtered data large divergence is obvious. The model puts inflation at much higher rates than actually the case in the economy. The same goes for the real interest rate, which is not surprising due to the close tie between the two $r_t = i_t + \E[\pi_{t+1}]$. Moreover, the confidence intervals are rather broad on both inflation and interest rate. This, while no sign of good fit, at leasts shows that the model is uncertain about the actual behaviour of inflation. It does not deem its forecast erroneously accurate, which can be turned into an argument against complete miss-specification.		
	While the filtered data provides decent results the NK's forecasts are far from informative as illustrated in Figure\ref{fig:nk_kfor}. Output and consumption are not contained in the forecasts' 95\% interval. Moreover, the broad confidence intervals on inflation and interest rate conceal the fact that inflation forecasts actually move opposite to real data. Only labour is proxied to reasonable extend by the confidence interval, while its point forecast is systematically off as well. To standards of visual inspection the NK specification seems to fit data to a lesser extend, suggesting its specification to be less accurate than the RBC. 
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.5\textheight]{mod5_nk_vanilla_lin2_y_kalman_filter.png}
			\caption{NK model filtered data with observables $C$ and $N$}\label{fig:nk_kfil}
			\includegraphics[width=.5\textheight]{mod5_nk_vanilla_lin2_y_kalman_forecast.png}
			\caption{NK model forecasts with observables $C$ and $N$}\label{fig:nk_kfor}
		\end{center}
	\end{figure}
		
	\subsubsection{New-Keynesian energy model}
	Figure\ref{fig:nke_fil} depicts the New-Keynesian Energy model's filtered time series. Compared to the previous models its data is the furthest off reality. The dynamics of output and labour are poorly captured by erroneously narrow confidence intervals. The same goes for inflation which despite contained as observable does not improve this inability to track output or consumption. Neither is the real interest rate, even-though closely tied to inflation, correctly traced by its confidence bands. Figure~\ref{fig:nke_fil_s} reveals that including petrol in the model specification does not improve this circumstance. Instead, the filter clings erroneously to the petrol price, while ignoring other variables. The aim of including petrol into the NK framework was to supplement the analysis of inflation with external confounding factors. The filtered time series reveals that the model specification is unable to live up to this aspiration. Including the petrol price in the filter does not provide the theorised improvement.\\
	
	The poor filter fit is obviously reflected across the forecasts as well Figure~\ref{fig:nke_for}. While point forecasts steer off in opposite directions even the confidence bands course of data, excluding real data for consumption, inflation and the interest rate.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.5\textheight]{mod6_nk_energy_lin2_y_pi_s_c_kalman_filter.png}
			\caption{NK model Kalman Filter with variables c, n}\label{fig:nke_fil}			
			\includegraphics[width=.5\textheight]{mod6_nk_energy_lin2_y_pi_s_s_kalman_filter.png}
			\caption{NK model Kalman Filter with variables y}\label{fig:nke_fil_s}
			\includegraphics[width=.5\textheight]{mod6_nk_energy_lin2_y_pi_s_kalman_forecast.png}
			\caption{NK model Kalman Filter with variables y}\label{fig:nke_for}
		\end{center}
	\end{figure}

	\subsubsection{BVAR} \label{forecast_bvar}
	Using a Vector Autoregression as reference model for comparing DSGE model forecasts has been commonly done across literature \cite{schorfheide_loss_2000, chin_bayesian_2019}. 
	Naturally the BVAR does not provide confidence intervals like frequentist methods of forecasting. Instead model parameters are defined through a sampler, which in this case ran in four iterations. Sampling from these iterations yields the posterior coefficient distribution. Using several draws from the posterior then allows to derive information about the range of possible outcomes. This range can then be interpreted as a confidence interval \cite{chin_bayesian_2019}. A sample of 200 posterior specifications is illustrated in blue across Figure~\ref{fig:bvar}, while the sample's mean is marked in red. 
	The range of outcomes for output seem reasonable. While growing in spread with forecast distance, the mean approximates the real course with a slight downward bias in the forecast. Similar dynamics can be observed for labour, though confidence bands widen more broadly at period $t+3$. The consumption forecasts overestimates the real trend at first, returning to the true value. The inflation forecast, realises the first down turn but as of $t+2$ looses track of the slight uptrend, resulting in a biased forecast as well. the forecast on the interest rate is the furthest off. However, this is not surprising as the data used to build the VAR originates in low-interest environments. Capturing the exogenously determined up-trend of the FED interest rate policy is thus beyond the scope of a VAR model. On first account the BVAR appears to deliver reasonable forecasts. 
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.4\textheight]{bvar_forecast.png}
			\caption{NK model Kalman Filter with variables y}\label{fig:bvar}
		\end{center}
	\end{figure}
	
	\subsubsection{Forecast comparison}
	In comparing structural models across their forecasting performance economic literature provides two approaches, a frequentist and a Bayesian one. The frequentist approach compares models based on a their mean squared error, testing for systematic differences. This is formally provided by the Diebold-Mariano test. This tests is recommended for on step ahead forecasts and its statistic is based on a large number of observations. It is consequently dissuaded from for quarterly data \cite{chin_bayesian_2019}. Bayesian statistics provides model a model averaging approach, comparing models through a log-likelihood based information criterion. Across the DSGE literature the predominant criterion is the \ac{WAIC}, a choice this work will follow suit \cite{chin_bayesian_2019}. In Bayesian model averaging the weights of each model are calculated from their respective posterior probability \cite{chin_bayesian_2019}. This is traditionally done for structurally similar models in order to discriminate between different priors. However this procedure has also been employed in comparing different model classes \cite{chin_bayesian_2019}. In the context of this work a comparison according to the \ac{WAIC} points to the RBC as the most suitable model specification. At this point it is important to emphasize, that this is a relative evaluation, meaning the RBC is the best amongst the here present models but not a globally good model.
	
	\section{Conclusion}
	\subsubsection{Conclusion on MH MCMC}
	The MH MCMC sampler used across this work is one of the simpler likelihood sampling algorithms. In more comprehensive DSGE models it fails to converge, but for the small scale models analysed in this work it is still deemed appropriate (ref). However, as the above illustrates some identification problems especially regarding 'weaker' parameters have become apparent. Despite providing reasonable estimates for most parameters these should also be handled with care. 
	
	Possible explanations for the deterioration of model fit across the NKE could lie in its specification. The model was originally build around domestic output which combined with petrol consumption generates overall consumption. Overall output was then derived off nominal output by including a GDP deflator. This extension was not necessary for closing the model but a way of including the correct output specification to make the model comparable to data. The NKE at its core does thus not describe output as it is specified in the data, wherefore it is no surprise that it does badly in tracing the real data. In that the model does deliver further insight in possible links between petrol and inflation. However, it does so in a way that cannot be verified with observable data.
	
% Bibliography
	\bibliography{20230505_m1_dsge}
	\addcontentsline{toc}{section}{References}

% Appendix
	\section*{Appendix}
	
	\pagebreak
	\subsection*{Appendix A} \label{appa}
	\input{{./graphs/fred_variables.tex}}
	\input{{./graphs/priors_table.tex}}
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=\textwidth]{cov_matrix.png}
			\caption{Covariance matrices}
			\label{fig:cov_matrix}
		\end{center}
	\end{figure}
	

%BVAR
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.4\textheight]{BVAR_coeff.png}
			\caption{NK model Kalman Filter with variables y}\label{fig:bvar_coef}
		\end{center}
	\end{figure}

	\pagebreak
%	\begin{center}
%		\csvautotabular{{./graphs/priors_table.csv}}
%	\end{center}
%% tables
%	\small 
%	\input{{./graphs/fred_variables.tex}}
	
	\pagebreak
	\subsection*{Appendix B} \label{appb}
	Euler Equation log-linearisation\\
	
	The NK Euler condition is given by the below, which can be transformed into an exponential expression as:
	\begin{equation}\label{eq:log_euler}
		\begin{aligned}
			Q_{t} = \E_t \left[ \frac{C_{t+k}}{C_t} \right]^\sigma \frac{P_t}{P_{t+1}} \\
			1 = \E_t \{\exp(i_t + \sigma \Delta c_{t+1} - \pi_{t+1} - \rho) \}
		\end{aligned}
	\end{equation}
	where log-deviations are given according to: 
	\begin{equation}
		\begin{aligned}
			\rho \equiv - \ln(\beta) \\
			\Delta c_{t+1} \equiv c_{t+1} - c_t \equiv \ln\left( \frac{C_{t+1}}{C_t} \right) \\
			i_t \equiv - \ln(Q_t) \equiv - \ln(1+i_t) \\
			\pi_{t+1} \equiv p_{t+1} - p_t \equiv \ln \left( \frac{P_{t+1}}{P_t} \right)
		\end{aligned}
	\end{equation}
	For the linearisation one aspect is important: GDP will grow at the constant rate $\gamma$ in the steady state. This corresponds to the output growth per capita, the output trend. As output per capita grows so will consumption, wherefore in the steady state $\sigma \Delta c_{t+1} = \sigma \gamma$ must hold. Replacing for consumption in the above exponential \eqref{eq:log_euler} a condition for the natural rate of interest arises. For the exponential to hold this equivalence must hold $i_{ss} = \sigma \gamma + \pi_{ss} + \rho$. 
	Using the property $\hat{x}_t = \ln(x_t) - ln(\bar{x}) = \ln\left(\frac{x_t}{\bar{x}}\right)$ the Euler equation can be rewritten as:
	\begin{equation}
		\begin{aligned}
			1 \approx i_t + \E_t \{\sigma \Delta c_{t+1} + \pi_{t+1} \} + \rho \\
			c_t = \E_t[c_{t+1}] - \frac{1}{\sigma} (i_t - \rho - \E_t[\pi_{t+1}])	
		\end{aligned}
	\end{equation}

	The Euler equation in the RBC is given below and can be linearised in a similar fashion. The important realisation with the RBC is that in the steady state $\beta = (1 - delta + \bar{r})$, where $\bar{r} = \ln(\bar{R})$ the capital rate of return steady state.
	\begin{equation}
		\E_t \left[ \frac{C_{t+1}}{C_t} \right]^\sigma = \beta \left(1 - \delta  + \E_t [R_{t+1}] \right)
	\end{equation}
	This yields: $\hat{c}_t = \hat{c}_{t+1} - \hat{r}_{t+1}$

	\pagebreak
	\subsection*{Appendix C} \label{appc}
	RBC numerical solutions
	\input{{./graphs/rbc_steady_state.tex}}
	\input{{./graphs/rbc_transition_matrix.tex}}
	\pagebreak
	
	\subsection*{Appendix D} \label{appd}
	Metropolis Hastings MCMC sampler code\\
	Deriving the multivariate covariance matrix $G$ from prior the prior distributions standard deviation
	\lstinputlisting[language=Python, firstline=33, lastline=44]{mh_mcmc_code.py}
	Suggesting new priors based on the random walk law of motion
	\lstinputlisting[language=Python, firstline=85, lastline=99]{mh_mcmc_code.py}
	MH MCMC acceptance procedure
	\lstinputlisting[language=Python, firstline=175, lastline=195]{mh_mcmc_code.py}
	
	\pagebreak
	\subsection*{Appendix E} \label{appe}
	\verbfilenobox[\tiny]{{./graphs/mod4_rbc_vanilla.txt}}
	\verbfilenobox[\tiny]{{./graphs/mod5_nk_vanilla_lin2.txt}}
	\verbfilenobox[\tiny]{{./graphs/mod6_nk_energy_lin2.txt}}


	
\end{document}