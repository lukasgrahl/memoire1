\documentclass[12pt,a4paper,english]{article} % document type and language
\usepackage[hidelinks]{hyperref}
\usepackage{layout}
\usepackage{babel}   % multi-language support
\usepackage{float}   % floats
\usepackage{url}     % urls
\usepackage{graphicx}
\usepackage{amsmath} % matrix algebra
\usepackage{multirow} % tables
\usepackage{booktabs} % tables
\usepackage{blindtext}
\usepackage{geometry}
\usepackage[parfill]{parskip} % no indent
\usepackage{amssymb}
\usepackage{csvsimple}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{verbatimbox}
\usepackage{filecontents,catchfile}
\usepackage{float}
\usepackage{nameref}

% acronyms
\usepackage[printonlyused,withpage]{acronym}
\renewenvironment{description}
{\list{}{\labelwidth0pt\itemindent-\leftmargin
		\parsep0pt\itemsep0pt\let\makelabel\descriptionlabel}}
{\endlist}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}


\lstset{style=mystyle}
% Default fixed font does not support bold face
%\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
%\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% graphics path
\graphicspath{{./graphs/}}	% graphics

% title slide
%\author{Lukas Gahl}
%\title{\textbf{Dynamic Stochastic General Equilibrium: An empirical application to energy prices and inflation}\\}
%\
%\date{\today} 

% user commands
\newcommand{\matr}[1]{\mathbf{#1}} % matrix font
\newcommand{\Lagr}{\mathcal{L}} % likelihood L
\newcommand{\E}{\mathbb{E}} % expectation
% RBC expressions
\newcommand{\Rss}{\frac{1}{\beta} + \delta - 1}
\newcommand{\Ass}{\bar{A}}
\newcommand{\KLss}{\left[ \frac{\alpha \Ass}{\Rss} \right]^{\frac{1}{1-\alpha}}}

% set layout
\let\oldsection\section
\renewcommand\section{\clearpage\oldsection}

\geometry{
	a4paper,
	total={170mm,257mm},
	left=25mm,
	right=30mm,
	top=25mm,
	bottom=25mm,
}
\linespread{1.5}

% bibliography
\bibliographystyle{alpha}

\begin{document}
	
	
	\begin{titlepage}
		\begin{center}
			\vspace*{1cm}
			
			\Large
			Dynamic Stochastic General Equilibrium: An empirical application to petrol prices and inflation
			
			
			\vspace{1.5cm}
			
			Lukas Ambrosius Grahl
			
			\vspace{2 cm}
			
			A thesis presented for the degree of\\
			Master of Science Economics
			
			\vspace{2 cm}
			
			\includegraphics[scale=.15]{{./graphs/paris1.png}}

			\vspace{3 cm}
			
		\end{center}
		Presented to: \\
		Professeur Jean-Bernard Chatelain \\
		Centre d'Economie de la Sorbonne\\
		Paris 1 Panthéon-Sorbonne \\
		France\\
		27th of May 2023
		
			
			

	\end{titlepage}

	\pagebreak

	\section*{Abstract}
	This work retraces the Dynamic Stochastic General Equilibrium Literature from its roots in Real Business Cycles theory, through the standard New-Keynesian model to an extension by exogenous petrol prices. In doing so it aims at investigating the theorised link between inflation and petrol prices by drawing from Bayesian estimation of structural models. 
	Following an outline of the modelling mechanism, model parameters are calibrated using the Kalman filter and Metropolis-Hastings sampling. Evaluating the calibrated models in their data fit as well as forecasting properties beyond a Vector Autoregression allows for two main conclusion. 
	The modelling procedure is capable of producing forecasts beyond such Vector Autoregression. Yet, an extension of the New-Keynesian framework by petrol prices does not provide further forecasting power.
	\pagebreak
	

	
	\section*{List of Acronyms}
	\begin{acronym}
		\acro{BVAR}{Bayesian Vector Autogreression}
		\acro{CES}{Constant Elasiticty of Substitution}
		\acro{CPI}{Consumer Price Index}
		\acro{DSGE}{Dynamic Stochastic General Equilibrium}
		\acro{GDP}{Gross Domestic Product}
		\acro{GMM}{General Method of Moments}
		\acro{HP}{Hodrick-Prescott Filter}
		\acro{IS}{Investment-Savings Curve}
		\acro{MH-MCMC}{Metropolis-Hastings Monte-Carlo Markov-Chain}
		\acro{NK}{New-Keynesian}
		\acro{NKE}{New-Keynesian Energy}
		\acro{NKP}{New-Keynesian Phillips Curve}
		\acro{RBC}{Real Business Cycle}
		\acro{VAR}{Vector Autoregression}
		\acro{WAIC}{Widely Applicable Information Criterion}
	\end{acronym}
	\listoffigures
	\listoftables
	\pagebreak
	
		
	\tableofcontents
	\pagebreak
	
	
	 \section{Introduction}
	 The recent simultaneous surge in energy prices and inflation has reintroduced questions about a connection between both variables. Already the late 1970s oil shock illustrated such link, causing a recession across most western economies. Later oil price shocks in the 90s and 2000s, despite being of similar magnitude, did not match their predecessor's economic impact \cite{blanchard_macroeconomic_2007}. Recent events however, seem to suggest that such link has re-emerged. This calls for an reevaluation of the literature on inflation dynamics.
	 Research in this area has for much of the recent past relied on structural rational expectation models. These models analyse the transition mechanism of inflation, while allowing to take into account confounding factors such as oil prices. This work draws from this past literature and applies it to recent events. In doing so three strains of literature are merged, namely: \ac{RBC} and \ac{NK} literature, Bayesian estimation of structural models and literature on structural models encompassing petrol. 
	 
	 \ac{RBC} and \ac{NK} models are often also labelled \ac{DSGE} models. They emerged as a response to the Lucas critique (1981), providing a micro-based approach in which parameters were no longer subject households behaviour \cite{lucas_jr_tobin_1981}. They instead describe static agent preferences \cite{gali_monetary_2008}. This property made DSGE models attractive for policy analysis, making them the predominant modelling approach across macroeconomics. In their success Bayesian estimation provided an important stepping stone by linking \ac{DSGE} models to data. This link provided means of estimating the time independent model parameters as well as to compare the models' data generating process against real data. DSGE models could thus be evaluated based on their fit to data and forecasting properties \cite{del_negro_forming_2008}. This provided an new criteria to the evaluation of structural models \cite{kocherlakota_model_2007}. Lastly, a body of literature on the link between oil prices and inflation in structural models has emerged. This work will review this literature and draw from one model in particular \cite{blanchard_macroeconomic_2007}. 
	 
	 The purpose of this work is to analyse the inclusion of petrol prices into the DSGE framework and to evaluate the pertinence of such exercise. In its evaluation this work will employ two standard models of macroeconomic literature, the RBC and NK model. Both will be compared to a petrol model in their fit to real data and forecasting performance . Lastly, a \ac{VAR} will be used as a forecasting benchmark. This work is organised as follows. First a literature review on structural models and their incorporation of oil price inflation will be provided. Three subsequent sections will be dedicated to developing the mathematical reasoning behind the RBC, the standard NK and lastly the \ac{NKE} model. Having demonstrated the assumptions involved in drawing up structural models and their linearisation this work will proceed in discussing the data used model evaluation. A fifth section will then be dedicated to introducing Bayesian estimation and forecasting procedures. The obtained estimation will  be discussed in a seventh section, while a final section will outline the results implications of this work.
	 
	\section{Literature Review}
	The RBC literature was the first to introduce structural models of time-invariant parameters. Its parameters are rooted in the optimising behaviour of representative agents and consequently convey their preferences. Preferences are thought off as time-invariant \cite{prescott_theory_1986}. The RBC literature further contributed in providing an argument on business cycle efficiency. Previously seen as inefficient divergence from the steady state, the RBC model suggests that in absence of nominal rigidities business cycles are driven by technology innovation and consequently are efficient adjustments towards the economy's steady state \cite{christiano_current_1992}. This notion of efficiency was subsequently questioned by New-Keynesian literature demonstrating that under nominal rigidities changes in the interest rate are not matched by immediate changes in expected inflation \cite{gali_monetary_2008}. Following the New-Keynesian argument business cycles are driven by interventionist monetary policy, causing a gap in agent expectation and thus inefficiency\cite{gali_monetary_2008}. Following the oil price induced stagflation crisis of the 1970s a rich body of economic literature has been dedicated to the effects of oil prices on inflation and the economy at large \cite{barsky_oil_2002, bernanke_systematic_1997}. The majority of these approaches were empiric. However, the growing DSGE literature was soon applied to questions of energy prices and inflation. In doing so two dominant approaches were pursued.
	
	Early analysis of oil prices and their interaction with the economy introduced petrol as an exogenous supply to the economy. As such models did not distinguish between crude oil and refined petrol imports. Instead, a unified petrol good enters the consumption basket consumption as well as the production function. The first such model was inspired by large differences in the inflationary response to oil price shocks \cite{blanchard_macroeconomic_2007}. Blanchard \& Galì (2007) extended the New-Keynesian model by petrol in order to reconcile these different responses under one framework \cite{blanchard_macroeconomic_2007}. Across this model imperfectly competitive firms optimise with regards to inputs of petrol and labour to their production. Moreover, petrol is consumed by households as a fixed share of overall consumption. The inflationary impact of exogenous petrol price is consequently propagated through the firm's elasticity of substitution between factors of production, namely petrol and \cite{blanchard_macroeconomic_2007}. As petrol gets more expensive production costs rise and firms substitute petrol for labour. This places the firms elasticity of substitution at the heart of petrol driven inflation. This first model has later been extended in complexity. It was shown that the introduction of capital accumulation amplifies the stagflationary effects of petrol on the economy \cite{acurio_vasconez_what_2015}. Advances in mainstream DSGE literature provided more sophisticated approaches to including wage rigidities, endowing households with wage setting power \cite{smets_shocks_2007}. This mechanism was then used to extend the model by including wage rigidities \cite{leduc_quantitative_2004}. With petrol entering into the consumption basket, households are directly concerned by higher petrol prices. Endowed with negotiation power they to some degree pass on higher prices to firms. This increases petrol's inflationary  pressure by broadening its channel of transmission \cite{leduc_quantitative_2004}.	This first approach to petrol was much focussed on the analysis of short-run shocks. Assumptions such as a fixed petrol share in consumption or an exogenous oil price are reasonable in the short-run, as prices surge exogenously. However, for analysing the long-run effects of petrol, outside of crisis-regimes this modelling approach lacked foundation. This lead to an extensions of the petrol modelling literature.
	
	With the adoption of DSGE modelling across many central banks complex models tailored to the conditions of national economies came about. Soon petrol was introduced as an important aspect to these models. Pioneered by the Bank of England, its small open economy model was extend by oil \cite{harrison_evaluating_2010, harrison_impact_2011}. Alongside this extension endogenous oil production was introduced \cite{harrison_impact_2011}. The model further relies on a distinction between crude oil and refined petrol, with the latter resulting from a separate sector of production. This is complemented by a utility sector, modelling the effect of gas prices on marginal electricity production \cite{harrison_impact_2011}. Both refined petrol and utilities factor into the consumption basket as well as into the domestic production sector. Consequently, the petrol inflation channel assumes a more complex dynamic, operating through production and household wage negotiation \cite{harrison_impact_2011}. Moreover, the possibility to trade has important implications as with rising petrol prices a substitution away from oil then allows for trading non-utilised domestic oil. Substitution thus not only avoids higher cost but turns into a source of revenue, which is an important aspect for long-run dynamics of petrol utilisation, hence inflation \cite{harrison_impact_2011}. Following the BoE much research has been conducted in tailoring models, containing amongst many other factors petrol, to the specific dynamics of national economies \cite{lees_introducing_2009, malakhovskaya_are_2014, hou_oil_2016}. 
	
	As this work's purpose is an analysis of petrol prices and their interaction with inflation it will rely on the model of Blanchard \& Galì (2008) \cite{blanchard_macroeconomic_2007}. This model focusses on the foundation of petrol inflation interaction while later approaches aim at a comprehensive analysis of the economy. In doing so more complex models confound many factors, not allowing for the clear cut analysis of the basic mechanism behind oil price inflation. This work aims at identifying the contribution of that mechanism and thus opts for simplicity. 
	
	\section{Real Business Cycle model}
	The Real Business Cycle model (RBC) was first introduced by Prescott (1986) \cite{prescott_theory_1986} attempting to explain the origin of business cycles. Departing from Schumpeterian technology induced growth, the RBC model is at its heart a de-trended growth model. It emphasises the link between technology growth to real wages and return to capital through increases in total factor productivity. Technology growth thus increases income, leading the economy to a new optimum \cite{prescott_theory_1986}. 
	In making this argument several assumption are required, namely: Perfect competition, flexible prices and the use of an identical production technology across firms. While the argument of efficient business cycles is antithetical to the New-Keynesian literature, who regards business cycles as inefficient, both share the same mathematical foundation. The following will therefore lay the foundation for the standard New-Keynesian model in deriving the RBC model.
	
	\subsection{The household}
	Households are assumed to have homogeneous preferences and to solve a simultaneous optimisation problem. Given these properties the RBC introduces a representative agent solving the problem for a continuum of households.
	The agent is faced with an income from labour wage $W_t$, returns on last periods capital holdings $K_{t-1}$ and firm dividends $\Pi_t$. Income is divided between consumption, capital investment $I_t$ and leisure, while the leisure expenditure is implicitly made by choosing the number of labour hours $L_t$. The RBC assumes a Robinson-Crusoe economic setting in which the household owns firms and consequently receive profit dividends. However, under assumptions of perfect competition firm profits are zero and consequently so are dividends at equilibrium \cite{prescott_theory_1986}. 
	Moreover, households are infinitely lived, thus solving their problem discounted into all future periods. Moreover, the utility function of households is assumed to take the following form.
	\begin{equation}
		U(C_t, L_t) = \frac{C_t^{1-\sigma}}{{1-\sigma}} - \frac{L_t^{1+\phi}}{1+\phi} 
	\end{equation}
	In this specification $\sigma$ is the inverse elasticity of intertemporal consumption substitution and $\phi$ is the marginal disutility to labour. While other utility specification have been employed across the RBC literature this work will remain with a separable utility function in order to be consistent with the standard New-Keynesian model. The function $U(C_t, L_t)$ exhibits the usual characteristics of a well behaved utility function $\frac{\partial U}{\partial C_t} > 0$, $\frac{\partial U}{\partial L_t} \leq 0$ and $\frac{\partial^2 U}{\partial C_t^2} \leq 0$, $\frac{\partial^2 U}{\partial L_t^2} \leq 0$.
	
	The household maximisation problem reads as follows, where investment factors into the capital stock and $K_t$ evolves over time based on the difference between quarterly inflows and depreciation $\delta \leq 1$.
	\begin{equation}
		\begin{aligned}
			\max_{B_t, C_t, L_t} \quad \sum_{\infty}^{t=0} \beta^{t} U(C_{t}, L_{t}) \\
			\textrm{s.t.} \quad C_t + I_t = R_t K_{t-1} + W_t N_t + \Pi_t \\
						  K_t = (1 - \delta) K_{t-1} + I_t \\
			\textrm{FOC:}\\
							 \quad - \frac{U_{n,t}}{U_{c, t}} = W_t \\
							 \quad	\E_t \left[ \frac{U_{c,t}}{U_{c,t+1}} \right]^{\sigma} = \beta \left( 1 - \delta + \E_t[R_{t+1}]\right)
		\end{aligned}
	\end{equation}
	The household optimisation yields two main conditions. The Euler equation describes the intertemporal consumption trade-off and the labour supply. Replacing the partial derivates of the utility function leads to the two main equations of the household's problem.
	\begin{equation} \label{eq:rbc_hh_foc}
		\begin{aligned}
			C_t^\sigma L_t^\phi	= W_t \\
			\E_t \left[ \frac{C_{t+1}}{C_t} \right]^\sigma = \beta \left(1 - \delta  + \E_t [R_{t+1}] \right)
		\end{aligned}
	\end{equation}

	\subsection{The firm}
	Just as households, firms are assumed to be identical in their production technology, wherefore the continuum of firms is replaced by a representative firm. The firm solves a static optimisation problem in choosing its production inputs capital and labour only for the period $t$. Due to the assumption of perfect competition and flexible prices the firms pays the marginal product to its production factors labour and capital. This is formalised below and yields the capital and labour demand equations.
	\begin{equation} \label{eq:rbc_firm_foc}
		\begin{aligned}
			\max_{N_t} \quad A_t K_t^\alpha N_t^{1 - \alpha} \\
			\textrm{s.t.} \quad Y_t = W_t N_t + K_t R_t\\
			\textrm{FOC:} \quad W_t = (1 - \alpha) A_t K_t^\alpha N_t^{-\alpha}\\
							\quad K_t = \alpha A_t K_t^{\alpha -1} N_t^{1-\alpha}
		\end{aligned}
	\end{equation}

	\subsection{Equilibrium}
	Having derived the above optimality condition an analysis of the equilibrium requires two further pieces. These are the equilibrium resource constraint arising from the output accounting identity $Y = I + C$ and an expression for technology $A_t$. The innovation of technology is assumed to be exogenous and path dependent, it is therefore modelled as a stochastic first order autoregressive process AR(1), where $\bar{A}$ represents the equilibrium value of technology.
	\begin{equation} \label{eq:rbc_eqil}
		\begin{aligned}
			Y_t = C_t + I_t \\
			\log(A_t) = (1- \rho_A) \log(\bar{A}) + \rho_A \log(A_{t-1}) + \epsilon_t^A
		\end{aligned}
	\end{equation}

	The derived system of equations \eqref{eq:rbc_eqil} \eqref{eq:rbc_firm_foc}, \eqref{eq:rbc_hh_foc} in its current state is non-linear. Non-linearity arises from the multiplicative Cobb-Douglas production function and partial depreciation of capital \cite{campbell_inspecting_1994}. In order to analyse the system's behaviour around its equilibrium and to link it to real data some form of linearisation is needed. The most common approach adopted across literature is a log-linear first order Taylor approximation, this work will follow suit \cite{campbell_inspecting_1994}. Deriving the log-linearisation yields the below system. Further reasoning required for the linearisation is provided in the \nameref{appc}.
	 
	\begin{table}[H]
		\caption{RBC Equations}
		\fontsize{10pt}{10pt}\selectfont
		\centering
		\begin{tabular}{llr}
			\textbf{Equation name} & Equation & \textbf{Log-linear expression}\\
			\hline
			Euler equation &
			$\E_t \left[ \frac{C_{t+1}}{C_t} \right]^\sigma = \beta \left[ (1 - \delta)  + \E_t [R_{t+1}] \right]$ &
			$\hat{c}_t = \hat{c}_{t+1} - \hat{r}_{t+1}$ \\
			Capital supply & 
			$K_t = (1 - delta) K_{t-1} + I_t$ &
			$\hat{i}_t = \delta \hat{k}_{t+1} - \frac{1-\delta}{\delta} \hat{k}_t$ \\
			Capital demand &
			$\hat{y}_t - \hat{k}_t = \hat{r}_t$ &
			$K_t = \alpha A_t K_t^{\alpha -1} N_t^{1-\alpha}$ \\
			Labour supply & 
			$- \frac{U_{n,t}}{U_{c, t}} = W_t$ &
			$\hat{c}_t = \hat{w}_t - \frac{\bar{l}}{1-\bar{l}} \hat{l}_t$ \\
			Labour demand &
			 $W_t = (1 - \alpha) A_t K_t^\alpha N_t^{-\alpha}$ & 
			 $\hat{y}_t - \hat{l}_t = \hat{w}_t$ \\
			Production function &
			$Y_t = A_t K_t^\alpha N_t^{1 - \alpha}$  &
			$\hat{y}_t = \hat{a}_t + \alpha \hat{k}_t + (1-\alpha) \hat{l}_t$ \\
			Equilibrium condition &
			$Y_t = C_t + I_t$ &
			 $\hat{y}_t = \frac{\bar{y}}{\bar{c}} \hat{c}_t + \frac{\bar{i}}{\bar{y}} \hat{i}_t$ \\
			Technology &
			 $\log(A_t) = (1- \rho_A) \log(\bar{A}) + \rho_A \log(A_{t-1}) + \epsilon_t^A$ &
			 $\hat{a}_t = \rho_a \hat{a}_{t-1} + \epsilon_{a,t}$ \\
		\end{tabular}
	\end{table}
	
	The above linearisation still contains some variable's steady state values indicated by $\bar{x}$. In order to express the model in linear form these need to be replaced. The RBC model with separable consumption labour utility, does however not possess a deterministic steady state for labour \cite{romer_advanced_2018}. This issues has sometimes been circumvented by employing a logarithmic specification of household utility \cite{campbell_inspecting_1994} \footnote{In the work of Campbell (1994) the analytical derivation relies on a log-utility function}. The below developed NK model relies on separable utility, in order to compare the two fairly this work will rely on separable utility. 
	Further analysis of the model can be provided in expressing the steady state values as ratios of labour \cite{romer_advanced_2018}. The deterministic labour ratios for the RBC model are listed below.
	
	\begin{table}[H]
		\fontsize{9pt}{9pt}\selectfont
		\centering
		\caption{RBC steady state}
		\begin{tabular}{lr}
			\textbf{Variable} & \textbf{Deterministic steady state}\\
			\hline 
			Rate of return & $\bar{R} = \Rss$ \\
			Capital labour ratio & $\frac{\bar{K}}{\bar{L}} = \KLss$ \\
			Wage & $\bar{W} = (1 - \alpha) \Ass \left(\KLss\right)^\alpha$ \\
			Investment labour ratio & $\frac{\bar{I}}{\bar{L}} = \delta \KLss$ \\
			Output labour ratio & $\frac{\bar{Y}}{\bar{L}} = \Ass \left(\KLss\right)^\alpha$ \\
			Consumption labour ratio & $\frac{\bar{C}}{\bar{L}} = \frac{\bar{Y}}{\bar{L}} - \frac{\bar{I}}{\bar{L}}$ \\
		\end{tabular}
	\end{table}

	In order to obtain a linear state-space representation of the RBC model a numerical approximation of the steady state for the value of labour $\bar{l}$ is required. This work relies on the software package gEconpy \cite{jessegrabowski_geconpy_2023} for this numerical approximation. The specification of the input file, can be found in the \nameref{appe}. The resulting steady-state values and the state-space transition matrix can be found in the \nameref{appb}. 
	Having specified the steady state values and log-linear equations, the system's stability has to be checked. This work relies on the Eigenvalue condition of Blanchard-Kahn (1980), which for each value of the parameter vector will be checked using gEconpy \cite{blanchard_solution_1980}.

	\section{Standard New-Keynesian model} \label{sec:NK}
	The New-Keynesian model developed in this section is based on the work of Galì (2008) \cite{gali_monetary_2008}. Moreover, this section has drawn greatly from the explanations of \cite{bergholt_basic_2023}. It diverges from the RBC model in two key aspects.
	
	First of all, the NK model introduces nominal rigidities through monopolistic competition. Firms being endowed with some market power allows them to reset prices above marginal cost. This introduces a gap between real and nominal marginal cost, resulting in inflation. This extension has several consequences. This is, changes in the interest are note directly matched by changes in expected inflation, hence monetary policy is neutral in the short-run \cite{gali_monetary_2008}. The expectation gap then becomes the driver of short-run fluctuation in the economy. Business cycles are thus no-longer driven by technological progress but result from interest rate adjustments. The standard New-Keynesian model therefore presents a perspective in which a monetary policy interventions cause inefficient short-term adjustments of the economy \cite{gali_monetary_2008}. The NK returns to Keyne's (1936) perspective, who regarded business cycles as inefficient. 	
	Second of all the NK model in its standard form analyses an economy without capital. Capital stock is thus no longer the intertemporal instrument, instead the model relies on the gap between actual and nominal output \cite{gali_monetary_2008}.
		
	\subsection{Consumption bundling} \label{nk_bundle}
	The \ac{NK} framework introduces imperfectly substitutable goods as source of market power, resulting in monopolistic competition \cite{gali_monetary_2008}. Consequently, firms are able to set a price beyond marginal cost. Moreover, imperfect substitution requires goods to be considered as a continuum, hence the RBC's assumption of a singular good and firm no longer hods. As a result the households allocation problem no is twofold, encompassing the composition of the optimal consumption basket and the consumption-leisure trade-off \cite{gali_monetary_2008}.
	
	Households need to determine the basket of products that optimises their utility. Only once the optimal basket and its price is determined the household is able decide on the optimal allocation of income across consumption and leisure. The consumption basket problem thus needs to be solved prior to the consumption-leisure trade-off. Across the DSGE literature the aggregation of consumption is also sometimes referred to as the output of a good bundling firm. This intermediary party determines the optimal basket of goods according to the household preferences. In a perfectly competitive environment the household buys the optimal bundle at no mark up beyond the producer's mark-up \cite{gali_monetary_2008}.
	Determining the optimal basket requires some function of aggregation across the continuum of goods. The most commonly employed aggregator is the Dixit-Stiglitz function \cite{dixit_monopolistic_1977}. This aggregator has appealing properties of a \ac{CES}. The CES will will become important later in deriving marginal cost.
	
	The bundling optimisation problem reads as follows where each good $C_{it}$ is matched by its price $P_{it}$ purchased with budget $Z_t$. The elasticity of substitution is given by $\epsilon$.
	\begin{equation}
		\begin{split}
			\max_{C_{it}}
			\quad \left(\int_{0}^{1} C_{it}^{ \frac{\epsilon - 1}{\epsilon} } di 
			\right)^{ \frac{\epsilon}{\epsilon - 1} }\\
			\textrm{s.t.}\\
			\quad \int_{0}^{1} P_{it} C_{it} di \leq Z_t
		\end{split}
	\end{equation}
	Solving the above optimisation problem results in three important expression. First, the aggregated price level $P_t$ of the optimal consumption bundle is derived. This corresponds to the consumer price index (CPI).
	\begin{equation} \label{eq:1}
			P_t = \left(
						\int_{0}^{1} P_{it}^{ 1 - \epsilon } di 
					\right)^{ \frac{1}{1 - \epsilon} }
	\end{equation}
	Further manipulation leads to the share of product $C_{it}$ in the consumption basket $C_t$. This share is defined by the goods price relative to the price level $P_t$ and the CES between goods $\epsilon$.
	\begin{equation} \label{eq:cshare}
		C_{it} = \left( \frac{P_{it}}{P_t} \right)^{- \epsilon} C_t
	\end{equation}
	Lastly, using the two above it can be demonstrated that under a binding budget constraint individual good prices correspond to the CPI times the optimal consumption bundle.
	This property is important as it allows to solve the household and firm optimisation problem as a representative agent problem.
	\begin{equation}
		\int_{0}^{1} P_{it} C_{it} di = Z_t = P_t C_t
	\end{equation}

	\subsection{Household} \label{nk_household}
	Having found $C_t$ and its price $P_t$ the household's consumption-leisure allocation can be formulated. The derivation follows from the RBC with the exception of omitted investment expenditure and an extension by bond holdings $B_t$. While $B_t$ is zero in equilibrium it introduces the nominal interest rate into the households problem, contained in the discount factor $Q_t = \frac{1}{1+i_t}$. This is the first step to introducing a monetary authority. Similar to the RBC a representative infinitely lived agent solves the optimisation problem for all future periods. 
	\begin{equation}
		\begin{aligned}
			\max_{B_t, C_t, L_t} \quad \sum_{\infty}^{t=0} \beta^{t} U(C_{t}, L_{t}) \\
			\quad P_t C_t + Q_t B_t = B_{t-1} + W_t N_t \\
			\textrm{FOC:} \\
				- \frac{U_{n,t}}{U_{c, t}} = \frac{W_t}{P_t}\\
				Q_t = \beta \E_t \left[ \frac{U_{c, t+1}}{U_{c,t}}^\sigma \frac{P_t}{P_{t+1}} \right] 			
		\end{aligned}
	\end{equation}
	Solving the optimisation problems yields the labour supply, which now explicitly depends on the real wage. Moreover, the Euler equation is obtained, which now contains nominal interest $i_t$ instead of the rental rate of capital as well as the inflations inverse $\Pi_{t+1}^{-1} \equiv \frac{P_t}{P_{t+1}}$. Assuming the function form of $U(C_t, L_t) = \frac{C_t^{1-\sigma}}{{1-\sigma}} - \frac{L_t^{1+\phi}}{1+\phi}$ and plugging its partial derivatives into the first order condition yields.
		\begin{equation} \label{eq:nk_hh_foc}
		\begin{aligned}
			C_t^\sigma L_t^\phi	= \frac{W_t}{P_t} \\
			Q_{t} = \E_t \left[ \frac{C_{t+k}}{C_t} \right]^\sigma \frac{P_t}{P_{t+1}}
		\end{aligned}
	\end{equation}
	
	\subsection{Price setting} \label{nk_price_setting}
	The main innovation of the NK model lies in modelling inflation. As mentioned in the above inflation arises from the imperfect substitutability of consumption goods, endowing firms with price setting power. Modelling the price setting process is consequently at the heart of the NK model. In doing so the standard model relies on the Calvo-pricing mechanism \cite{calvo_staggered_1983}. Firms are able to reset prices in any period with the probability $\theta$. This is equal to saying that every period a share $\theta$ of firms can reset its price to its optimum $\hat{P}_t$ while $1-\theta$ firms remain with last periods price $P_{t-1}$. This allows to describe the evolution of the price level as an in the following intertemporal equation, which if divided by $P_{t-1}$ yields an expression for inflation.
	\begin{equation}
		P_t = 
		\left[
		\theta P_{t-1}^{1 - \epsilon} + (1 - \theta) \hat{P}_t^{1 - \epsilon}
		\right]^{\frac{1}{1 - \epsilon}}
	\end{equation}

	\begin{equation} \label{eq:inflation}
		\Pi_t^{1-\epsilon} = \theta + (1 - \theta) \left(\frac{\hat{P}_t}{P_{t-1}} \right)^{1-\epsilon}
	\end{equation}
	The key aspect to the price evolution problem lies in the optimal price $\hat{P_t}$, all other variables are known. The further $\hat{P}_t$ diverges from last periods price, the higher inflation in the period. 
	
	\subsection{The firm} \label{nk_the_firm}
	In the RBC the firm solve a static optimisation problem providing labour and capital demand. In the NK's uni-factor production function the static optimisation collapsed to the production function. Moreover, firms are faced with setting the optimal price $\hat{P}_t$. For this a continuum of goods is assumed, where each firms produces one good. Firm output can therefore also be seen as a continuum and be summarised by the Dixig-Stiglitz aggregator \cite{dixit_monopolistic_1977}. 
	
	In setting the optimal price firms face the trade-off between two opposing forces. On the one hand, as illustrated by \eqref{eq:cshare} the share of any good in the overall consumption basket depends on its price relative to the CPI. This implies that setting a price far above $P_t$ leads to a lower demand for a firm's single good $C_{i,t}$. Firms thus have an interest of setting a price close to the CPI.
	On the other hand, as firms are limited in their price setting frequency a price needs to satisfy future cost constraints. Under the Calvo-scheme a set price will be unchanged with probability $\theta^k$ for $k$ periods. If this price is too low it will at some point occur a loss. This provides the incentive of setting the highest price possible, in order to maximise future profits.
	By implication, firms aim at a that allows for an acceptable share the basket of goods while also maximising profitability across future periods. In this decision the size of $\theta \in [0,1]$ combined with the time discount factor $Q_t \beta$ weights the two forces against each other. 
	The problem can be formalised as a future discounted sum of revenue minus total cost. The notation of $Y_{t+k|t}$ hereby refers to the income in period $t+k$ at price of period $t$. The notation of $Q_{t,t+k}$ is the stochastic discount factor of today's discount carried forward into period $t+k$. The budget constraint makes use of the fact that under equilibrium assumption all output is consumed as part of the consumption basket $Y_{it} = C_{it}$, Section~\ref{nk_eqili} will discuss this further.	
	\begin{equation}
		\begin{aligned}
			\max_{\hat{P}_t}
			\quad
			\sum_{k=0}^{\inf} \theta^k \E_t 
			\left[
			Q_{t, t+k} 
			\left(
			\hat{P}_t Y_{t+k|t} - TC_{t+k|t}^n(Y_{t+k|t})
			\right)
			\right] \\
			\textrm{s.t.}
			\quad
			Y_{it+k|t} = \left(\frac{\hat{P}_t}{P_{t+k}} \right)^{-\epsilon} C_{t+k} \\
			\textrm{FOC:} \quad
			\sum_{k=0}^{\infty} \theta^k \E_t 
			\left[
			Q_{t,t+k} Y_{t+k|t} 
			\left(
			\hat{P}_t - \frac{\epsilon}{1 - \epsilon} MC_{t+k|t}^n
			\right)
			\right]
			= 0
		\end{aligned}
	\end{equation}
	
	Inserting the budget constraint and the discount factor as defined by the Euler condition \eqref{eq:nk_hh_foc} results in:
	\begin{equation}
		\frac{\hat{P}_t}{P_t} = \frac{\epsilon}{\epsilon-1} 
		\frac{
		\E_t \sum_{k=0}^{\infty} \theta^k \beta^k C_{t+k}^{1-\sigma} (\frac{P_{t+k}}{P_t})^\epsilon MC_{t+k}^r
		}{
		\E_t \sum_{k=0}^{\infty} \theta^k \beta^k C_{t+k}^{1-\sigma} (\frac{P_{t+k}}{P_t})^{\epsilon-1}
		}
	\end{equation}
	The above describes the mark-up of the optimal price $\hat{P}_t$ beyond the price level CPI. Recalling the definition of inflation \eqref{eq:inflation} illustrates the direct link of the mark-up to inflation. 
	
	A second property can be obtained from the above. Recognising that inflation is zero in the steady state implies that firms do not reset prices at equilibrium. This is equivalent to setting $\theta=0$. Multiplying the above by $P_t$ and setting $\theta=0$ results in the expression of price and marginal cost $\hat{P}_t = \frac{\epsilon}{\epsilon-1} MC_t^n$. This allows to derive an important aspect to the NK model, the price-mark up over nominal marginal cost in the steady state, resulting from firm market power. Defining real marginal as $MC_t^r \equiv \frac{MC_t^n}{P_t}$ and using the that in the steady state $\hat{P}_t = P_t$ the steady state mark-up $MC^r$ can be derived as:	
	\begin{equation} \label{eq:nk_mark-up}
		MC^r = \frac{\epsilon}{\epsilon-1}
	\end{equation} 
	This allows to define the gap between marginal-cost and its steady state value is defined as $\hat{MC}_t^r \equiv MC_t^r - MC^r$.
	
	\subsection{Equilibrium conditions}  \label{nk_eqili}
	To this point three optimal choices have been derived. The households decides on the consumption basket and consumption-leisure trade-off, firms set their optimal price. Closing the model now requires a market clearing condition connecting these optimality conditions. Market clearing implies that all individual output is consumed yielding the accounting identity $Y_{it} = C_{it}$. In order to close the model one needs to demonstrate that this property holds on an economy wide level $Y_t = C_t$ as well.
	
	Overall output $Y_t$ is given by an aggregator function. Using the accounting identity combined with \eqref{eq:cshare} allows to link $Y_t$ to its relative price. Rephrasing then allows to obtain the accounting identity on a global level. 	
	\begin{equation}
			Y_t = 
			\left( 
				\int_{0}^{1} Y_{it}^{\frac{\epsilon - 1}{\epsilon}} di 
			\right)^{\frac{\epsilon}{\epsilon - 1}}
			=
			\left( 
			\int_{0}^{1} 
			\left[
			\left( \frac{P_{it}}{P_t} \right)^\epsilon C_t
			\right]^{\frac{\epsilon - 1}{\epsilon}} di 
			\right)^{\frac{\epsilon}{\epsilon - 1}}
			=
			P_t^{\epsilon} C_t P_t^{-\epsilon}
			=
			C_t
	\end{equation}
	A similar reasoning is required in order to derive an economy wide production function from the firms' individual production functions $Y_{it} = A_t N_{it}$. This yields the below expression:
	\begin{equation}
		N_t = 	
		\int_{0}^{1} \left( \frac{Y_t}{N_{it}} \right)^{\frac{1}{1 - \alpha}}
		\int_{0}^{1} \left( \frac{P_{it}}{P_t} \right)^{-\frac{\epsilon}{1 - \alpha}} di
	\end{equation}
	For this expression it can be shown that up to a second order approximation $\int_{0}^{1} \left( \frac{P_{it}}{P_t} \right)^{-\frac{\epsilon}{1 - \alpha}} \approx 1$, wherefore labour can be expressed as \cite{gali_monetary_2008}:
	\begin{equation}\label{eq: Nt}
		N_t = \int_{0}^{1} \left( \frac{Y_t}{N_{it}} \right)^{\frac{1}{1 - \alpha}}
	\end{equation}
	
	\subsection{Log linearisation} \label{nk_log_lin}
	This completes the number of equations required for the derivation of the New Keynesian model. As the above equations are a non-linear system without a closed form solution some method of linearisation is required. The below will follow the procedure of \cite{gali_monetary_2008} and apply log-linearisation first order Taylor approximations. Throughout the log-linearisation some further reasoning is needed for the Euler equation. This can be found in \nameref{appc}. In the below lower-case expressions are the log-equivalent to previous upper case variables $\ln(X_t) \equiv x_t$.\\
		
	Euler equation
	\begin{equation} \label{eq:lleuler}
		c_t = \E_t[c_{t+1}] - \frac{1}{\sigma} (i_t - \rho - \E_t [\pi_{t+1}])
	\end{equation}
	Labour supply 
	\begin{equation} \label{eq:llslabour}
		w_t - p_t = \sigma c_t + \phi n_t
	\end{equation}
	Inflation
	\begin{equation}\label{eq:llpi}
		\pi_t = (1 - \theta) (\hat{p}_t - p_{t-1})	
	\end{equation}
	Optimal price 
	\begin{equation}\label{eq:llpstar}
		\hat{p}_t = (1 - \theta \beta) \E_t
		\left[
		\sum_{k=0}^{\infty} \theta^k \beta^k \left( mc_{t+k|t}^r - mc^r +p_{t+k}\right)
		\right]		
	\end{equation}
	Production function
	\begin{equation}\label{eq:llprod}
		y_t = a_t + (1 - \alpha) n_t
	\end{equation}
	Equilibrium condition 
	\begin{equation}\label{eq:lleq}
		y_t = c_t
	\end{equation}

	\subsection{New Keynesian Phillips and IS curve} \label{nk_nkp_is}
	The next step is the derivation of an expression for real marginal cost $mr_{t}^r$ contained in the optimal price setting. In the absence of other production factors the marginal product of labour $MPN_t = \frac{\partial Y}{\partial N} = A_t (1- \alpha) N_t^{-\alpha}$ corresponds to nominal marginal cost $mc_t^n$. Real marginal cost can thus be obtained as $MC_t^r = \frac{W_t}{P_t MCN_t}$. This allows to formulate the below property in logs by including the production function \eqref{eq:llprod}.
	\begin{equation} \label{eq:26}
		\begin{aligned}
			mc_t^r = w_t - p_t - a_t - mpn_t \\
%			mc_t^r = w_t - p_t - a_t - \ln(1 - \alpha) + \alpha n_t \\
			mc_t^r = w_t - p_t - \frac{a_t - \alpha y_t}{1 - \alpha} - \ln(1 - \alpha) \\
		\end{aligned}
	\end{equation}
	This expression supplies two essential conclusions. 
	Firstly, it allows to derive a relation between $mc_t^r$ and output $y_t$. Using labour supply \eqref{eq:llslabour} and the production function \eqref{eq:llprod} one can rephrase the above as. 
	\begin{equation}
		\begin{aligned}
			mc_t^r = \sigma c_t + \phi n_t - [a_t + \alpha n_t + \ln(1-\alpha)] \\
			mc_t^r = 
			\frac{
				\sigma (1 - \alpha) + \phi + \alpha
			}{
				(1 - \alpha)	
			}	 y_t
			- \frac{
				(1 + \phi)	
			}{
				(1 - \alpha)	
			} a_t
			+ \ln(1-\alpha)
		\end{aligned}
	\end{equation}
	In the steady state marginal cost should be equal to its natural level and $mc_t^r = mc^r$. In this case the above describes natural output $y_t^n$. This fact can be used to derive an expression for the gap between marginal cost and its steady state value in logs $\hat{mc_t}^r \equiv mc_t^r - mc^r$. Reformulating the above by replacing $mc_t^r$ by $mc^r$ yields an expression of the marginal cost gap and the output gap defined as $\tilde{y}_t \equiv y_t - y_t^n$ . 
	\begin{equation} \label{eq:llmcrhat}
		\begin{aligned}
			\hat{mc_t}^r = \frac{\sigma (1 - \alpha) + \phi + \alpha}
			{1 - \alpha)} \tilde{y}_t
		\end{aligned}
	\end{equation}
	The second insight arising from \eqref{eq:26} is its translation in period $t+k$. Combined with the share of consumption good $i$ \eqref{eq:cshare} and the market clearing condition \eqref{eq:lleq} results in:
	\begin{equation}
		y_{t+k|t} = -\epsilon(p_{t+k|t} - p_{t+k}) + y_{t+k}
	\end{equation}
	Brought to period $t+k$ an expression of marginal cost and the price level can be derived.
	\begin{equation}
		mc_{i, t+k|t}^r = mc_{t+k}^r - \frac{\epsilon \alpha}{1 - \alpha}(\hat{p}_t - p_{t+k})
	\end{equation}
	The above combined with the expression for the optimal price level \eqref{eq:llpstar} yields:

	\begin{equation}
		\begin{aligned}
			\hat{p}_t - p_{t-1} =
			\quad
			(1 - \theta \beta) \E_t
			\left[
			\sum_{k=0}^{\infty} \theta^k \beta^k \left( mc_{t+k|t}^r - mc^r +p_{t+k} - p_{t-1}\right)
			\right] \\			
			=
			\quad
			(1 - \theta \beta) \Theta \E_t
			\sum_{k=0}^{\infty} \theta^k \beta^k (mc_{t+k}^r - mc^r) + 
			\sum_{k=0}^{\infty} \theta^k \pi_{t+k} \\
			=
			\quad
			\theta \beta \E_t (\hat{p}_{t+1} - p_{t}) + (1 - \theta \beta) \Theta (mc_{t+k}^r - mc^r) + (1 - \theta) (\hat{p}_t - p_{t-1})
		\end{aligned}		
	\end{equation} 
	Writing the above in more compact form leads to an expression of inflation and marginal cost gap \footnote{where $\Theta \equiv \frac{1 - \alpha}{1 - \alpha + \alpha \epsilon} \leq 1$} \footnote{	where $\lambda \equiv \frac{(1-\theta)(1-\beta\theta)}{\theta} \Theta$}.
	\begin{equation}
		\pi_t = \beta E_t [\pi_{t+1}] + \lambda \hat{mc}_{t}
	\end{equation}
	The above combined with \eqref{eq:llmcrhat} allows to construct the \ac{NKP}, on of the two core equations of the NK model\footnote{where $\kappa = \lambda \frac{\sigma (1 - \alpha) + \phi + \alpha}{1 - \alpha)}$}.
	\begin{equation} \label{eq:llnkp}
		\begin{aligned}
			\pi_t = \beta E_t [\pi_{t+1}] + \kappa \tilde{y}_t
		\end{aligned}
	\end{equation}
	The second core equation of the New-Keynesian model is derived from the Euler equation \eqref{eq:lleuler}, the equilibrium condition \eqref{eq:lleq} and the log linear Fisher identity $r_t \equiv i_t - \E_t[\pi_{t+1}]$. Inserting $y_t$ for $c_t$ and introducing the yields the \ac{IS}:
	\begin{equation}  \label{eq:llnkis}
		\begin{aligned}
			\tilde{y}_t = 
			\left[
			\E_t[y_{t+1}] - \frac{1}{\sigma} (i_t - \rho - \E_t[\pi_{t+1}])
			\right]
			-
			\left[
			\E_t[y_{t+1}^n] - \frac{1}{\sigma} (r_t^n - \rho)
			\right] \\
			\tilde{y}_t = \E_t[\tilde{y}_{t+1}] - \frac{1}{\sigma} (i_t - r_t^n - \E_t[\pi_{t+1}])
		\end{aligned}
	\end{equation}
	
	Having arrived at the core of the New-Keynesian model the New-Keynesian Phillips Curve \eqref{eq:llnkp} and the Investment-Savings curve \eqref{eq:llnkis} deserve some perspective. 
	The Phillips curve relates inflation not to unemployment but to the output gap, a relation also established in traditional Keynesian literature through Okun's law \cite{schumpeter_general_1936}. The NKP describes expected inflation as an autoregressive process of order one plus a multiple of the output gap. In doing so the \ac{NKP} uses the mark-up channel to describe inflation. Firms set prices according to the optimisation programme of Section~\ref{nk_the_firm}, where their mark-up collapsed to the its natural level in the steady state. Inflation expectation is accordingly driven by the firms, putting little emphasis on the consumer. In more complex models the consumer expectation is introduced through wage negotiation power \cite{smets_shocks_2007}. However, in the NK model regardless of complexity inflation remains a push-mechanism of optimal mark-ups beyond marginal cost. As cost rise prices are gradually adjusted through the mark-up chain resulting in inflation outside of the equilibrium.
	
	The New-Keynesian \ac{IS} curve results from the Euler equation and as such brings the consumer's behaviour to the equilibrium. Output gap expectations are an AR(1) process plus a term of the real interest rate gap. As the real interest rate diverges from its equilibrium value future output is adjusted according to the elasticity of consumption $\sigma$. This represents the steering capability of interest rates on the economy as theorised by Keynesian literature. While in the traditional Keynesian model interest rates affect output through investment the NK model derives a similar effect solely based on consumption. 

	\subsection{Monetary policy}
	Having derived the core equations of household and firm optimality closing the model requires an equation characterising the monetary authority. This work will rely on a traditional Taylor rule with an exogenous interest rate shock following Galì (2008) \cite{gali_monetary_2008}.
	\begin{equation}
		\begin{aligned}
			i_t = \rho + \phi_{\pi} \pi_t + \phi_{y} \tilde{y}_t + v_t \\
			v_t = \rho_v v_{t-1} + \epsilon_t^v \\
			\epsilon_{v,t} \sim N(0, \sigma_{\epsilon_v})
		\end{aligned}
	\end{equation}
	
	At this point the NK model is complete, across this work it has been implemented using gEconpy \cite{jessegrabowski_geconpy_2023}. The log-linear model specification provided to the software can be found in \nameref{appe}. As with the RBC the Eigenvalue stability condition of Blanchard-Kahn (1980) have been verified for each specification using gEconpy \cite{blanchard_solution_1980}.

	\section{New-Keynesian energy model}
	The below is an extension of the standard NK model by petrol as a consumption good and factor of production. It is based on the work of Blanchard \& Galì (2008) \cite{blanchard_macroeconomic_2007}. Across this model petrol is assumed to be an import good with an exogenous price. Beyond petrol there is no foreign trade and all closed economy assumptions hold, making the model very similar to the standard NK model. The below therefore departs from Section~\ref{sec:NK} pointing out the differences and extensions to the standard model.
	
	\subsection{Consumption bundling}
	Petrol is considered a consumption good and as such enters into the good basket $C_t$. This of course affects the bundling process now encompassing domestic and foreign goods. The bundling of domestic goods follows from Section~\ref{nk_bundle}. Variables $C_{q,t}$ and $P_{q,t}$ are aggregated consumption and its price. The optimisation across the continuum of goods results in the same properties as with the NK model. These are share of good $C_{i,q,t}$ in the consumption basket $C_{q,t}$ and the aggregated price index $P_{q,t}$ as well as an expression of aggregate consumption.
	\begin{equation} \label{eq:o_pindex}
		P_{q,t} = \left( \int_{0}^{1} P_{i,q,t}^{1 - \epsilon} di \right)^{\frac{1}{1-\epsilon}} \\
	\end{equation}
	\begin{equation} \label{eq:o_cshare}
		C_{i,q,t} = \left( \frac{P_{i,q,t}}{P_{q,t}} \right)^{-\epsilon} C_{q,t}
	\end{equation}
	\begin{equation} \label{eq:o_pcon}
		\int_{0}^{1} P_{it} C_{it} di = Z_t = P_t C_t
	\end{equation}
	Foreign goods consumption $C_{m,t}$ exclusively consists of petrol and its share $\chi$ in overall consumption $C_t$ is assumed to be constant. The assumption of a constant share is supported by empirical evidence for the short-run. As replacing durable petrol consuming goods (e.g. cars) is expensive, households cannot substitute away from petrol in the short-run \cite{blanchard_macroeconomic_2007}. Overall consumption is defined as: \footnote{where $\Theta = \chi^{-\chi}(1-\chi)^{(\chi-1)}$}
	\begin{equation}
		C_t \equiv \Theta_\chi C_{m,t}^\chi C_{q,t}^{1-\chi}
	\end{equation}
	The over overall price level $P_{c,t}$ of the consumption basket $C_t$ can then be derived, introducing $S_t \equiv \frac{P_{m,t}}{P_{q,t}}^\chi$, the real price of oil.
	\begin{equation}
		P_{c,t} \equiv P_{m,t}^\chi P_{q,t}^{1-\chi} 
			= P_{q,t} \frac{P_{m,t}}{P_{q,t}}^\chi
			= P_{q,t} S_t^\chi
	\end{equation}
	Following this reasoning and using \eqref{eq:o_pcon} an expression for overall consumption can be written as:
	\begin{equation}
		C_t P_{c,t} = C_{m,t}P_{m,t} + C_{q,t}P_{q,t}
	\end{equation}

	\subsection{The household}
	Having derived expressions for the consumption basket and its price the household's optimisation problem can be solved. As in the standard NK model this reads as follows. 
% log-utility	
	\begin{equation}
		\begin{aligned}
			\max_{B_t, C_t, L_t} \quad \sum_{\infty}^{t=0} \beta^{t} U(C_{t}, L_{t}) \\
			\textrm{s.t.} \quad P_{c,t} C_t + Q_t B_t = B_{t-1} + W_t N_t \\
			\textrm{FOC:} \\
			- \frac{U_{n,t}}{U_{c, t}} = \frac{W_t}{P_{c,t}}\\
			Q_t = \beta \E_t \left[ \left( \frac{U_{c, t+1}}{U_{c,t}} \right)^\sigma \frac{P_{c,t}}{P_{c,t+1}} \right] 			
		\end{aligned}
	\end{equation}
	
	The authors use a log-utilty function $U(C_t,L_t) \equiv \log(C_t) - \frac{N_t^{(1+\phi)}}{1+\phi}$ which results in the following first order conditions.
	\begin{equation}
		\begin{aligned}
			Q_t = \beta \E_t \left[ \frac{C_t}{C_{t+1}} \frac{P_{c,t}}{P_{c,t+1}} \right] \\
			\frac{W_t}{P_{c,t}} = C_t N_t^\phi
		\end{aligned}
	\end{equation}

	\subsection{The firm}
	Now faced by a two-factor production function firms are conduct static optimisation as in the RBC, obtaining the optimal factor demands by equalising marginal costs.  
	\begin{equation}
		\begin{aligned}
%			\max_{M_{i,t}} \quad A_t M_{i,t}^{\alpha_m}
			\max_{M_t} \quad A_t M_{it}^{\alpha_m} N_{it}^{\alpha_n} \\
			\textrm{s.t.} \quad Y_{i,q,t} = W_t N_{i,t} + M_{i,t} P_{m,t}\\
			\textrm{FOC:} \\
			\quad W_{t} = \alpha_n A_t M_{i,t}^{\alpha_m} N_{i,t}^{\alpha_n -1}\\
			\quad P_{m,t} = \alpha_m A_t M_{i,t}^{\alpha_m -1} N_{i,t}^{\alpha_n}
		\end{aligned}
	\end{equation}
	An expression for nominal marginal cost $MC_t^n$ thus can be defined at the intersection of marginal labour and petrol costs. 
	\begin{equation}
		MC_t^n = \frac{W_{i,t}}{\alpha_n Y_{i,q,t} N_{i,t}^{-1}} = \frac{P_{m,t}}{\alpha_m Y_{i,q,t} M_{i,t}^{-1}}
	\end{equation}
	Using the above an expression for the firm's gross mark-up is defined as $\xi_{i,t} \equiv \frac{P_{q,t}}{MC_{i,t}^n}$. With some further manipulation Blanchard \& Galì (2007) use this to obtain the demand for petrol \cite{blanchard_macroeconomic_2007}.
	\begin{equation}
		\begin{aligned}
		\xi_{i,t} \frac{P_{m,t}}{P_{q,t}} M_{i,t} = \alpha_m Q_{i,t} \frac{P_{i,q,t}}{P_{q,t}} \\
		M_{i,t} = \alpha_m \frac{Q_{i,t}}{S_t \xi_{i,t}} \frac{P_{i,q,t}}{P_{q,t}}
	\end{aligned}
	\end{equation}

	\subsection{Price setting}
	The price setting problem for domestic goods follows straightforward from the NK model in Section~\ref{nk_price_setting}. \ac{CPI} inflation is obtained from the ratio $\Pi_{q,t} = \frac{P_{q,t}}{P_{q,t-1}}$.
	\begin{equation}
		P_{q,t} = 
		\left[ 
		\theta P_{q,t-1}^{1 - \epsilon} + (1 - \theta) \hat{P}_t^{1 - \epsilon}
		\right]^{\frac{1}{1 - \epsilon}}
	\end{equation}
	The firm solves the same optimisation problem as in Section~\ref{nk_price_setting}. 
	\begin{equation}
		\begin{aligned}
			\max_{\hat{P}_t}
			\sum_{k=0}^{\inf} \theta^k \E_t 
			\left[
			Q_{t, t+k} 
			\left(
			\hat{P}_t Y_{t+k|t} - TC_{t+k|t}^n(Y_{t+k|t})
			\right)
			\right] \\
			\textrm{s.t.}\\
			\quad
			Y_{it+k|t} = \left(\frac{\hat{P}_t}{P_{t+k}} \right)^{-\epsilon} C_{t+k} \\
			\textrm{FOC:} \\ 
			\quad
			\sum_{k=0}^{\infty} \theta^k \E_t 
			\left[
			Q_{t,t+k} Y_{q,t+k|t} 
			\left(
			\hat{P}_{q,t} - \frac{\epsilon}{1 - \epsilon} MC_{t+k|t}^n
			\right)
			\right]
			= 0
		\end{aligned}
	\end{equation}

	\subsection{Equilibrium}
	The equilibrium conditions follow the standard NK model in assuming the resource constraint $Y_{i,q,t} = C_{i,q,t}$ on individual firm level. As showed in Section~\ref{nk_eqili} overall domestic output can be linked to consumption of domestic goods as follows: 
	\begin{equation}
		Y_{q,t} = \left( \int_{0}^{1} Y_{it}^{1-\frac{1}{\epsilon}} di \right)^{\frac{\epsilon}{\epsilon - 1}} = C_{q,t}
	\end{equation}
	Using this fact and the resource constraint casted into the \eqref{eq:o_cshare}, resulting in $Y_{q,t} = \frac{P_{i,q,t}}{P_{q,t}}$ allows to derive an expression for oil as a production factor solely depending on domestic output.
	\begin{equation} \label{eq:oil_prod}
		M_t = \frac{\alpha_m Y_{q,t}}{\xi_{t} S_t}
	\end{equation}
	Using \eqref{eq:oil_prod} and the fact that bond holdings are zero in equilibrium allows to derive an expression for the overall price level, factoring out petrol:
	\begin{equation}
		P_{c,t}C_t = P_{q,t} C_{q,t} - P_{m,t}M_t = \left(1 - \frac{\alpha_m}{\xi_t} \right) P_{q,t} Y_{q,t}
	\end{equation}
	To this point the model is closed and one could proceed to log-linearisation.
	However, no expression for overall output has been derived yet. The model can in this state not be linked the real data, as domestic output $Y_{q,t}$ account for petrol consumption. The solution proposed by Blanchard \& Galì (2007) is to define a GDP deflator $P_{y,t}$ trough which overall GDP $Y_t$ can be obtained off domestic output $Y_{q,t}$ \cite{blanchard_macroeconomic_2007}.
	\begin{equation}
		\begin{aligned}
		P_{q,t} \equiv P_{y,t}^{1-\alpha_m} P_{m,t}^{\alpha_m}
		P_{y,t} Y_t \equiv P_{q,t} Y_{q,t} - P_{m,t} M_t = \left( 1 - \frac{\alpha_m}{\xi_t^n} \right) P_{q,t} Y_{q,t}
		\end{aligned}
	\end{equation}
		
	\subsection{Log linearisation}
	The model be log-linearise in the following form, closely following the standard NK model.
	\begin{table}[H]
		\centering
		\fontsize{9pt}{9pt}\selectfont
		\caption{New-Keynesian Energy model log-linear equations}
		\begin{tabular}{lll}
			Equation name & Equation & Log-linear Equation \\
			\hline
			 Euler equation & 
			 $Q_t = \beta \E_t\left[ \frac{C_{q,t}}{C_{q,t+1}} \frac{P_{q,t}}{P_{q,t+1}} \right]$ &
			 $c_t = \E_t [c_{t+1}] - (i_t - \E_t[\pi_{q,t+1}] - \rho)$ \footnotemark \\
			 
			 Labour supply &
			 $\frac{W_t}{P_{c,t}} = C_t N_t^\phi$ &
			 $w_t - p_t = c_t + n_t \phi$ \\
			 
			 Oil demand &
			 $M_t = \frac{\alpha_m Y_{q,t}}{\xi_t^n S_t}$ &
			 $m_t = -\mu_t - st + y_{q,t}$ \footnotemark \\
			 
			 Production &
		%A_t M_t^{\alpha_m} N_t^{\alpha_n} = 
			 $Y_{q,t} = A_t N_t^{\alpha_n} \left( \frac{\alpha_m Y_{q,t}}{\xi_t^n S_t} \right)^{\alpha_m}$ &
			 $y_{q,t} = \frac{1}{1-\alpha_m} (a_t + \alpha_n n_t - \alpha_m s_t - \alpha_m \mu_t)$ \\
			 
			 Gross output & 
			 $P_{c,t}C_t = (1-\frac{\alpha_m}{\xi_t^n})P_{q,t}Y_{q,t}$ &
			 $c_t = y_{q,t} - \chi s_t + \eta \mu_t$ \footnotemark \\
			 
			 GDP deflator &
			 $P_{q,t} \equiv P_{y,t}^{1-\alpha_m} P_{m,t}^{\alpha_m}$ &
			 $p_{y,t} = p_{q,t} - \frac{\alpha_m}{1-\alpha_m}s_t$ \\
			
			GDP &
			$P_{y,t}Y_t = \left(1-\frac{\alpha_m}{\xi_t^n}\right) P_{q,t}Y_{q,t}$ &
			$y_t = y_{q,t} + \frac{\alpha_m}{1-\alpha_m}s_t + \eta \mu_t$ \\
			
			Inflation & 
			$\left[\frac{P_{q,t}}{P_{q,t-1}}\right]^{-\epsilon}= \theta + (1 - \theta) \pi_t$ &
			$\pi_t = (1-\theta)(\hat{p}_t - p_{t-1})$ \\
			
			Price setting &
			$\sum_{k=0}^{\infty} \theta^k \E_t 
			\left[
			Q_{t,t+k} Y_{q,t+k|t} 
			\left(
			\hat{P}_{q,t} - \frac{\epsilon}{1 - \epsilon} MC_{t+k|t}^n
			\right)
			\right]
			= 0$ &			
			$\pi_{q,t} = \beta E_t [\pi_{q,t+1}] + \lambda (\mu_t - \mu)$ \\
			\hline
		\end{tabular}
	\end{table}
	\footnotetext[6]{where $\rho \equiv - \ln(\beta)$ and $i_t = -\ln(Q_t)$}
	\footnotetext[7]{where $\mu_t^n \equiv \ln(\xi_t^n)$}
	\footnotetext[8]{where $\eta \equiv \frac{\alpha_m}{\xi^n-\alpha_m}$}

	Combining the expressions for $y_t$, $c_t$ and $m_t$ through the mark-up $\mu_t$ a simplified expression for output and consumption can be derived, In doing so Blanchard \& Galì (2008) make the assumption that steady-state mark-up $\mu^n$ is small enough so that $\frac{alpha_m}{mc^r - \alpha_m}-\frac{alpha_m}{a - \alpha_m} \mu^n \approx 0$. This yields the below expressions for output and consumption.
	\begin{equation}
		\begin{aligned}
			y_t = \frac{1}{1-\alpha_m} (a_t + \alpha_n n_t) \\
			c_t = y_t - (\frac{\alpha_m}{1-\alpha_m} + \chi) s_t \\
%			\mu_t^n = - \Gamma_n n_t - \Gamma_s s_t
		\end{aligned}
	\end{equation}
	In order to close the model an exogenous law of motion for the oil price is required, and defined as follows: 
	\begin{equation}
		\begin{aligned}
			s_t = \rho_s s_{t-1} + \epsilon_{s,t} \\
			\epsilon_{s,t} \sim N(0, \sigma_{\epsilon_{s}})
		\end{aligned}
	\end{equation}
	Moreover, the monetary authority's behaviour is characterised by the following interest rate rule, assuming $\phi_{\pi}>1$. 
	\begin{equation}
		\begin{aligned}
			i_t = \phi_{\pi} \pi_{q,t} \\
		\end{aligned}
	\end{equation}

	The above combined with the Euler condition and law of price setting yield the model's core equations the \ac{NKP} and the \ac{IS} curve.
	\begin{equation}
		\begin{aligned}
			y_t = \E_t[y_{t+1}] - (i_t - \E_t[\pi_{q,t+1}]) + \frac{\alpha_m (1-\rho_s)}{1-\alpha_m} s_t \\
			\pi_{q,t} = \beta \E_t[\pi_{t+1}] + \kappa y_t + \lambda_p \Gamma_p s_t
		\end{aligned}
	\end{equation}
	where $\kappa \equiv \lambda_p \Gamma_n \frac{1-\alpha_m}{\alpha_n}$ and 
	$\Gamma_n \equiv \frac{(1 - \alpha_n - \alpha_m) + (1 - \alpha_m) (1 + \phi_{\pi})}{1 - \alpha_m - (1 - \alpha_m) \eta}$
	and $\frac{\frac{(1 - \theta)(1 - \beta \theta)}{\theta}}{1 + (1 - \alpha_m + \alpha_n)(\epsilon - 1)}$.
	
	In this NK model extended by petrol the core equations are extended by the petrol price. This illustrates the theorised direct impact of petrol prices on inflation and consequently output. As prices rise firms marginal cost change, which through their mark-up $\mu_t^n$ carries forward into the optimal price. This leads to changes in inflation. 
	As with inflation expectations the output is similarly depend on petrol prices. Moreover, the IS equation no longer takes the natural rate of interest into account. This is linked to the fact that this model's no longer uses the output gap as its instrument. Instead it relies on output itself. Apart from these changes the expectation formation is structurally similar to the New-Keynesian model. 
	
	This model has been implemented using gEconpy, where the log-linear model as provided to the simulation programme can be found in \nameref{appe}. The system's stability has been assessed according to the Blanchard-Kahn conditions of stability for each parameter specification \cite{blanchard_solution_1980}. 
	
	\section{Data}
	This work relies quarterly macroeconomic variables from 1975 to 2020. All data is obtained from the St. Louis Federal Reserve data base, where it has been seasonally adjusted and deflated \cite{noauthor_federal_nodate}. The specific data codes can be found in Table~\ref{tab:fred_variables} in the \nameref{appb}. Most data used across this work is standard to the DSGE literature. However, some extensions are necessary to allow for the inclusion of a petrol sector. The below will briefly outline the use of standard data, while providing more detail on the data specific to this work.
	
	The common variables for DSGE model evaluation are nominal interest rates, consumption, wage and labour hours. The nominal interest rate is proxied by the federal funds effective rate as suggested Herbst \& Schorfheide 2016 \cite{herbst_bayesian_2016}. Consumption is measured by household expenditure data. Labour hours and wage refer to non-farm business total hours worked and average hourly wage. The exclusion of the agricultural sector is generally recommended for the US economy \cite{guerron-quintana_bayesian_2013}. Moreover, this work employs the WTI Crude Oil Index as proxy for the price of petrol. The WTI dates back longer than the more recent Brent Crude Oil Index, wherefore this work will rely on it.
	With regards to inflation $\pi$ this work relies on the median consumer price index for the RBC and the NK model. The NK energy model's aim is to analyse inflation under the inclusion of petrol in the economy. Relying on the CPI, which includes petrol prices, would thus be counterproductive as the model should generate inflation resulting from petrol endogenously. In order to accurately reflect the models ability, energy prices need to be removed from inflation data. In the NKE model therefore relies on the St. Louis Fed's "Sticky Price Consumer Price Index less Food and Energy". Similar reasoning applies to consumption expenditure which for the NKE specifically excludes expenditure for energy and utilities.
	
	\subsection{Preprocessing}
	Economic data is usually considered in per capita terms in order to account for different population dynamics. Since population dynamics are not continuously collected they usually commingle actual measurements and imputations. This leads to irregularity in the data, which if divided by would distort more carefully collected economic variables \cite{pfeifer_guide_2021}. Population data thus requires some preprocessing on its own. Smoothing the data with the \ac{HP} at a smoothing factor of 10,000 is recommended in order to de-noise population data \cite{edge_judging_2013}. The smoothed time series then captures the real population dynamics and can be used to obtain per capita variables. 
	Having removed population dynamic the next step is to separate the cyclicality off trending variables. As DSGE models are defined in terms of log-deviations from the steady state data needs to be transformed accordingly. This requires an identification of the steady state and its evolution over time, which commonly relies on the \ac{HP} filter. The filter separates trend and deviation with at smoothing parameter of 1600 for quarterly variables \cite{ravn_adjusting_2002}. As input to the HP filter data is usually log-transformed, in order to account for the log-deviation definition of DSGE models. 
	This procedure by construction yields stationary time series of cyclicality. However, to comply with common standards stationarity is formally confirmed by the Augmented-Dickey Fuller test \cite{dickey_distribution_1979}. Having transformed all trending variables have into stationary log-deviation from their natural level only the percentage variables remain, namely inflation and interest rate. They require no such treatment, however need translation from annual to quarterly rates. This has been done according to the following formula $(1 + \frac{x}{100})^{1/4} - 1$.
		
	\subsection{Data description}
	\input{{./graphs/data_descriptives.tex}}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.9\textwidth]{data_variables_transformed.png}
			\caption{Log transformed HP filtered stationary data series}
			\label{fig:data}
		\end{center}
	\end{figure}

	The data used for the analysis spans from 1985 to 2020. Moreover, data from 1975 to 1985 has been used to inform the choice of the parameters' prior distributions. 
	As can be seen across the transformed GDP series \ref{fig:data} the period of analysis spans three major economic crisis, the 2000 dot.com bubble, the 2008 financial crisis and most recently the Covid-19 pandemic. These recessions are similarly present across consumption, labour hours and investment. When turning to the interest rate great differences across the sample period become apparent. The mid-80s, still influenced by the Volker-shock, have rather high interest rates when compared to the more recent low-interest environment starting as of 2009.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.9\textwidth]{bvar_forecast1.png}
			\caption{Bayesian VAR 2 lags}
			\label{fig:bvar_2lags}
		\end{center}
	\end{figure}
	Running a first two-lag \ac{BVAR} on output $y$, consumption $c$ and labour $l$ reveals the difficulty of forecasting. Depicted in blue in Figure~\ref{fig:bvar_2lags} systematic bias across output and consumption forecasts becomes apparent. Similarly labour hours are forecasted to be much more volatile than actually the case. A more comprehensive \ac{VAR} model will be presented later in \nameref{forecast_bvar}.
	The initial success of RBC literature has been based on its ability to replicate real data's first and second order moments.
	For this purpose covariance matrices of real data and the model specifications where compared. This work follows suit and includes the covariance matrix for each of the above models as well as the data covariance matrix in \nameref{appa}.
			
	\section{Bayesian Estimation}
	As explained in the above the RBC and NK models are based on structural parameters of the economy. As such they are not subject to the Lucas 1981 critique as their parameters do not depend on behaviour of agents but describe their preferences \cite{lucas_jr_tobin_1981}. Parameters thus describe the time invariant dynamic system underling the economy. Evaluating the accuracy of such system is thus tied to evaluating its parameters in capturing the true dynamics \cite{herbst_bayesian_2016}. In this aim two main approaches of evaluation have been drawn up by research over time, frequentist and Bayesian methods. 
	The frequentist approach first on \ac{GMM}, attempting to minimize the distance between simulated and real data moments \cite{blanchard_solution_1980}. This allowed to discriminate between several specification of the same structural model \cite{christiano_current_1992}. However, as the number of parameters grew GMM conditions were no longer sufficient to estimate model fit \cite{guerron-quintana_bayesian_2013}. Instead, maximum-likelihood  estimation was used but likewise unable to estimate the growing number of parameters \cite{guerron-quintana_bayesian_2013}. Bayesian statistics offered remedy in the convenient formulation of conditional likelihood \cite{guerron-quintana_bayesian_2013}. Based on Baye's law the posterior of parameter vector $\Theta$ can be obtained by multiplying the prior $P(\Theta)$ with the conditional likelihood of the data $Y_t$ given the parameters. 
	\begin{equation} \label{eq:blaw}
		P( \Theta | Y_{T}) = P(\Theta) L(Y_{T} | \Theta)
	\end{equation}
	Three ingredients are thus required, a distribution reflecting prior believes about the parameters, a likelihood function and an iterative procedure allowing to sample from the posterior. The following section will introduce these three concepts. Moreover, it will point out the forecasting properties that naturally arise from joining the three. 
		
	\subsection{Kalman Filter}
	Bayesian analysis of DSGE models is concerned with the transition from a prior distribution reflecting previous held knowledge to its posterior \cite{herbst_bayesian_2016}. In this an expression of conditional likelihood is needed \eqref{eq:blaw}. In obtaining the likelihood of a linear system one can make use of the fact that the system is itself a data generating process \cite{andrews_kalman_2008}. The likelihood function is obtained through evaluating the probability of a given data series being generated by the system. 
	A method allowing to do so is the Kalman filter, a method originally developed for tracking moving objects across several sensors \cite{kalman_new_1960}. In order to track any object the filter requires the object's position and velocity to be described in a linear system. The filter then uses this system alongside the measurement data to form a believe about the object's position, also referred to as the system's state \cite{andrews_kalman_2008}. Applied to DSGE modelling the system is the above derived model and its state are the magnitude to which the variables deviate from their respective steady state.	
	
	In forming a believe about the system states the filter exploits the fact that sensor signals are inherently noisy \cite{andrews_kalman_2008}. The filter imposes the assumption of Gaussian distributed noise, making use of the fact that multiplying Gaussian distributions results in another Gaussian distribution with lower or equal variance \cite{kalman_new_1960}. Decreasing a distribution's variance is equivalent to narrowing down the believe about the current state of the tracked object. Applied to DSGE modelling the assumption of Gaussian noise implies that the joint distribution of model shocks must be Gaussian \cite{herbst_bayesian_2016}. This is satisfied for small scale New Keynesian models if the individual shocks are Gaussian \cite{herbst_bayesian_2016}. 
	
	The Kalman filter is especially desirable for two properties. For one it translates its believe about a system's state into a likelihood function \cite{kalman_new_1960}. Moreover, it is the optimal forecast for a linear system \cite{kalman_new_1960}. The below will briefly explain how the filter operates as well as its construction of the likelihood function. For that it will refer to the generic linear system with transition matrix $\matr{F}$, which propagates state variables $\matr{X}_{k-1}$ to $\matr{X}_k$. The shock noise matrix $\matr N$ describes the effect of shocks on this transition process. In forming a believe about the current state $\matr{X}_k$ the Kalman filter proceeds in two steps, a forecast and an update step. In implementing the below in python this work has from filterpy, a library for Bayesian filtering \cite{labbe_rlabbefilterpy_2023}.
	\begin{equation}
		\begin{aligned}
			\matr{X}_k = \matr F \matr{X}_{k-1} + \matr N \matr{\epsilon}_k \\
			\matr{\epsilon}_k \sim N(0, \sigma_{\epsilon})
		\end{aligned}
	\end{equation}
	
	\subsubsection{Kalman forecast step} \label{kalman_forecast_step}
	In the forecast step the Kalman filter propagates system $\matr X_{k}$ according to the transition matrix $\matr F$. The propagation then yields a believe about the state given $\matr{\hat{X}}_{k|k-1}$ state given information from period $k-1$.
	\[
		\matr{\hat{X}}_{k|k-1} = \matr F \matr X_{k-1|k-1}
	\]
	The second key aspect to the Kalman Filter is the system state variable's covariance matrix $\matr P_{k}$. As pointed out in the above, this matrix is assumed to be Gaussian, thus semi-definite positive \cite{andrews_kalman_2008}. In propagating the covariance matrix $\matr P_{k}$ the process noise $\matr Q$ is used to reflect variance around new measurements. This results in $\matr{P}_{k| k-1}$ another Gaussian distribution, reflecting the uncertainty around the predicted state $matr{\hat{X}}_{k|k-1}$.
	This procedure, without the below filtering step, is used to obtain the three period ahead forecasts used in \nameref{forecast_eval}. The filter's forecast is repeatedly applied without taking reference to a measurement. The confidence intervals of this forecast are derived from the multivariate distribution $\matr{P}_{k| k-1}$ representing the certainty of the believe about state $\matr{\hat{X}}_{k|k-1}$. It's diagonal contains the state variable's standard deviation and is used to construct confidence interval around the predicted state. 
	\[
		\matr{P}_{k| k-1} = \matr F \matr{P}_{k-1| k-1} \matr{F}^T + \matr Q
	\]
	\subsubsection{Kalman filter step}
	Once the predicted state $\matr{\hat{X}}$ and its covariance matrix $\matr{P}_{k| k-1}$ have been obtained the Kalman filter proceeds to identifying the most likely system state. For this it takes into account the state measurement $\matr{z}_t$ as well as its noise. Due to this noise the system's real state is in-between its predicted state $\matr{X}_{k| k-1}$ and its measurement $\matr{z}_k$. The objective of filtering is to form a believe about the exact point in-between the two \cite{andrews_kalman_2008}. For this purpose the Kalman filter derives the Kalman gain $\matr{K}_{k| k-1}$, a weighting function between $\matr{X}_{k| k-1}$ and $\matr{z}_k$. In building $\matr{K}_{k| k-1}$ the filter takes into account the process noise, the covariance matrix and the accuracy of past iterations.	
	This requires $\matr{y}_k$ a measure of distance between measurement and prediction. In calculating $\matr{y}_k$ the measurement matrix $\matr H$ translats measurement $\matr{z}_t$ into the same units as the predicted state $\matr{X}_{k| k-1}$. For the purposes of this work all unit transformation has been done prior to filtering, wherefore the measurement matrix corresponds to the identity matrix $\matr{I}$.
	\[
		\matr{y}_{k} = \matr{z}_k - \matr H \hat{\matr{X}}_{k| k-1}
	\]
	Filter uncertainty $\matr{S}_k$ is obtained by combining the covariance $\matr{P}_{k| k-1}$ of the predicted state $\matr{\hat{X}}_{k| k-1}$ with the process noise $\matr{R}$. In the context of DSGE filtering the process noise is derived from the covariance matrix $\matr{N}$ as defined above \cite{guerron-quintana_bayesian_2013}.
	\[
		\matr{S}_k = \matr H \matr{P}_{k| k-1} \matr{H}^T + \matr{R}
	\]
	The uncertainty term $\matr{S}_{k}$ is then used to calculate the weighting function between measurement and predicted state, the Kalman gain. 
	\[
		\matr{K}_k = \matr{P}_{k| k-1} \matr{H}^T \matr{S}_{k}^{-1}
	\]
	The filtered state, which will be at the beginning of the next iterations, is then obtained from the Kalman gain and distance between measurement and predicted state $\matr{y}_k$. The covariance matrix $\matr{P}_{k|k-1}$ is likewise updated. This yields a new narrower Gaussian distribution $\matr{P}_{k|k}$, summarising the uncertainty around the system's true state.
	\begin{equation}
		\begin{aligned}
			\matr{X}_{k|k} = \matr{X}_{k| k-1} + \matr{K}_k \matr{y}_k \\
			\matr{P}_{k|k} = (\matr I - \matr{K}_k \matr{H}) \matr{P}_{k|k-1}		
		\end{aligned}
	\end{equation}
	Having introduced the Kalman filter algorithm the question of its initial condition remains. As DSGE models describe deviations from the steady state $\matr{X}_{0|0}$ is set to zero, assuming the system to be in steady state. This approach is generally followed by filtering literature \cite{schorfheide_loss_2000}. 
	
	\subsubsection{Likelihood function} \label{kalman_ll}
	The likelihood function for each iteration is obtained from the assumption of Gaussian system noise \cite{kalman_new_1960}. The likelihood is defined as a multivariate normal distribution, which is generically specified below with mean $\mu$, standard deviation $\sigma$ and the measurement $x$.
	\begin{equation}
		\Lagr = \frac{1}{\sigma \sqrt{2 \pi}} \exp [- \frac{1}{2} \frac{x - \mu}{\sigma}]^2
	\end{equation}
	The Kalman filter makes use of this definition in deriving its likelihood function. In the below the cumulated likelihood across the entire data set with $T$ periods is referred to as the Kalman likelihood. It is formally given by:
	\begin{equation}
		\Lagr_T = \sum_{k=0}^{T} \left[ \quad \frac{1}{\matr{S}_k \sqrt{2 \pi}} \exp [- \frac{1}{2} \matr{y}_k^{T} \matr{S}_{k}^{-1} \matr{y}_k] \right]
	\end{equation}
		
	\subsection{Metropolis-Hastings Sampler}
	The aim of Bayesian estimation is the identification of the true parameter distribution, the posterior. However, mapping a DSGE model's parameters to its posterior is non-linear in the parameter vector $\Theta$, wherefore its posterior cannot be evaluated analytically \cite{herbst_bayesian_2016}. Instead a numerical approximation mechanism is require, this is referred to as the sampler \cite{guerron-quintana_bayesian_2013}. The Metropolis-Hastings Monte Carlo Markov Chain (MH-MCMC) sampler is the predominant sampling method in linear Bayesian estimation literature \cite{guerron-quintana_bayesian_2013}. The MH-MCMC sampler is applicable to small scale New Keynesian models. It usually delivers good results for small models as their joint parameter distribution is well-behaved elliptic \cite{herbst_bayesian_2016}. This reduces the posterior identification problem to a global problem. Larger models usually exhibit no-elliptic parameter distributions, requiring samplers which are able to distinguish local and global likelihood maxima \cite{herbst_bayesian_2016}. However, as this thesis is concerned with the canonical model the MH-MCMC sampler is sufficiently accurate.	
	The MH-MCMC sampler suggests new posterior candidates according to a multivariate random walk. The posterior is extend by a suggestion if it provides an improvement in the likelihood as described in Section~\ref{kalman_ll}. The algorithm generates a stable Markov chain, which is meant to exhibit low autocorrelation and low variance of the MH estimator. Under such conditions the resulting chain is equivalent to the posterior distribution \cite{herbst_bayesian_2016}. The below will outline this thesis implementation of the MH-MCMC sampler for which the code can be found in \nameref{appd}. 
	
	The algorithm proceeds in three main steps, first a candidate for the posterior $\matr{\hat{\Theta}}_{k|k-1}$ is suggested based on the random-walk law of motion. The random-walk departs from the last accepted posterior candidate and suggests a new candidate by departing from the $\matr{\Theta}_{k-1|k-1}$ according to the zero-mean Gaussian distribution $\epsilon$. The distribution's variance $\matr{\Sigma}$ can be set to either the identity matrix $\matr{I}$ or to contain the parameter prior variances on its diagonal. The latter is recommended if well-defined priors are at play and thus will be followed by this work \cite{herbst_bayesian_2016}.
	\begin{equation}
		\begin{aligned}
			\matr{\hat{\Theta}}_{k|k-1} = \matr{\Theta}_{k-1|k-1} + \eta \epsilon \\
			\epsilon = N(0, \matr{\Sigma})
		\end{aligned}
	\end{equation}
	Across the law of motion the \ac{MH-MCMC} sampler employs the gain parameter $\eta$ which essentially scales the distance of the new posterior candidate away from the last accepted candidate $\matr{\Theta}_{k-1|k-1}$. This value has been calibrated to be $\eta \in [0.2, 0.4]$ \cite{gelman_weak_1997}. Generally the value of $\eta$ should correspond to an acceptance rate between 20\% and 40\% of suggested draws \cite{herbst_bayesian_2016}. This work puts $\eta$ at 0.25 which yields an acceptance rate of around 30\% for each model.
	A new posterior candidate $\matr{\hat{\Theta}}_{k|k-1}$ is once obtained evaluated through the Kalman likelihood. This yields the likelihood of the candidate given the data $\matr{Y}_T$.
	\[
		 \Lagr(Y| \matr{\hat{\Theta}}_{k|k-1})
	\]
	Once the likelihood is obtained the algorithm proceeds into the two step acceptance procedure \cite{guerron-quintana_bayesian_2013}. First the likelihood of candidate $\matr{\hat{\Theta}}_{k|k-1}$ is compared to the most recently accepted posterior in likelihood.
	\begin{equation}
		\omega_k = \min \left\{ \frac{ \Lagr(Y|\matr{\hat{\Theta}}_{k|k-1}) P(\matr{\hat{\Theta}}_{k|k-1})}{\Lagr(Y| \matr{\Theta}_{k-1|k-1})  P(\matr{\Theta}_{k-1|k-1})}, 1 \right\}
	\end{equation}
	In a second step the likelihood ratio $\omega_k$ is randomly accepted if above uniform random variable $\phi \sim U(0, 1)$. If $\omega_k \leq \phi_k$ the draw is accepted and the candidate is assumed into the posterior. This process is repeated $N$ times generating the a posterior sample. 
% check number of sampler runs
	As an recursive method the MH-MCMC sampler requires an initial condition. This initial condition is usually obtained throughout a so-called burn-in period corresponding to 50\% of total iterations. Moreover, a number of at least 20,000 is recommended for the sampler \cite{herbst_bayesian_2016}.
	
			
	\subsection{Priors} \label{priors}
	In response to the Lucas critique \ac{DSGE} models were build around structural time invariant parameters \cite{lucas_jr_tobin_1981}. Consequently, parameters can be expressed as a joint distribution of preferences, which in its entirety is usually impossible to evaluate \cite{del_negro_forming_2008}. Instead parameters are analysed in building individual prior distributions reflecting independent and prior knowledge about them \cite{del_negro_forming_2008}. Here the notion of independence is important in tow ways. Firstly, only if the data used to construct the priors is independent of the likelihood mechanism the resulting posterior is unbiased \cite{del_negro_forming_2008}. This condition is fulfilled if the data for prior formation at least pre-dates the of the likelihood evaluation \cite{herbst_bayesian_2016}. Secondly, assuming independence between parameters is only reasonable within limits, as shocks do not necessarily occur independently \cite{herbst_bayesian_2016}. However, the latter only poses a problem for large multi-shock models. This work analyses the canonical singular shock model, hence not concerned by this second issues. The below will therefore focus on identifying priors based on independent information. In doing so the approach of Del Negro \& Schorfheide (2008), dividing priors into three groups for analysis, will be followed \cite{del_negro_forming_2008}. An overview of the prior to posterior transition can be found in Graphs~(\ref{fig:rbcpp}, \ref{fig:nk5pp}, \ref{fig:nk6pp}). Across the following sections each prior group will be briefly outlined and its posterior transition analysed. An overview of all priors distributions can be found in the \nameref{appb}. 
	
	\subsubsection{Steady state priors}
	The group of steady state priors can be easily identified from long-run relationship of model variables \cite{del_negro_forming_2008}. These relationships, such as the labour or capital share are also sometimes referred to as the 'great ratios' \cite{kydland_time_1982}. For the models in question the steady state parameters are: 
	\begin{equation}
		\begin{aligned}
			\Theta_{ss}^{RBC} = \{ \alpha, \beta, \delta \} \\
			\Theta_{ss}^{NK} = \{\alpha, \beta, \theta \} \\
			\Theta_{ss}^{NKE} = \{\alpha_m, \alpha_n, \beta, \theta, \chi \} \\
		\end{aligned}
	\end{equation}
	The time discount factor $\beta$ is usually not calibrated, as it results directly from the inverse of the interest rate \cite{guerron-quintana_bayesian_2013}. The pre-1985 data used to estimate prior distributions puts $\beta$ to 0.995.	
	The capital share can be derived from the long-run share in industrialised nations. For smaller models using a uniform distribution covering the possible parameter interval $\alpha \in [0,1[$ is suggested \cite{del_negro_forming_2008}. In the RBC $\alpha$'s posterior converges from a uniform to a centred distribution Figure~\ref{fig:rbcpp}. While this is still rather broad and far off the empirical value of 33\% it is indication of convergence towards the true posterior Figure~\ref{fig:rbcpp}.
	For both New-Keynesian models a more targeted distribution has been used to ensure steady state existence \cite{guerron-quintana_bayesian_2013}. Consequently the values for $\alpha$ are more left skewed Figure~\ref{fig:nk5pp}. Interestingly enough the calibration of the NKE greatly overestimates the share of oil in production which by \cite{blanchard_macroeconomic_2007} is reckoned at below 5\% Figure~\ref{fig:nk6pp}. The labour share in production is more reasonable but also off the empirical estimate of 0.7 \cite{blanchard_macroeconomic_2007}.
	The capital depreciation rate $\delta$ is very far off its empirical estimate of 2.5\% per quarter \cite{campbell_inspecting_1994}. However, its posterior has assumed reasonable values in limiting itself to below 0.8 Figure~\ref{fig:rbcpp}. This is indication of at least some convergence. 
	The price setting probability $\theta \in [0,1]$ has reasonable estimates in the NK model Figure~\ref{fig:nk5pp}. Its mean is around 0.8 which corresponds to an average price duration of roughly four month, close to the empirical estimate \cite{blanchard_macroeconomic_2007}. This is not the case for the NKE model where $\theta$ exceeds one, which given its probabilistic specification is not possible. The NKE estimates are much more accurate for the share of petrol in consumption, whose posterior remains close to the empirical mean of 1.7\% Figure~\ref{fig:nk6pp} \cite{blanchard_macroeconomic_2007}.

	\subsubsection{Exogenous prior}
	Exogenous priors contain the exogenous law of motion and standard deviation of shocks. In a simple model with singular shocks they can be identified from the first and second order moments of the economic variables affected by the shocks \cite{del_negro_forming_2008}. This work thus calibrated the shock standard deviation based on the variance of pre-1985 output cyclicality amounting to 0.0243. Moreover, in setting the autoregressive parameter this work draws from the distributions suggested by \cite{vasconez_what_2015}. Looking across the posterior all estimates for $\rho$ are bounded at one. While this might appear to be a successful identification this is merely due to the stability requirement of the dynamic system. If $\rho \in \quad ]-1,1[ $ is violated the dynamic system is no longer stable and cannot be solved, leading the sampler to discard any value $|\rho| > 1$. The high variance of the posterior thus indicates problems of identification. However, its skewness towards higher values seems to confirm the prior assumption of relatively time persistent shocks across all models Figure~\ref{fig:nk5pp}.
	\begin{equation}
		\begin{aligned}
			\Theta_{exog}^{RBC} = \{\rho_A \} \\
			\Theta_{exog}^{NK} = \{\rho_v \} \\
			\Theta_{exog}^{NKE} = \{\rho_s \}
		\end{aligned}
	\end{equation}

	\subsubsection{Endogenous priors}
	All remaining parameters enter into the group of endogenous priors. These parameters cannot be measured directly through economic variables' long-term relationships. Instead they rely on micro-evidence from external data sets \cite{del_negro_forming_2008}. 
		\begin{equation}
		\begin{aligned}
			\Theta_{endog}^{RBC} = \{\sigma_C, \sigma_L, \} \\
			\Theta_{endog}^{NK} = \{\sigma_C, \sigma_L, \epsilon, \phi_{y}, \phi_{\pi} \} \\
			\Theta_{endog}^{NKE} = \{\epsilon, \phi_{y}\}
		\end{aligned}
	\end{equation}
		
	The elasticity of substitution $\sigma_C$ and $\sigma_L$ belong in this category. In setting their prior distribution this work relied on the recommendation from Del Negro \& Schorfheide (2008) \cite{del_negro_forming_2008}. For the RBC the posterior values demonstrate large spreads indicating that the MH sampler did not identify the posterior correctly Figure~\ref{fig:rbcpp}. Even larger dispersion can be observed for the NK model Figure~\ref{fig:nk5pp} \footnote{In the \ac{NKE} model the labour and consumption elasticities do not enter into the log-linear model specification due to the log-utility function}. This occurs frequently as the consumption and labour elasticity are parameters with low influence on the transition dynamics \cite{guerron-quintana_bayesian_2013}. Consequently they are dominated by stronger parameters in the likelihood function and the posterior identification fails for these variables.
	The elasticity of substitution $\epsilon$ originates from the Dixit-Stiglitz aggregator and factors into the steady-state mark-up of firms. As such it is one of the driving parameters behind inflation and strong in its influence on the transition dynamics. In setting the prior distribution one can rely on information on the steady-state mark-up $MC^r = \frac{\epsilon}{1-\epsilon}$. Following this a value of $\epsilon$ slightly above 0.5 appears realistic. Consulting the posterior both put the mean around 0.5 Figure~\ref{fig:nk5pp} Figure~\ref{fig:nk6pp}. While the standard deviation is still large the posterior specification does appear to be within reasonable bounds. 
	
	The central bank's preference parameters $\phi_y$ and $\phi_{\pi}$ also appear within reasonable bounds. As suggested by \cite{gali_monetary_2008} the central bank has a stronger response to inflation that to the output gap. This has been incorporated into priors and the resulting posterior seems to confirm this assumption in the NK model Figure~\ref{fig:nk5pp}. The NKE model relies on output and not the output gap as intertemporal instrument, wherefore only $\phi_{\pi}$ appears. However, the posterior estimate appear to confirm the same reasoning Figure~\ref{fig:nk6pp}.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.5\textheight]{mod4_rbc_vanilla_posterior_hist.png}
			\caption{RBC model prior posterior comparison.}\label{fig:rbcpp}
			\includegraphics[width=.5\textheight]{mod5_nk_vanilla_lin2_posterior_hist.png}
			\caption{New Keynesian model prior posterior comparison.}\label{fig:nk5pp}
			\includegraphics[width=.5\textheight]{mod6_nk_energy_lin2_posterior_hist.png}
			\caption{New Keynesian Energy model prior posterior comparison.}\label{fig:nk6pp}
		\end{center}
	\end{figure}
	\pagebreak
		
	\section{Forecast evaluation} \label{forecast_eval}
	Having identified the parameter posterior distribution this section uses the Kalman forecasting ability as outlined in Section~\ref{kalman_forecast_step} to obtain a three quarter ahead forecast. Each model will be using the mean posterior parameters in its specification as this corresponds to the most likely model. The individual forecasts will then be analysed across the next three section. A fourth section will introduce a \ac{BVAR} forecast, while a last section will establish a comparison between DSGE models and the \ac{BVAR} forecast. In order to accommodate for the Kalman filter convergence the following will be analysing data after 2003. The variables revealed as observables to the filter vary across models in order to reflect the differences in model specification such as investment expenditure, inflation and petrol prices. This is indicated in each section.
	
	\subsection{Real Business Cycle model}	
	The Kalman filter closely traces output and investment as can be seen in Figure~\ref{fig:rbc_kfil}. The real data lies within the filter's 95\% confidence interval for all post 2010 observations which is an important criteria of model fit. This is also true for the recession in early 2020, emphasising the filter's good  fit to real data. Consumption, while not contained as an observable in the filter is also closely traced and contained within the confidence bands. This points to even better model accuracy, as the dynamic system is able to replicate a non-included variable to good extend. The interest rate is less closely traced and predicted to be much more volatile than actually the case. While this indicates problems in the model specification it is important to note that the US Federal Reserve (FED) pursued a notorious low interest rate policy for much of the sample period. It thus followed a different dynamic than in the RBC where the rate of return is dictated by changes in capital demand. A lesser fit to the interest rate is thus not surprising. Labour is the only variable continuously outside of the 95\% confidence interval. The model even proposes countercyclical movements in labour hours during crisis. While peculiar dynamics where at play for the Covid-19 pandemic the 2008 crisis was certainly more standard in-terms of labour response, yet the labour dynamics are countercyclical. This points to a structural misspecification of the RBC model, unable to replicated recessionary unemployment.
	Generally, pre-2010 dynamics are captured less well by the model. Output and consumption data escapes the filter's 95\% confidence interval before and after the 2008 financial crisis. The fit to more recent data is significantly improved. This is can circumstantial or related to the expanded period of growth and relative economic stability.
				
	The RBC forecasts, as depicted in Figure~\ref{fig:rbc_kfor}, seem reasonable in direction and confidence. The 95\% intervals naturally increase with distance to the point of prediction. However, especially at the one period ahead forecast they exhibit reasonable bounds. Moreover, the point forecasts, with exception of labour, closely trace real variables with no systematic bias in either direction. Labour as explained in the above appears to be incorrectly specified in the model's functional form. Yet, this does not impede the model in its forecasting of other variables. 
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.5\textheight]{mod4_rbc_vanilla_y_I_kalman_filter_00.png}
			\caption{RBC model filtered data with observables $Y$ and $C$}\label{fig:rbc_kfil}
			
			\includegraphics[width=.5\textheight]{mod4_rbc_vanilla_y_I_kalman_forecast.png}
			\caption{RBC model forecasted data with observables $Y$ and $C$}\label{fig:rbc_kfor}
		\end{center}
	\end{figure}
	
	\subsection{Standard New-Keynesian model}
	At first glance it is evident from Figure~\ref{fig:nk_kfil} that the New-Keynesian model traces data less accurately. As with the RBC output is more closely traced after 2010 while confidence intervals exclude actual data around the 2008 crisis. However, other than with the RBC the NK does not manage to track the 2020 crisis. The slump in output is even deemed outside the 95\% confidence interval. A similar dynamic can be observed with consumption. Due to absence of investment in the NK model output is formally equal to consumption. The filter therefore essentially remodels output data, hence consumption can be seen as lesser of a performance indicator. Labour on the other hand is more closely traced and differently from the RBC no longer exhibits counter-cyclicality during recessions. This is indication of better fit by the NK's functional form. Yet, the main contribution of the New-Keynesian literature is the inclusion of inflation. Looking at the actual dynamics against filtered data large divergence become apparent. The model puts inflation at much higher rates than actually the case in the economy. The same goes for the real interest rate, which is not surprising due to the close tie between the two $r_t = i_t + \E[\pi_{t+1}]$. Moreover, the confidence intervals are rather broad on both inflation and interest rate. This, while no sign of good fit, at leasts shows that the model is uncertain about the actual behaviour of inflation. It does not deem its forecast erroneously accurate, which can be turned into an argument against complete miss-specification.		
	While the filtered data provides decent results the NK's forecasts are far from informative as illustrated in Figure~\ref{fig:nk_kfor}. Output and consumption are not contained in the forecasts' 95\% interval. Moreover, the broad confidence intervals on inflation and interest rate conceal the fact that inflation forecasts actually move opposite to real data. Only labour is proxied to reasonable extend by the confidence interval, while its point forecast is systematically off as well. To standards of visual inspection the NK specification seems to fit data to a lesser extend, suggesting its specification to be less accurate than the RBC. 
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.5\textheight]{mod5_nk_vanilla_lin2_y_kalman_filter.png}
			\caption{NK model filtered data with observables $Y$}\label{fig:nk_kfil}
			\includegraphics[width=.5\textheight]{mod5_nk_vanilla_lin2_y_kalman_forecast.png}
			\caption{NK model forecasts with observables $Y$}\label{fig:nk_kfor}
		\end{center}
	\end{figure}
		
	\subsection{New-Keynesian energy model}
	Figure+\ref{fig:nke_fil} depicts the New-Keynesian Energy model's filtered time series. Compared to the previous models its data is the furthest off reality. The dynamics of output and labour are poorly captured by erroneously narrow confidence intervals. The same goes for inflation which despite being contained as observable does not improve this inability to track output or consumption. Neither is the real interest rate, even-though closely tied to inflation, correctly traced by its confidence bands. Figure~\ref{fig:nke_fil_s} reveals that including petrol in the model specification does not improve this circumstance. Instead, the filter clings erroneously to the petrol price, while ignoring other variables. The aim of including petrol into the NK framework was to supplement the analysis of inflation with external confounding factors. The filtered time series reveals that the model specification is unable to live up to this aspiration. Including the petrol price in the filter does not provide the theorised improvement.
	The poor filter fit is obviously reflected across the forecasts as well Figure~\ref{fig:nke_for}. While point forecasts steer off in opposite directions even the confidence bands course of data, deeming real data for consumption, inflation and the interest rate as improbably generated by the model.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.5\textheight]{mod6_nk_energy_lin2_y_pi_s_c_kalman_filter.png}
			\caption{NKE model Kalman filter with variables $Y$, $\pi_c$, $C$}\label{fig:nke_fil}			
			\includegraphics[width=.5\textheight]{mod6_nk_energy_lin2_y_pi_s_s_kalman_filter.png}
			\caption{NKE model Kalman filter with variables $Y$, $\pi_c$, $S$}\label{fig:nke_fil_s}
			\includegraphics[width=.5\textheight]{mod6_nk_energy_lin2_y_pi_s_kalman_forecast.png}
			\caption{NKE model Kalman forecast with variables $Y$, $\pi_c$, $C$}\label{fig:nke_for}
		\end{center}
	\end{figure}

	\subsection{Bayesian Vector Autoregression} \label{forecast_bvar}
	Using a Vector Autoregression as reference model for comparing DSGE model forecasts has been commonly done across literature \cite{schorfheide_loss_2000, chin_bayesian_2019}. 
	Naturally the BVAR does not provide confidence intervals like frequentist methods of forecasting. Instead model parameters are defined through a sampler, which in this case ran in four iterations. Sampling from these iterations yields the posterior coefficient distribution. Using several draws from the posterior then allows to derive information about the range of possible outcomes. This range can then be interpreted as a confidence interval \cite{chin_bayesian_2019}. A sample of 200 posterior specifications is illustrated in blue across Figure~\ref{fig:bvar}, while the sample's mean is marked in red. 
	The range of outcomes for output seem reasonable. While growing in spread with forecast distance, the mean approximates the real course with a slight downward bias in the forecast. Similar dynamics can be observed for labour, though confidence bands widen more broadly at period $t+3$. The consumption forecasts overestimates the real trend at first, returning to the true value. The inflation forecast, realises the first down turn but as of $t+2$ looses track of the slight uptrend, resulting in a biased forecast as well. the forecast on the interest rate is the furthest off. However, this is not surprising as the data used to build the VAR originates in low-interest environments. Capturing the exogenously determined up-trend of the FED interest rate policy is thus beyond the scope of a VAR model. On first account the BVAR appears to deliver reasonable forecasts. 
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.4\textheight]{bvar_forecast.png}
			\caption{Bayesian VAR with 2 lags}\label{fig:bvar}
		\end{center}
	\end{figure}
	
	\subsection{Forecast comparison}
	In comparing structural models across their forecasting performance economic literature provides two approaches, a frequentist and a Bayesian one. The frequentist approach compares models based on a their mean squared error, testing for systematic differences. This is formally provided by the Diebold-Mariano test. This tests is recommended for on step ahead forecasts and its statistic is based on a large number of observations. It is consequently dissuaded from for quarterly data \cite{chin_bayesian_2019}. Bayesian statistics provides a model averaging approach, comparing models through a log-likelihood based information criterion. Across the DSGE literature the predominant criterion is the \ac{WAIC}, a choice this work will follow suit \cite{chin_bayesian_2019}. In Bayesian model averaging the weights of each model are calculated from their respective posterior probability \cite{chin_bayesian_2019}. This is traditionally done for structurally similar models in order to discriminate between different priors. However this procedure has also been employed in comparing different model classes \cite{chin_bayesian_2019}. In the context of this work a comparison according to the \ac{WAIC} points to the RBC as the most suitable model specification. At this point it is important to emphasize, that this is a relative evaluation, meaning the RBC is the best amongst the here present models but not a globally good model.
	
	\section{Conclusion}
	This work has retraced the literature on Real Business Cycle and New-Keynesian models from its origins to an extension by petrol prices. This has been done in the aim of extending the modelled inflation dynamics with a factor that recently resurfaced as an influence on inflation, energy prices.
	In order to evaluate model pertinence this work has engaged in Bayesian estimation of parameter in order to obtain the most likely joint parameter distribution. Its results have then been employed in fitting the above models to data through the Kalman filter. Lastly, the models' forecast was compared to a Bayesian Vector Autoregression as a benchmark.
	
	Across this exercise the RBC model has shown the best fit, even beyond the empirical BVAR. This can be noted as a success and proof of concept for the methods at hand. Moreover, this work was able to replicate the RBC's well established deficiency in describing labour dynamics \cite{christiano_current_1992}. 
	The New-Keynesian model, despite its extension by inflation dynamics, did not live up to the performance set by the RBC. However, its filtered data was able to replicate real dynamics and improved especially with regards to labour. Yet, inflation and interest rates were poorly replicated. On the one hand this can to some extend be explained by the past years stable interest policy starkly contrasting the Taylor-rule dynamics assumed in th NK model. On the other hand, it illustrates once again the intricacy of inflation. 
	The extension of the New-Keynesian framework by petrol aimed at further consolidating the analysis of inflation. However, an evaluation against real data showed that the introduction of an exogenous oil supply does not improve model pertinence, quite the contrary. This fact is a finding on its own. Against all intuition growing model complexity did not improve model pertinence. Two possible explanations are at hand. One explanation lies in this works limitations, the other suggests that the modelled dynamics did not correspond to the real system for the United States. 
	
	As mentioned in Section~\ref{priors} the MH-MCMC sampler encountered identification problems, especially across variables with weak influence on the system dynamics. This sampler is highly susceptible to miss-specification of the posterior if the prior is far away from the posterior or weakly captured by the likelihood function \cite{herbst_bayesian_2016}. While less of an issue across the RBC, both New-Keynesian models were concerned by issues in posterior convergence. As a result their estimation is likely to be biased. However, these parameters being less influential in the overall system, might alleviate the impact of such miss-specification.
	With regards to the NKE model specification it is important to note, that real output is not contained in the main model. Instead domestic output supplemented by exogenous petrol supply forms the basis of the model. In order to make the model accessible to data Blanchard \& Galì (2007) joined domestic and petrol inflation in the GDP deflator, linking domestic to overall output \cite{blanchard_macroeconomic_2007}. Domestic inflation in the NKE draws to large extend from the standard-NK model, which as illustrated was unable to capture real inflation dynamics. The NKE model's only link to data going through insufficiently specified inflation makes the model's evaluation highly susceptible to the very same misspecification. The model's link to real data through a deflator thus might be an explanation for its poor fit.
	Moreover, as this work is evaluating models against US data a further aspect might be at play. The USA are an imported of crude oil, especially in times of low prices. However, ever since the late 1970s the US administration made a point in petrol autarky, augmenting extraction once prices surges. This peculiar dynamic is related to the fact that most domestic oil extraction is based on fracking. An expensive process which only is profitable under high oil prices. This circumstance presents the US economy with some absorption power of high oil prices, weakening the link between petrol prices and inflation. This fact is not contained in the model but likely to be very influential across the estimation process.
	
	This work replicated established findings of the RBC and NK literature based on an empirical investigation into these models. With this proven method at hand the analysis moved on to a New-Keynesian model extended by exogenous petrol supply. In modelling the impact of petrol prices on inflation the extension relied predominantly on the firms production function. As petrol prices rise firms are under higher pressure to increase prices, leading to inflation. While a theoretical argument can be made for this transition channel the evidence of obtained in the above does not find this channel to be particularly strong. This raises two questions regarding research on inflation and petrol prices.
	
	For one, using the firm as price setter and thus driver of inflation is at the heart of New-Keynesian literature. In including petrol one unfolds the argument that changes in real prices directly translate into the retail price of a good. The firm thus is accorded little say beyond production factor substitution in carrying forward inflation. This work's evidence does not fully support this mechanism. This could be seen as an indication that firms go through a more complex process in resetting prices.
	Secondly, inflation is generally seen as heavily expectation based. As the CPI goes up, as the case with energy prices, household increase their inflation expectation. This increase in household expectation is not contained in the simple models investigated in the above. This pull factor of household expectations is however, very important when analysing goods that enter into the CPI such as petrol. While DSGE literature has endowed households with wage setting power to include this phenomenon this remedy after all also operate through the firm. A potential research are to be considered is therefore inflation as a phenomenon arising from household expectation as a pull factor rather than higher real production costs as a pull factor.
	
	These are just two aspects within a range of questions still surrounding inflation, leaving room for future research. This illustrates that the confounding factors of inflation are manifold. As such a research approach rooted in empirics might be more fruitful in order to determine which factors matter most. Once identified one can return to modelling them in order to calibrate their joint effect. This might present the more direct way to identifying causes of inflation. 
	
	
% Bibliography
	\bibliography{20230505_m1_dsge}
	\addcontentsline{toc}{section}{References}

% Appendix
	\section*{Appendix}
	
	\pagebreak
	\subsection*{Appendix A} \label{appa}
	\input{{./graphs/fred_variables.tex}}
	\input{{./graphs/priors_table.tex}}
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=\textwidth]{cov_matrix.png}
			\caption{Covariance matrices}
			\label{fig:cov_matrix}
		\end{center}
	\end{figure}
	

%BVAR
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.4\textheight]{BVAR_coeff.png}
			\caption{NK model Kalman Filter with variables y}\label{fig:bvar_coef}
		\end{center}
	\end{figure}

	\pagebreak
	
	\pagebreak
	\subsection*{Appendix B} \label{appb}
	Euler Equation log-linearisation\\
	
	The NK Euler condition is given by the below, which can be transformed into an exponential expression as:
	\begin{equation}\label{eq:log_euler}
		\begin{aligned}
			Q_{t} = \E_t \left[ \frac{C_{t+k}}{C_t} \right]^\sigma \frac{P_t}{P_{t+1}} \\
			1 = \E_t \{\exp(i_t + \sigma \Delta c_{t+1} - \pi_{t+1} - \rho) \}
		\end{aligned}
	\end{equation}
	where log-deviations are given according to: 
	\begin{equation}
		\begin{aligned}
			\rho \equiv - \ln(\beta) \\
			\Delta c_{t+1} \equiv c_{t+1} - c_t \equiv \ln\left( \frac{C_{t+1}}{C_t} \right) \\
			i_t \equiv - \ln(Q_t) \equiv - \ln(1+i_t) \\
			\pi_{t+1} \equiv p_{t+1} - p_t \equiv \ln \left( \frac{P_{t+1}}{P_t} \right)
		\end{aligned}
	\end{equation}
	For the linearisation one aspect is important: GDP will grow at the constant rate $\gamma$ in the steady state. This corresponds to the output growth per capita, the output trend. As output per capita grows so will consumption, wherefore in the steady state $\sigma \Delta c_{t+1} = \sigma \gamma$ must hold. Replacing for consumption in the above exponential \eqref{eq:log_euler} a condition for the natural rate of interest arises. For the exponential to hold this equivalence must hold $i_{ss} = \sigma \gamma + \pi_{ss} + \rho$. 
	Using the property $\hat{x}_t = \ln(x_t) - ln(\bar{x}) = \ln\left(\frac{x_t}{\bar{x}}\right)$ the Euler equation can be rewritten as:
	\begin{equation}
		\begin{aligned}
			1 \approx i_t + \E_t \{\sigma \Delta c_{t+1} + \pi_{t+1} \} + \rho \\
			c_t = \E_t[c_{t+1}] - \frac{1}{\sigma} (i_t - \rho - \E_t[\pi_{t+1}])	
		\end{aligned}
	\end{equation}

	The Euler equation in the RBC is given below and can be linearised in a similar fashion. The important realisation with the RBC is that in the steady state $\beta = (1 - delta + \bar{r})$, where $\bar{r} = \ln(\bar{R})$ the capital rate of return steady state.
	\begin{equation}
		\E_t \left[ \frac{C_{t+1}}{C_t} \right]^\sigma = \beta \left(1 - \delta  + \E_t [R_{t+1}] \right)
	\end{equation}
	This yields: $\hat{c}_t = \hat{c}_{t+1} - \hat{r}_{t+1}$

	\pagebreak
	\subsection*{Appendix C} \label{appc}
	RBC numerical solutions
	\input{{./graphs/rbc_steady_state.tex}}
	\input{{./graphs/rbc_transition_matrix.tex}}
	\pagebreak
	
	\subsection*{Appendix D} \label{appd}
	Metropolis Hastings MCMC sampler code\\
	Deriving the multivariate covariance matrix $G$ from prior the prior distributions standard deviation
	\lstinputlisting[language=Python, firstline=31, lastline=39]{mh_mcmc_code.py}
	Suggesting new priors based on the random walk law of motion
	\lstinputlisting[language=Python, firstline=78, lastline=96]{mh_mcmc_code.py}
	MH MCMC acceptance procedure
	\lstinputlisting[language=Python, firstline=158, lastline=195]{mh_mcmc_code.py}
	
	\pagebreak
	\subsection*{Appendix E} \label{appe}
	\verbfilenobox[\tiny]{{./graphs/mod4_rbc_vanilla.txt}}
	\verbfilenobox[\tiny]{{./graphs/mod5_nk_vanilla_lin2.txt}}
	\verbfilenobox[\tiny]{{./graphs/mod6_nk_energy_lin2.txt}}
	
	\pagebreak
	\subsection*{Appendix F}
	All code can be found under https://github.com/lukasgrahl/memoire1. The README file with installation instructions is provided underneath.
	\verbfilenobox[\tiny]{{./graphs/read_me.txt}}

	
\end{document}