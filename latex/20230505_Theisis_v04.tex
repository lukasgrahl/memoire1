\documentclass[12pt,a4paper,english]{article} % document type and language

\usepackage{layout}
\usepackage{babel}   % multi-language support
\usepackage{float}   % floats
\usepackage{url}     % urls
\usepackage{graphicx}
\usepackage{amsmath} % matrix algebra
\usepackage{multirow} % tables
\usepackage{booktabs} % tables
\usepackage{blindtext}
\usepackage{geometry}
\usepackage[parfill]{parskip} % no indent
\usepackage{amssymb}
\usepackage{csvsimple}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{verbatimbox}
\usepackage{filecontents,catchfile}
\usepackage{float}
\usepackage{nameref}

% acronyms
\usepackage[printonlyused,withpage]{acronym}
\renewenvironment{description}
{\list{}{\labelwidth0pt\itemindent-\leftmargin
		\parsep0pt\itemsep0pt\let\makelabel\descriptionlabel}}
{\endlist}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}


\lstset{style=mystyle}
% Default fixed font does not support bold face
%\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
%\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% graphics path
\graphicspath{{./graphs/}}	% graphics

% title slide
\author{Lukas Gahl}
\title{\textbf{\huge Memoire }\\}
\date{\today} 

% user commands
\newcommand{\matr}[1]{\mathbf{#1}} % matrix font
\newcommand{\Lagr}{\mathcal{L}} % likelihood L
\newcommand{\E}{\mathbb{E}} % expectation
% RBC expressions
\newcommand{\Rss}{\frac{1}{\beta} + \delta - 1}
\newcommand{\Ass}{\bar{A}}
\newcommand{\KLss}{\left[ \frac{\alpha \Ass}{\Rss} \right]^{\frac{1}{1-\alpha}}}

% set layout
\let\oldsection\section
\renewcommand\section{\clearpage\oldsection}

\geometry{
	a4paper,
	total={170mm,257mm},
	left=25mm,
	right=30mm,
	top=25mm,
	bottom=25mm,
}
\linespread{1.5}

% bibliography
\bibliographystyle{abbrv}
\begin{document}
	
	
	\maketitle
	\pagebreak

	\section*{Abstract}
	\pagebreak

	
	\section*{List of Acronyms}
	\begin{acronym}
		\acro{USA}{United States of America}
		\acro{DSGE}{Dynamic Stochastic General Equilibrium Model}
		\acro{RBC}{Real Business Cycle Model}
		\acro{NK}{New-Keynesian Model}
		\acro{NKE}{New-Keynesian Energy Model}
		\acro{GDP}{Gross Domestic Product}
		\acro{MH-MCMC}{Metropolis-Hastings Monte Carlo Markov Chain}
		\acro{BVAR}{Bayesian Vector Autogreression}
		\acro{VAR}{Vector Autoregression}
	\end{acronym}
	\listoffigures
	\listoftables
	\pagebreak
	
		
	\tableofcontents
	\pagebreak
	
	
	 \section{Introduction}
	 The recent simultaneous surge in energy prices and inflation has reintroduced questions about a connection between the two. The late 1970s oil shock caused a recession across most western economies. Later oil price shocks in the 90s and 2000s, despite being of similar magnitude, did not match their predecessor's economic impact \cite{blanchard_macroeconomic_2007}. Recent events seem again to suggests similar strong implications of energy price shocks. Inflation appears again heavily influenced by energy prices. This calls for a reevaluation of the literature on inflation dynamics. Research in this area has for much of the recent past relied on structural rational expectation models. These analyse the transition mechanism of inflation, while allowing to take into account confounding factors such as oil prices. Relying on structural models in analysing its effect on the inflation mechanism has thus a broad base in literature. This work draws from this past literature in applying it to recent events. In doing so three strains of literature are merged, these are \ac{RBC} and \ac{NK} literature, Bayesian estimation of structural models and literature on structural models encompassing petrol. Literature on \ac{RBC} and \ac{NK} models, often also labelled \ac{DSGE} models,  where a response to the Lucas critique (1981) \cite{lucas_jr_tobin_1981}. They provided micro-based models in which parameters were not longer subject households behaviour but rather described their static preferences \cite{gali_monetary_2008}. This made them attractive for policy analysis, making them the predominant modelling approach in macroeconomics. In their success Bayesian estimation of structural models provided an important stepping stone. It linked \ac{DSGE} models to data by providing means of estimating the time independent model parameters. Moreover, it introduced methods of comparing the models' data generating process against real data allowing for further evaluation and forecasting \cite{del_negro_forming_2008}. This provided an new criteria to the evaluation of structural models \cite{kocherlakota_model_2007}. Lastly, a body of literature on the link between oil prices and inflation in structural models has emerged. This work will review this literature and draw from one model in particular \cite{blanchard_macroeconomic_2007}. 
	 
	 This work draws from the above in the purpose of analysing the inclusion of petrol prices into a DSGE framework and evaluating the pertinence of such exercise. In its evaluation this work will employ two standard models of macroeconomic literature, the RBC and NK model. Both will be compared to the petrol model in their fit to real data and forecasting performance. Lastly, a \ac{VAR} will be used as a forecasting benchmark. This work is organised as follows. First a literature review on structural models and their incorporation of oil price inflation will be provided. Three subsequent sections will be dedicated to developing the mathematical reasoning behind an RBC, the standard NK model and lastly the \ac{NKE} model. Having demonstrated the assumptions of drawing up structural models and linearising them this work will proceed in discussing the data used for the evaluation of these models. A fifth section will then be dedicated to introducing Bayesian estimation and forecasting procedures. The obtained estimation will the be discussed, while a final section will outline this work's results and its implications.
	 
	\section{Literature Review}
	The RBC literature was the first to arise, introducing structural models of time-invariant parameters. Its parameters are rooted in the optimising behaviour of representative agents and consequently convey their preferences. Preferences are thought off as time-invariant \ref{prescott_theory_1986}. The RBC literature further contributed in providing an argument on business cycle efficiency. Previously seen as inefficient divergence from the steady state, the RBC model suggests that in absence of nominal rigidities business cycles are driven by technology innovation and consequently are efficient adjustments towards the economy's steady state \cite{christiano_current_1992}. This notion of efficiency was subsequently questioned by New-Keynesian literature demonstrating that under nominal rigidities changes in the interest rate are not matched by immediate changes in expected inflation \cite{gali_monetary_2008}. Business cycles are thus driven by interventionist monetary policy, which causes the expectation gap, rendering cyclicity inefficient \cite{gali_monetary_2008}. Following the oil price induced stagflation crisis of the 1970s a rich body of economic literature has been dedicated to the effects of oil price on inflation and the economy at large \cite{barsky_oil_2002, bernanke_systematic_1997}. The majority of these approaches were empiric. The newly arisen DSGE literature was soon applied to questions of energy prices and inflation as well. In doing so two main approaches were pursued.
	
	Early analysis of oil prices and their interaction with the economy introduced petrol as an exogenous supply to the economy. As such models did not distinguish between crude oil and refined petrol imports. Instead, a unified petrol good enters the consumption basket consumption and is used as production input. The first such model was inspired by the differences in response to oil price shocks of similar in recent history \cite{macroeconomic_2007}. In order to reconcile this difference in one modelling framework a simple New-Keynesian model was extended by petrol. Across this model imperfectly competitive firms optimise with regards the amount of petrol used in their production. Households consume petrol as a fixed share of their consumption expenditure. The inflationary impact of exogenous petrol price is consequently driven through the elasticity of substitution between the production factors oil and labour \cite{blanchard_macroeconomic_2007}. As oil gets more expensive production costs rise and firms substitute petrol for labour. This place the firms elasticity of substitution at the heart of oil driven inflation dynamics. This first model has later been extended by in increasing the model in complexity. It was shown that the introduction of capital accumulation amplifies the stagflationary effects of petrol on the economy \cite{vasconez_what_nodate}. Advances in mainstream DSGE literature provided better ways of including wage rigidities into DSGE models by endowing households with wage setting power \cite{smets_shocks_2007}. This mechanism was then used to extend the model by including wage rigidities which factored into the firms elasticity of substitution \cite{leduc_quantitative_2004}. With petrol entering into the consumption basket, households are able to negotiate higher wages to make up for inflation. This broadened the channel of oil driven inflation \cite{leduc_quantitative_2004}.	However, this first approach to including petrol was much focussed on the analysis of short-run shocks. Assumptions such as a fixed petrol share in consumption is reasonable under such short-run perspective, as households struggle to quickly replace petrol consuming investment goods \cite{blanchard_macroeconomic_2007}. An exogenous oil price similarly is reasonable for the analysis of short-term shocks as prices surge exogenously. However, in order to analyse the effects of petrol in the long-run and under non-crisis regimes extensions of the model were required.
	
	This leads to the second stream of literature. With the adoption of DSGE modelling across many central banks complex models tailored to the conditions of national economies came about. Soon petrol was introduced as an important aspect to these models. Pioneered by the Bank of England (BoE) the small open economy model of the BoE was extend by oil \cite{harrison_evaluating_2010, harrison_impact_2011}. This introduced endogenous oil production in a small open economy \cite{harrison_impact_2011}. The model further introduced a distinction between crude oil and refined petrol, whereas the latter results from a separate sector of production. This is complemented by a utility sector, modelling the effect of gas prices on marginal electricity production \cite{harrison_impact_2011}. Both refined petrol and utilities factor into the consumption basket as well as into the domestic production sector. Consequently, the petrol inflation channel assumes a more complex dynamic, operating through production and household wage negotiation \cite{harrison_impact_2011}. Moreover, the possibility to trade has important implications since with rising petrol prices a substitution away from oil also allows for trading non-utilised domestic oil production. Substitution not only avoids higher cost but turns into a source of revenue, an important aspect for long-run dynamics of petrol utilisation \cite{harrison_impact_2011}. Following the BoE much research has been conducted in tailoring models, containing amongst many other factor petrol, to the specific dynamics of national economies \cite{lees_introducing_2009, malakhovskaya_are_2014, hou_oil_2016}. 
	
	As this work's purpose is an analysis of petrol prices and their interaction with inflation it will rely on the model of Blanchard \& Galì (2008) \cite{blanchard_macroeconomic_2007}. This model focusses on the foundation of petrol inflation interaction while later approaches aim at a comprehensive analysis of the economy. In doing so more complex models confound many factors, not allowing for the clear cut analysis of the basic mechanism behind oil price inflation.
	
	\section{Real Business Cycle model}
	The Real Business Cycle model (RBC) was first introduced by Prescott (1986) \cite{prescott_theory_1986} providing an explanation to the origin of business cycles. Departing from Schumpeterian technology induced growth the RBC model is at its heart a de-trended growth model emphasising the link between technology growth and total factor productivity. It argues that technological innovation increases productivity and raises real wages and return to capital. The resulting increase in income leads to the efficient adjustment of the economy to its new optimum \cite{prescott_theory_1986}. 
	In making this argument several assumption are required. These are namely perfect competition, flexible prices and the use of an identical production technology across firms. 
	While the argument of efficient business cycles is antithetical to the New-Keynesian literature, who regards business cycles as inefficient, both models share a the mathematical foundation. The following will therefore derive an RBC model laying the foundation for the standard New-Keynesian model.
	
	\subsection{The Household}
	Households are assumed to have homogeneous preferences and to solve their optimisation problem simultaneously. Given these properties the RBC introduces one representative agent solving the problem in place of a continuum of households.
	The agent is faced with an income from labour wage $W_t$, returns on last periods capital holdings $K_t{1-}$ and firm dividends $\Pi_t$. Income is divided between consumption, capital investment $I_t$ and leisure, while the leisure expenditure is implicitly made by choosing the number of labour hours $L_t$. The RBC assumes a Robinson-Crusoe economic setting in which households own firms and consequently receive dividends from profits. However, under perfect competition firm profits are zero and so is dividend income at equilibrium \cite{prescott_theory_1986}. 
	Moreover, households are infinitely lived, thus solve their problem discounted into all future periods. The utility function of households is assumed to take the following form.
	\begin{equation}
		U(C_t, L_t) = \frac{C_t^{1-\sigma}}{{1-\sigma}} - \frac{L_t^{1+\phi}}{1+\phi} 
	\end{equation}
% check \sigma claim
	In this specification $\sigma$ is a coefficient of risk aversion and $\phi$ is the marginal disutility to labour. While other utility specification have been employed across the RBC literature this work will remain with a separable utility function in order to be consistent with the New-Keynesian literature. The function $U(C_t, L_t)$ exhibits the usual characteristics of well behaved utility function $\frac{\partial U}{\partial C_t} > 0$, $\frac{\partial U}{\partial L_t} \leq 0$ and $\frac{\partial^2 U}{\partial C_t^2} \leq 0$, $\frac{\partial^2 U}{\partial L_t^2} \leq 0$.
	
	The household maximisation problem then reads as follows, where investment factors into the capital stock. $K_t$ then evolves over time based on the inflows and annual depreciation $\delta < 1$.
	\begin{equation}
		\begin{aligned}
			\max_{B_t, C_t, L_t} \quad \sum_{\infty}^{t=0} \beta^{t} U(C_{t}, L_{t}) \\
			\textrm{s.t.} \quad C_t + I_t = R_t K_{t-1} + W_t N_t + \Pi_t \\
						  K_t = (1 - \delta) K_{t-1} + I_t \\
			\textrm{FOC:}\\
							 \quad - \frac{U_{n,t}}{U_{c, t}} = W_t \\
							 \quad	\E_t \left[ \frac{U_{c,t}}{U_{c,t+1}} \right]^{\sigma} = \beta \left( 1 - \delta + \E_t[R_{t+1}]\right)
		\end{aligned}
	\end{equation}
	The household optimisation yields two main conditions. The Euler equation describes the intertemporal consumption trade-off and the labour supply. Replacing the partial derivates of the utility function leads to the two main equations of from the household optimisation.
	\begin{equation} \label{eq:rbc_hh_foc}
		\begin{aligned}
			C_t^\sigma L_t^\phi	= W_t \\
			\E_t \left[ \frac{C_{t+1}}{C_t} \right]^\sigma = \beta \left(1 - \delta  + \E_t [R_{t+1}] \right)
		\end{aligned}
	\end{equation}
	
	\subsection{The firm}
	Just as households firms are also assumed to be identical in their production technology wherefore the continuum of firms is replaced by a representative firms. The firm solves a static optimisation problem in choosing its production inputs capital and labour. Due to the assumption of perfect competition and flexible prices firms pay the marginal product to its production factors. This is formalised below and yields the capital and labour demand equations.
	\begin{equation} \label{eq:rbc_firm_foc}
		\begin{aligned}
			\max_{N_t} \quad & A_t K_t^\alpha N_t^{1 - \alpha} \\
			\textrm{s.t.} \quad & Y_t = W_t N_t + K_t R_t\\
			\textrm{FOC:} \quad W_t = (1 - \alpha) A_t K_t^\alpha N_t^{-\alpha}\\
							\quad K_t = \alpha A_t K_t^{\alpha -1} N_t^{1-\alpha}
		\end{aligned}
	\end{equation}

	\subsection{Equilibrium}
	Having derived the above optimality condition an analysis of the equilibrium requires two further assumptions. These are the equilibrium resource constraint arising from the output accounting identity $Y = I + C$ and an expression of for technology $A_t$. The innovation of technology is assumed to be exogenous and path dependent, it is therefore modelled as a stochastic first order autoregressive process AR(1), where $\bar{A}$ represents the equilibrium value of technology.
	\begin{equation} \label{eq:rbc_eqil}
		\begin{aligned}
			Y_t = C_t + I_t \\
			\log(A_t) = (1- \rho_A) \log(\bar{A}) + \rho_A \log(A_{t-1}) + \epsilon_t^A
		\end{aligned}
	\end{equation}

	The derived system of equations \eqref{eq:rbc_eqil} \eqref{eq:rbc_firm_foc}, \eqref{eq:rbc_hh_foc} in its current state is non-linear. This is due to the multiplicative Cobb-Douglas production function and the evolution of capital as a state variable without full depreciation \cite{campbell_inspecting_1994}. In order to be able to analyse the system in its behaviour around the equilibrium as well as to link the system to data some form of linearisation is needed. This method of linearisation has significant impact the when relating models to real data (ref) (Taylor  Uhlig, 1990). The most common approach adopted across literature is a log-linear first order Taylor approximation \cite{campbell_inspecting_1994}. This work will follow suit.
% put into appendix
	Deriving the log-linearisation requires some additional reasoning which is provided in the \nameref{appc}.
	
	\begin{table}
		\caption{RBC Equations}
		\fontsize{9pt}{9pt}\selectfont
		\centering
		\begin{tabular}{llr}
			\textbf{Equation name} & Equation & \textbf{Log-linear expression}\\
			\hline
			Euler equation &
			$\E_t \left[ \frac{C_{t+1}}{C_t} \right]^\sigma = \beta \left[ (1 - \delta)  + \E_t [R_{t+1}] \right]$ &
			$\hat{c}_t = \hat{c}_{t+1} - \hat{r}_{t+1}$ \\
			Capital supply & 
			$K_t = (1 - delta) K_{t-1} + I_t$ &
			$\hat{i}_t = \delta \hat{k}_{t+1} - \frac{1-\delta}{\delta} \hat{k}_t$ \\
			Capital demand &
			$\hat{y}_t - \hat{k}_t = \hat{r}_t$ &
			$K_t = \alpha A_t K_t^{\alpha -1} N_t^{1-\alpha}$ \\
			Labour supply & 
			$- \frac{U_{n,t}}{U_{c, t}} = W_t$ &
			$\hat{c}_t = \hat{w}_t - \frac{\bar{l}}{1-\bar{l}} \hat{l}_t$ \\
			Labour demand &
			 $W_t = (1 - \alpha) A_t K_t^\alpha N_t^{-\alpha}$ & 
			 $\hat{y}_t - \hat{l}_t = \hat{w}_t$ \\
			Production function &
			$Y_t = A_t K_t^\alpha N_t^{1 - \alpha}$  &
			$\hat{y}_t = \hat{a}_t + \alpha \hat{k}_t + (1-\alpha) \hat{l}_t$ \\
			Equilibrium condition &
			$Y_t = C_t + I_t$ &
			 $\hat{y}_t = \frac{\bar{y}}{\bar{c}} \hat{c}_t + \frac{\bar{i}}{\bar{y}} \hat{i}_t$ \\
			Technology &
			 $\log(A_t) = (1- \rho_A) \log(\bar{A}) + \rho_A \log(A_{t-1}) + \epsilon_t^A$ &
			 $\hat{a}_t = \rho_a \hat{a}_{t-1} + \epsilon_{a,t}$ \\
		\end{tabular}
	\end{table}
	
	The above linearisation still contains steady state values indicated as $\bar{x}$. In order to express the model as a linear system these have to be replaced with their respective deterministic values. However, the RBC model under separable consumption labour utility, does not possess a deterministic steady state for labour (ref). This issues has often been circumvented by relying on a logarithmic specification of household utility \cite{campbell_inspecting_1994} \footnote{In the work of Campbell the analytical derivation relies on a log-utility function}. However, the purpose of this work is a comparison between an RBC and New-Keynesian models. The latter traditionally rely on separable utility, wherefore this work will use separable utility for the RBC as well, in order allow for comparison on similar basis. In order to provide further analysis the below will express the steady state values as ratios of labour \cite{prescott_theory_1986}. The deterministic labour ratios for the RBC model are as below.
	
	\begin{table}[H]
		\fontsize{9pt}{9pt}\selectfont
		\centering
		\caption{RBC steady state}
		\begin{tabular}{lr}
			\textbf{Variable} & \textbf{Deterministic steady state}\\
			\hline 
			Rate of return & $\bar{R} = \Rss$ \\
			Capital labour ratio & $\frac{\bar{K}}{\bar{L}} = \KLss$ \\
			Wage & $\bar{W} = (1 - \alpha) \Ass \left(\KLss\right)^\alpha$ \\
			Investment labour ratio & $\frac{\bar{I}}{\bar{L}} = \delta \KLss$ \\
			Output labour ratio & $\frac{\bar{Y}}{\bar{L}} = \Ass \left(\KLss\right)^\alpha$ \\
			Consumption labour ratio & $\frac{\bar{C}}{\bar{L}} = \frac{\bar{Y}}{\bar{L}} - \frac{\bar{I}}{\bar{L}}$ \\
		\end{tabular}
	\end{table}

	In order to obtain a linear state-space representation of the RBC model a numerical approximation of the steady state for the value of labour $\bar{l}$ is required. This work relies on the software package gEconpy (ref) for this numerical approximation. The specification of the input file, can be found in the \nameref{appe}. The resulting steady-state values and the state-space transition matrix can be found in the \nameref{appc}. 

	\section{NK} \label{sec:NK}
	The New-Keynesian model developed in this section is based on the work of \cite{gali_monetary_2008}. It diverges from the RBC model in three key aspects.
	Firstly, the NK model introduces nominal rigidities through monopolistic competition. Since firms are endowed with some market power, they can reset prices above marginal cost, introducing a gap between real and nominal marginal cost. This dynamic results in inflation. This extension has several consequences. For one monetary policy is neutral in the short-run, thus changes in interest are note directly matched by changes in expected inflation \cite{gali_monetary_2008}. This gap in expectations becomes the driver of short-run fluctuation in the economy. Business cycles is thus no-longer driven by technological progress but result from an adjustment of the interest rates. The standard New-Keynesian model thus presents a perspective in which the intervention of a monetary authority leads to inefficient short-term adjustments of the economy. The NK returns Keyne's perspective, regarding regarded business cycles as inefficient. The the main divergence from RBC literature consists in this change of perspective.
	
	Secondly, the NK model in its simplest form analyses an economy without capital. The capital stock is thus no longer the intertemporal instrument. Instead the NK model uses the gap between actual and nominal output, where nominal refers to the output without price frictions \cite{gali_monetary_2008}.
	
	\subsection{Consumption bundle}
	The New-Keynesian framework introduces imperfectly substitutable goods as source of market power in order to introduce monopolistic competition. This results in the firms ability to set a price beyond marginal cost for their good of production. This implies that there is not one single good but a continuum of differentiated goods. Consequently, the households allocation problem becomes twofold.
	For one households need to decide on the basket of products that optimises their utility. Only once the optimal basket and its price is determined the household is able decide on the optimal allocation of income across consumption and leisure. The consumption basket composition thus needs to be solved prior to the consumption-leisure trade-off. 
	Across the DSGE literature the aggregation of consumption is also sometimes referred to as the output of a good bundling firm. This intermediary party determines the optimal basket of goods according to the household preferences. In a perfectly competitive environment the household buys the optimal bundle at no mark up beyond the initial producer's mark-up.
	Determining the optimal basket requires some function of aggregation across the continuum of goods. The most commonly employed aggregator is the Dixit-Stiglitz function (ref). This aggregator has appealing properties, namely a constant elasticity of substitution (CES), which will become important later in deriving marginal cost.
	
	The bundling optimisation problem reads as follows where each good $C_{it}$ is matched by its price $P_{it}$ purchased with budget $Z_t$.
	\begin{equation}
		\begin{aligned}
			\max_{C_{it}}
			\quad \left(\int_{0}^{1} C_{it}^{ \frac{\epsilon - 1}{\epsilon} } di 
			\right)^{ \frac{\epsilon}{\epsilon - 1} }\\
			\textrm{s.t.}\\
			\quad \int_{0}^{1} P_{it} C_{it} di \leq Z_t
		\end{aligned}
	\end{equation}
	Solving the above optimisation problem results in three important expression. First, the aggregated price level $P_t$ of the optimal consumption bundle is derived. This corresponds to the consumer price index (CPI).
	\begin{equation} \label{eq:1}
			P_t = \left(
						\int_{0}^{1} P_{it}^{ 1 - \epsilon } di 
					\right)^{ \frac{1}{1 - \epsilon} }
	\end{equation}
	Further manipulation leads to the share of product $C_{it}$ in the consumption basket $C_t$. This share is defined by the goods price relative to the price level $P_t$ and the CES between goods $\epsilon$.
	\begin{equation} \label{eq:cshare}
		C_{it} = \left( \frac{P_{it}}{P_t} \right)^{- \epsilon} C_t
	\end{equation}
	Lastly, using the two above it can be demonstrated that under a binding budget constraint individual good prices correspond to the CPI times the optimal consumption bundle.
	This property is important as it allows to solve the household and firm optimisation problem as a representative agent problem.
	\begin{equation}
		\int_{0}^{1} P_{it} C_{it} di = Z_t = P_t C_t
	\end{equation}

	\subsection{Household}
	Having found $C_t$ and its price $P_t$ the household's consumption-leisure allocation can be formulated. The derivation follows from the RBC except for investment expenditure and the extension by bond holdings $B_t$. While $B_t$ is zero in equilibrium they introduce the nominal interest rate into the households problem. The nominal interest rate is contained in the discount factor $Q_t = \frac{1}{1+i_t}$ and the first step to introducing a monetary authority. Similar to the RBC a representative infinitely lived agent solves the optimisation problem for all future periods. 
	\begin{equation}
		\begin{aligned}
			\max_{B_t, C_t, L_t} \quad \sum_{\infty}^{t=0} \beta^{t} U(C_{t}, L_{t}) \\
			\quad P_t C_t + Q_t B_t = B_{t-1} + W_t N_t \\
			\textrm{FOC:} \\
				- \frac{U_{n,t}}{U_{c, t}} = \frac{W_t}{P_t}\\
				Q_t = \beta \E_t \left[ \frac{U_{c, t+1}}{U_{c,t}}^\sigma \frac{P_t}{P_{t+1}} \right] 			
		\end{aligned}
	\end{equation}
	Solving the optimisation problems yields the labour supply. In the NK it explicitly depends on the real wage. Moreover, as in the and the Euler equation is obtained. Assuming the function form of $U(C_t, L_t) = \frac{C_t^{1-\sigma}}{{1-\sigma}} - \frac{L_t^{1+\phi}}{1+\phi}$ and plugging its derivatives into the first order condition yields.
		\begin{equation} \label{eq:nk_hh_foc}
		\begin{aligned}
			C_t^\sigma L_t^\phi	= \frac{W_t}{P_t} \\
			Q_{t,t+k} = \E_t \left[ \frac{C_{t+k}}{C_t} \right]^\sigma \frac{P_t}{P_{t+1}}
		\end{aligned}
	\end{equation}
	
	\subsubsection{Price setting}
	The main innovation of the NK model lies in modelling inflation. As mentioned in the above inflation arises from the imperfect substitutability of consumption goods. This results in monopolistic competition endowing firm's price setting power. Modelling the price setting process is thus at the heart of the NK model. In doing so the standard model relies on the Calvo-pricing mechanism (ref). Firms are able to reset prices in any period with the probability $\theta$. This is equal to saying that every period a share $\theta$ of firms can reset prices to their optimal price $\hat{P}_t$ while $1-\theta$ firms have to remain with last periods price $P_{t-1}$. This allows to describe the evolution of the price level as an in the following intertemporal equation, which if divided by $P_{t-1}$ yields an expression of inflation.
	\begin{equation}
		P_t = 
		\left[
		\theta P_{t-1}^{1 - \epsilon} + (1 - \theta) \hat{P}_t^{1 - \epsilon}
		\right]^{\frac{1}{1 - \epsilon}}
	\end{equation}

	\begin{equation} \label{eq: inlfation}
		\Pi_t^{1-\epsilon} = \theta + (1 - \theta) \left(\frac{\hat{P}_t}{P_{t-1}} \right)^{1-\epsilon}
	\end{equation}

	The key aspect to price evolution thus lies in the optimal price $\hat{P_t}$. The further it is away from last periods price, the higher is inflation. \\
	
	\subsubsection{The firm}
	In the RBC the firm solved a static optimisation problem providing labour and capital demand. In the NK model they are instead faced with setting the optimal price $\hat{P}_t$. For this the NK model assumes that a continuum of firms each producing one good provide factoring into the continuum of goods as outlined in (section). Setting the optimal price becomes an intertemporal problem as firms trade-off between two opposing forces. 	
	As illustrated by (ref) the share of any good in the overall basket depends on its price relative to the CPI. This implies that setting a price far above $P_t$ leads to a lower demand for a firm's single good $C_{i,t}$. 
	On the other hand, as firms are limited in their price frequency they need to set a price which they would be comfortable with in the future. Under the Calvo-scheme a price once set will be unchanged with probability $\theta^k$ for $k$ periods. Consequently, if the price once set is too low it will at some point allow for no further profits potentially even incur losses. 
	Firms thus try to set a price that allows them an acceptable share in the basket of goods while also guaranteeing profitability across future periods. In this decision the size of $\theta \in [0,1]$ combined with the time discount factor $Q_t \beta$ weights the second force against the first. 
	The problem can be formalised as a future discounted sum of revenue minus total cost. The notation of $Y_{t+k|t}$ hereby refers to the income in period $t+k$ at price of period $t$. The notation of $Q_{t,t+k}$ is the stochastic discount factor of today's discount carried forward into period $t+k$. The budget constraint makes use of the fact that under equilibrium assumption all output is consumed as part of the consumption basket $Y_{it} = C_{it}$. (section) will deliver a more in-depth perspective on this.	
	\begin{equation}
		\begin{aligned}
			\max_{\hat{P}_t}
			\quad
			\sum_{k=0}^{\inf} \theta^k \E_t 
			\left[
			Q_{t, t+k} 
			\left(
			\hat{P}_t Y_{t+k|t} - TC_{t+k|t}^n(Y_{t+k|t})
			\right)
			\right] \\
			\textrm{s.t.}
			\quad
			Y_{it+k|t} = \left(\frac{\hat{P}_t}{P_{t+k}} \right)^{-\epsilon} C_{t+k} \\
			\textrm{FOC:} \quad
			\sum_{k=0}^{\infty} \theta^k \E_t 
			\left[
			Q_{t,t+k} Y_{t+k|t} 
			\left(
			\hat{P}_t - \frac{\epsilon}{1 - \epsilon} MC_{t+k|t}^n
			\right)
			\right]
			= 0
		\end{aligned}
	\end{equation}
	
	Inserting the budget constraint and the discount factor as defined by the Euler condition \eqref{eq:nk_hh_foc} results in:
	\begin{equation}
		\frac{\hat{P}_t}{P_t} = \frac{\epsilon}{\epsilon-1} 
		\frac{
		\E_t \sum_{k=0}^{\infty} \theta^k \beta^k C_{t+k}^{1-\sigma} (\frac{P_{t+k}}{P_t})^\epsilon MC_{t+k}^r
		}{
		\E_t \sum_{k=0}^{\infty} \theta^k \beta^k C_{t+k}^{1-\sigma} (\frac{P_{t+k}}{P_t})^{\epsilon-1}
		}
	\end{equation}
	The above describes the mark-up of the optimal price $\hat{P}_t$ beyond the price level CPI. The difference between the two directly drives inflation as it implies a higher price level as can be seen in \eqref{eq: inflation}. 
	In the steady state inflation is zero by definition, implying that no firm resets prices. This is equivalent to saying that $\theta=0$. Multiplying the above by $P_t$ and setting $\theta=0$ results in $\hat{P}_t = \frac{\epsilon}{\epsilon-1} MC_t^n$. This allows to derive an important aspect to the NK model, the price-mark up over nominal marginal cost in the steady state. Firms are endowed with market power, allowing them to set a mark up above marginal cost. The real marginal defined as $MC_t^r \equiv \frac{MC_t^n}{P_t}$ cost can thus be derived. It is important to note that $MC^r$ are the real marginal cost in the steady state, wherefore they are time independent. They are complemented by $MC_t^r$, the time depended marginal cost, which can deviate from their steady state value leading to the gap of $\hat{MC}_t^r \equiv MC_t^r - MC^r$.
	\begin{equation}
		MC^r = \frac{\epsilon}{\epsilon-1}
	\end{equation} 
	
	
	\subsubsection{Equilibrium conditions}
	To this point a number of optimal choices have been derived. The households decides on the consumption basket and consumption-leisure trade-off. The firm sets the optimal price and consequently mark-up over nominal marginal costs, yielding an expression for inflation. Having derived the optimality conditions for all agents the model now requires market clearing condition to describe their interaction. This is based on accounting identity on individual good and production level $Y_{it} = C_{it}$. In order to close the model one needs to demonstrate, that this property on individual goods level translates in the global property $Y_t = C_t$.
	
	Output $Y_t$ is defined as an aggregator function. The accounting identity combined with \eqref{eq: cshare} allows to link $Y_t$ to the the price level, allowing to demonstrate that $Y_t = C_t$ holds.
	\begin{equation}
			Y_t = 
			\left( 
				\int_{0}^{1} Y_{it}^{\frac{\epsilon - 1}{\epsilon}} di 
			\right)^{\frac{\epsilon}{\epsilon - 1}}
			=
			\left( 
			\int_{0}^{1} 
			\left[
			\left( \frac{P_{it}}{P_t} \right)^\epsilon C_t
			\right]^{\frac{\epsilon - 1}{\epsilon}} di 
			\right)^{\frac{\epsilon}{\epsilon - 1}}
			=
			P_t^{\epsilon} C_t P_t^{-\epsilon}
			=
			C_t
	\end{equation}
	A similar reasoning is required in order to derive an economy wide production function an expression from the firm's individual production functions $Y_{it} = A_t N_{it}$. This yields the below expression, which will be kept unchanged for now.
	\begin{equation} \label{eq: Nt}
		N_t = 	
		\int_{0}^{1} \left( \frac{Y_t}{N_t} \right)^{\frac{1}{1 - \alpha}}
		\int_{0}^{1} \left( \frac{P_{it}}{P_t} \right)^{-\frac{\epsilon}{1 - \alpha}} di
	\end{equation}

	\subsubsection{Log linearisation}
	
	This completes the number of equations required for the derivation of the New Keynesian model. As the above equations are a non-linear system without a closed form solution some method of linearisation is required. The below will follow the procedure of \cite{gali_monetary_2008} and apply log-linearisation first order Taylor approximations. Throughout the log-linearisation several manipulations are required. The Euler equation relies on reasoning about the non-inflationary steady state so does the expression for $N_t$. Explanation and proofs can be found in \ref{appc}. Lower-case expressions are the log-equivalent to previous upper case variables. \\

	Euler equation
	\begin{equation} \label{eq:lleuler}
		c_t = \E_t[c_{t+1}] - \frac{1}{\sigma} (i_t - \rho - \E_t [\pi_{t+1}])
	\end{equation}
	Labour supply 
	\begin{equation} \label{eq:llslabour}
		w_t - p_t = \sigma c_t + \phi n_t
	\end{equation}
	Inflation
	\begin{equation}\label{eq:llpi}
		\pi_t = (1 - \theta) (\hat{p}_t - p_{t-1})	
	\end{equation}
	Optimal price 
	\begin{equation}\label{eq:llpstar}
		\hat{p}_t = (1 - \theta \beta) \E_t
		\left[
		\sum_{k=0}^{\infty} \theta^k \beta^k \left( mc_{t+k|t}^r - mc^r +p_{t+k}\right)
		\right]		
	\end{equation}
	Production function
	\begin{equation}\label{eq:llprod}
		y_t = a_t + (1 - \alpha) n_t
	\end{equation}
	Equilibrium condition 
	\begin{equation}\label{eq:lleq}
		y_t = c_t
	\end{equation}

	\subsubsection{New Keynesian Phillips and IS curve}
	
	The next step is the derivation of an expression for real marginal cost $mr_{t}^r$ contained in the optimal price setting. In the absence of other production factors the marginal product of labour $MPN_t = \frac{\partial Y}{\partial N} = A_t (1- \alpha) N_t^{-\alpha}$ corresponds to nominal marginal cost $mc_t^n$. Real marginal cost can thus be obtained from $MC_t^r = \frac{W_t}{P_t MCN_t}$. This allows to formulate the below property in logs by including the production function \eqref{eq:llprod}.
	\begin{equation}
		\begin{aligned}
			mc_t^r = w_t - p_t - a_t - mpn_t \\
%			mc_t^r = w_t - p_t - a_t - \ln(1 - \alpha) + \alpha n_t \\
			mc_t^r = w_t - p_t - \frac{a_t - \alpha y_t}{1 - \alpha} - \ln(1 - \alpha) \\
		\end{aligned}
	\end{equation}
	This expression is at the provides the basis for two essential conclusions. 
	Firstly, it allows to derive a relation between $mc_t^r$ and output $y_t$. Using labour supply \eqref{eq:llslabour} and the production function \eqref{eq:llprod} one can rephrase the above as. 
	\begin{equation}
		\begin{aligned}
			mc_t^r = \sigma c_t + \phi n_t - [a_t + \alpha n_t + \ln(1-\alpha)] \\
			mc_t^r = 
			\frac{
				\sigma (1 - \alpha) + \phi + \alpha
			}{
				(1 - \alpha)	
			}	 y_t
			- \frac{
				(1 + \phi)	
			}{
				(1 - \alpha)	
			} a_t
			+ \ln(1-\alpha)
		\end{aligned}
	\end{equation}
	In the steady state marginal cost should be equal to its natural level and $mc_t^r = mc^r$. In this case the above describes natural output $y_t^n$. This fact can be used to derive an expression for the gap between marginal cost and its steady state value in logs $\hat{mc_t}^r \equiv = mc_t^r - mc^r$. In doing so one 
	Reformulating the above by replacing $mc_t^r$ by $mc^r$ yields an expression of the output gap in terms of the marginal cost gap. 
	\begin{equation} \label{eq:llmcrhat}
		\begin{aligned}
			\hat{mc_t}^r = \frac{\sigma (1 - \alpha) + \phi + \alpha}
			{1 - \alpha)} (y_t - y_t^n)
		\end{aligned}
	\end{equation}
	The second insight arising from the real marginal cost is its translation in period $t+k$. Combining this with the share of consumption good $i$ in the overall basket as well as the market clearing condition \eqref{eq:lleq} logs gives 
% mc_{t+l}^r = w_{t+k} - p_{t+k} - \frac{a_ {t+k} - \alpha y_{i,t+k|t}}{1 - \alpha} - \ln(1 - \alpha)
% check this
	\begin{equation}
		y_{t+k|t} = -\epsilon(p_{t+k|t} - p_{t+k}) + y_{t+k}
	\end{equation}
	Combining the two yields an expression of marginal cost and the price level.
	\begin{equation}
		mc_{i, t+k|t}^r = mc_{t+k}^r - \frac{\epsilon \alpha}{1 - \alpha}(\hat{p}_t - p_{t+k})
	\end{equation}
	The above combined with the expression for the optimal price level.
% more math??	
	\begin{equation}
		\begin{aligned}
			\hat{p}_t - p_{t-1} =
			\quad
			(1 - \theta \beta) \E_t
			\left[
			\sum_{k=0}^{\infty} \theta^k \beta^k \left( mc_{t+k|t}^r - mc^r +p_{t+k} - p_{t-1}\right)
			\right] \\			
			=
			\quad
			(1 - \theta \beta) \Theta \E_t
			\sum_{k=0}^{\infty} \theta^k \beta^k (mc_{t+k}^r - mc^r) + 
			\sum_{k=0}^{\infty} \theta^k \pi_{t+k} \\
			=
			\quad
			\theta \beta \E_t (\hat{p}_{t+1} - p_{t}) + (1 - \theta \beta) \Theta (mc_{t+k}^r - mc^r) + (1 - \theta) (\hat{p}_t - p_{t-1})
		\end{aligned}		
	\end{equation} 

	This can be rewritten in more compact form as \footnote{where $\Theta \equiv \frac{1 - \alpha}{1 - \alpha + \alpha \epsilon} \leq 1$} \footnote{	where $\lambda \equiv \frac{(1-\theta)(1-\beta\theta)}{\theta} \Theta$}
	\begin{equation}
		\pi_t = \beta E_t [\pi_{t+1}] + \lambda \hat{mc}_{t}
	\end{equation}

	The above combined with \eqref{eq:llmcrhat} allows to construct the New-Keynesian Phillips curve, on of the two core equations of the NK model.\footnote{where $\kappa = \lambda \frac{\sigma (1 - \alpha) + \phi + \alpha}{1 - \alpha)}$}
	\begin{equation} \label{eq:llnkp}
		\begin{aligned}
			\pi_t = \beta E_t [\pi_{t+1}] + \kappa \tilde{y}_t
		\end{aligned}
	\end{equation}
	
	The second core equation of the New-Keynesian model is derived from the Euler equation \eqref{eq:lleuler}, the equilibrium condition \eqref{eq:lleq} and the log linear Fisher identity $r_t \equiv i_t - \E_t[\pi_{t+1}]$. Inserting $y_t$ for $c_t$ and introducing the difference between output and its natural level as $\tilde{y}_t \equiv y_t - y_t^n$ allows to write
	\begin{equation}
		\tilde{y}_t = 
		\left[
		\E_t[y_{t+1}] - \frac{1}{\sigma} (i_t - \rho - \E_t[\pi_{t+1}])
		\right]
		-
		\left[
		\E_t[y_{t+1}^n] - \frac{1}{\sigma} (r_t^n - \rho)
		\right]
	\end{equation}
	This provides the New-Keynesian IS curve.
	\begin{equation} \label{eq:llnkis}
		\tilde{y}_t = \E_t[\tilde{y}_{t+1}] - \frac{1}{\sigma} (i_t - r_t^n - \E_t[\pi_{t+1}])
	\end{equation}
% discussion of IS and P curve
	\subsubsection{Monetary policy}
	Closing the model requires an interest rate rule, describing the behaviour of the central bank. This work will rely on a traditional Taylor rule with an exogenous interest rate shock following \cite{gali_monetary_2008}.
	\begin{equation}
		\begin{aligned}
			i_t = \rho + \phi_{\pi} \pi_t + \phi_{y} \tilde{y}_t + v_t \\
			v_t = \rho_v v_{t-1} + \epsilon_t^v
		\end{aligned}
	\end{equation}

	\subsubsection{State Space}
	Having derived the New-Keynesian Phillips curve, New-Keynesian IS equation as well as a monetary policy rule allows to formulate the model as a linear state space. For this purpose 
	\begin{equation}
		\begin{bmatrix}
			\tilde{y}_t \\
			\pi_t
		\end{bmatrix}
		=
		\Omega
		\begin{bmatrix}
			\sigma & 1 - \beta \phi_{\pi} \\
			\sigma \kappa & \kappa + \beta (\sigma + \phi_y)
		\end{bmatrix}
		*
		\begin{bmatrix}
			\tilde{y}_{t-1} \\
			\pi_{t-1}
		\end{bmatrix}
		+
		\Omega
		\begin{bmatrix}
			1 \\
			\kappa
		\end{bmatrix}	
		(\hat{r}_t^n - v_t)
	\end{equation}
	where $\Omega \equiv \frac{1}{\sigma + \phi_y + \kappa \phi_{\pi}}$ and $\hat{r}_t^n \equiv r_t^n - \rho$.
	
%Define stability condition of the linear state-space, Blanchard-Kahn

	\section{NKE}
	The below is an extension of the standard NK model by petrol as a consumption good as well as factor of production. There is no domestic production of petrol, instead it is imported and its price is assumed to be exogenous. Beyond the import of petrol there is no foreign trade, all closed economy assumptions thus hold. 
	Given the exogenous nature of petrol the model is quite similar to the standard NK model. The below will therefore depart from (section) and only outline the changes to the model from petrol production. 	
	
	\subsubsection{Bundler}
	Petrol is considered a consumption good and as such enters into the good basket $C_t$. This inclusion is modelled across the bundling process, deriving the optimal basket across two classes of goods, domestic and foreign. 
	The bundling of domestic goods follows from the same procedure as outlined in (section). Variables $C_{q,t}$ and $P_{q,t}$ are aggregated consumption and price respectively. The optimisation across the continuum of goods results in the same properties as above. These are share of good $C_{i,q,t}$ in the consumption basket $C_{q,t}$ and the aggregated price index $P_{q,t}$ as well as an expression of aggregate consumption. \\
	\begin{equation} \label{eq:o_pindex}
		P_{q,t} = \left( \int_{0}^{1} P_{i,q,t}^{1 - \epsilon} di \right)^{\frac{1}{1-\epsilon}} \\
	\end{equation}
	\begin{equation} \label{eq:o_cshare}
		C_{i,q,t} = \left( \frac{P_{i,q,t}}{P_{q,t}} \right)^{-\epsilon} C_{q,t}
	\end{equation}
	\begin{equation} \label{eq:o_pcon}
		\int_{0}^{1} P_{it} C_{it} di = Z_t = P_t C_t
	\end{equation}
	Foreign goods consumption $C_{m,t}$ exclusively consists of petrol and its share in overall consumption $C_t$ is assumed to be constant. Parameter $\chi$ represents this share. 
	This assumption, though a multiplication, is viable as consumption demand for petrol is relatively inelastic in the short-run. Empirical evidence suggests that due to the inability replace durable goods (e.g. cars) that usually are at heart of consumption demand this demand is inelastic (ref). Overall consumption this is defined as \footnote{where $\Theta = \chi^{-\chi}(1-\chi)^{(\chi-1)}$}
	\begin{equation}
		C_t \equiv \Theta_\chi C_{m,t}^\chi C_{q,t}^{1-\chi}
	\end{equation}
	From this the overall price level of the consumption basket $P_{c,t}$ is derived, introducing $S_t \equiv \frac{P_{m,t}}{P_{q,t}}^\chi$ the real price of oil.
	\begin{equation}
		P_{c,t} \equiv P_{m,t}^\chi P_{q,t}^{1-\chi} 
			= P_{q,t} \frac{P_{m,t}}{P_{q,t}}^\chi
			= P_{q,t} S_t^\chi
	\end{equation}
		
	Following this reasoning and using \eqref{eq:o_pcon} overall consumption can be written as 
	\begin{equation}
		C_t P_{c,t} = C_{m,t}P_{m,t} + C_{q,t}P_{q,t}
	\end{equation}

	\subsubsection{Household}
	Having derived expressions for the consumption basket and its price the household's optimisation problem can be solved. As in the standard NK model this reads as follows. 
% log-utility	
	\begin{equation}
		\begin{aligned}
			\max_{B_t, C_t, L_t} \quad \sum_{\infty}^{t=0} \beta^{t} U(C_{t}, L_{t}) \\
			\textrm{s.t.} \quad P_{c,t} C_t + Q_t B_t = B_{t-1} + W_t N_t \\
			\textrm{FOC:} \\
			- \frac{U_{n,t}}{U_{c, t}} = \frac{W_t}{P_{c,t}}\\
			Q_t = \beta \E_t \left[ \left( \frac{U_{c, t+1}}{U_{c,t}} \right)^\sigma \frac{P_{c,t}}{P_{c,t+1}} \right] 			
		\end{aligned}
	\end{equation}
	
	The authors use a log-utilty function $U(C_t,L_t) \equiv \log(C_t) - \frac{N_t^(1+\phi)}{1+\phi}$ which results in the following first order conditions.
	\begin{equation}
		\begin{aligned}
			Q_t = \beta \E_t \left[ \frac{C_t}{C_{t+1}} \frac{P_{c,t}}{P_{c,t+1}} \right] \\
			\frac{W_t}{P_{c,t}} = C_t N_t^\phi
		\end{aligned}
	\end{equation}

	\subsubsection{The Firm}
	Petrol serving as a second factor of production the firm's optimisation problem is extended from the standard NK model. As in the RBC the firm obtains its production factor demand by equalising their marginal cost.  
	\begin{equation}
		\begin{aligned}
%			\max_{M_{i,t}} \quad A_t M_{i,t}^{\alpha_m}
			\max_{M_t} \quad & A_t M_{it}^{\alpha_m} N_{it}^{\alpha_n} \\
			\textrm{s.t.} \quad & Y_{i,q,t} = W_t N_{i,t} + M_{i,t} P_{m,t}\\
			\textrm{FOC:} \\
			\quad & W_{i,t} = \alpha_n A_t M_{i,t}^{\alpha_m} N_{i,t}^{\alpha_n -1}\\
			\quad & K_{i,t} = \alpha_m A_t M_{i,t}^{\alpha_m -1} N_{i,t}^{\alpha_n}
		\end{aligned}
	\end{equation}
	The real marginal cost can thus be expressed at the equality of marginal input cost of labour and petrol. This constitutes a deviation from the standard NK model,  The the firms nominal marginal cost $MC_{t}^n$ is therefore.
	\begin{equation}
		MC_t^n = \frac{W_{i,t}}{\alpha_n Y_{i,q,t} N_{i,t}^{-1}} = \frac{P_{m,t}}{\alpha_m Y_{i,q,t} M_{i,t}^{-1}}
	\end{equation}
	Having obtained nominal marginal cost an expression for the firm's mark-up can be defined as $\xi_{i,t} \equiv \frac{P_{q,t}}{MC_{i,t}^n}$. With some further manipulation \cite{blanchard_macroeconomic_2007} uses this to obtain the demand for petrol.
	\begin{equation}
		\begin{aligned}
		\xi_{i,t} \frac{P_{m,t}}{P_{q,t}} M_{i,t} = \alpha_m Q_{i,t} \frac{P_{i,q,t}}{P_{q,t}} \\
		M_{i,t} = \alpha_m \frac{Q_{i,t}}{S_t \xi_{i,t}} \frac{P_{i,q,t}}{P_{q,t}}
	\end{aligned}
	\end{equation}

	\subsubsection{Equilibrium}
	The equilibrium conditions follow the standard NK model in assuming the ressource constraint $Y_{i,q,t} = C_{i,q,t}$ on individual firm level. As showed in (section) of the standard NK model overall domestic this property also holds on the overall output level $Y_{q,t} = \left( \int_{0}^{1} Y_{it}^{1-\frac{1}{\epsilon}} di \right)^{\frac{\epsilon}{\epsilon - 1}} = C_{q,t}$. \\
	
	Using this fact the 
	\begin{equation}
		M_t = \frac{\alpha_m Y_{q,t}}{\xi_{t}^n S_t}
	\end{equation}
% show xi_t mathematecally 
	where $\xi_t^n$ is the individual firm mark-up weighted by its share in output $Y_{q,t}$ which is given by the ratio of $\frac{P_{i,q,t}}{P_{q,t}}$. \\
	As bond holdings are zero in equilibrium all  allows to derive an expression for the overall price level, factoring out petrol.
	% check the reasoning behind P_ct C_t = Pqt Cqt - P_mt M_t	
	\begin{equation}
		P_{c,t}C_t = P_{q,t} C_{q,t} - P_{m,t}M_t = \left(1 - \frac{\alpha_m}{\xi_t^n} \right) P_{q,t} Y_{q,t}
	\end{equation}
	
	\subsection{Real GDP}
	
	
	

	\cite{blanchard_macroeconomic_2007} then go ahead and define the GDP deflator $P_{y,t}$ as
	\begin{equation}
		P_{q,t} \equiv P_{y,t}^{1-\alpha_m} P_{m,t}^{\alpha_m}
	\end{equation}
	Moreover, they define the gross domestic product as 
	\begin{equation}
		P_{y,t} Y_t \equiv P_{q,t} Y_{q,t} - P_{m,t} M_t = \left( 1 - \frac{\alpha_m}{\xi_t^n} \right) P_{q,t} Y_{q,t}
	\end{equation}
	
	This yields:
	and expression for petrol inputs $M_t$
	and expression for time dependent real mark-up $\log(\xi_t) \equiv \mu_t \equiv mc_t^r$
	
	
	\subsubsection{Price setting}
	The price setting problem of the firm is equivalent to the price setting in the standard NK model. It diverges in only one notion, the real marginal cost $MC_t^r$. \\
	
	The expression for inflation is obtained as in the standard model
	\begin{equation}
		P_{q,t} = 
		\left[ 
		\theta P_{q,t-1}^{1 - \epsilon} + (1 - \theta) \hat{P}_t^{1 - \epsilon}
		\right]^{\frac{1}{1 - \epsilon}}
	\end{equation}
	The firm solves the same optimisation problem as in the above. 
		\begin{equation}
		\begin{aligned}
			\max_{\hat{P}_t}
			\sum_{k=0}^{\inf} \theta^k \E_t 
			\left[
			Q_{t, t+k} 
			\left(
			\hat{P}_t Y_{t+k|t} - TC_{t+k|t}^n(Y_{t+k|t})
			\right)
			\right] \\
			\textrm{s.t.}\\
			\quad
			Y_{it+k|t} = \left(\frac{\hat{P}_t}{P_{t+k}} \right)^{-\epsilon} C_{t+k}
		\end{aligned}
	\end{equation}
	with the first order condition 
	\begin{equation}
		\begin{aligned}
			\sum_{k=0}^{\infty} \theta^k \E_t 
			\left[
			Q_{t,t+k} Y_{q,t+k|t} 
			\left(
			\hat{P}_{q,t} - \frac{\epsilon}{1 - \epsilon} MC_{t+k|t}^n
			\right)
			\right]
			= 0
		\end{aligned}
	\end{equation}
	
	\begin{equation}
		\pi_{q,t} = \beta E_t [\pi_{q,t+1}] + \lambda \hat{mc}_{t}
	\end{equation}
	
	The difference in the New-Keynesian Phillips curve lies in $\hat{mc}_t \equiv mc_t^p - mc^p = $. Instead of the output gap the mark-up gap becomes the instrument at the heart of the model.
	
	\subsubsection{Log linearisation}
	The above is then log-linearised to the following form
	
	\begin{tabular}{llr}
		Equation name & Equation & Log-linear Equation \\
		\hline
		 Euler equation & 
		 $Q_t = \beta \E_t\left[ \frac{C_t}{C_{t+1}} \frac{P_t}{P_{t+1}} \right]$ &
		 $c_t = \E_t [c_{t+1}] - (i_t - \E_t[\pi_{t+1}] - \rho)$ \footnotemark \\
		 
		 Labour supply &
		 $\frac{W_t}{P_{c,t}} = C_t N_t^\phi$ &
		 $w_t - p_t = c_t + n_t \phi$ \\
		 
		 Oil factor demand &
		 $M_t = \frac{\alpha_m Y_{q,t}}{\xi_t^n S_t}$ &
		 $m_t = -\mu_t^n - st + y_{q,t}$ \footnotemark \\
		 
		 Production function &
	%A_t M_t^{\alpha_m} N_t^{\alpha_n} = 
		 $Y_{q,t} = A_t N_t^{\alpha_n} \left( \frac{\alpha_m Y_{q,t}}{\xi_t^n S_t} \right)^{\alpha_m}$ &
		 $y_{q,t} = \frac{1}{1-\alpha_m} (a_t + \alpha_n n_t - \alpha_m s_t - \alpha_m \mu_t^n)$ \\
		 
		 Gross output & 
		 $P_{c,t}C_t = (1-\frac{\alpha_m}{\xi_t^n})P_{q,t}Y_{q,t}$ &
		 $c_t = y_{q,t} - \chi s_t + \eta \mu_t^n$ \footnotemark \\
		 
		 GDP deflator &
		 $P_{q,t} \equiv P_{y,t}^{1-\alpha_m} P_{m,t}^{\alpha_m}$ &
		 $p_{y,t} = p_{q,t} - \frac{\alpha_m}{1-\alpha_m}s_t$ \\
		
		GDP &
		$P_{y,t}Y_t = \left(1-\frac{\alpha_m}{\xi_t^n}\right) P_{q,t}Y_{q,t}$ &
		$y_t = y_{q,t} + \frac{\alpha_m}{1-\alpha_m}s_t + \eta \mu_t^n$ \\
		
		\hline
	\end{tabular}
% fix footnotes
	\footnotetext{where $\rho \equiv - \ln(\beta)$ and $i_t = \ln(Q_t)$}
	\footnotetext{where $\mu_t^n \equiv \ln(\xi_t^n)$}
	\footnotetext{where $\eta \equiv \frac{\alpha_m}{\xi_t^n-\alpha_m}$}

	The expressions for gross output and output can be combined into
	\begin{equation}
		y_t = \frac{1}{1-\alpha_m} (a_t + \alpha_n n_t)
	\end{equation}
	which combined with the expression for consumption
	\begin{equation}
		c_t = y_t - (\frac{\alpha_m}{1-\alpha_m} + \chi) s_t
	\end{equation}
	
	

	
	\section{Data}
	This work relies on standard economic quarterly time series from 1975 to 2020. All data is obtained from the St. Louis Federal Reserve data base of seasonally adjusted deflated macroeconomic time series \cite{noauthor_federal_nodate}. The specific data codes can be found in the \nameref{appb}. Most data used across the following is standard to the DSGE literature. However, some extensions are necessary to allow for the inclusion of a petrol sector. The below will therefore briefly outline the use of standard data, while providing more detail on the data specific to this work.
	The common variables for DSGE model evaluation are nominal interest rates, consumption, wage and labour hours. The nominal interest rate is proxied by the federal funds effective rate (ref). Consumption is measured by household expenditure data. Labour hours and wage refer to non-farm business total hours worked and average hourly wage. The exclusion of the agricultural sector is generally recommended for the US economy (ref). Moreover, this work employs the WTI Crude Oil Index as proxy for the price of petrol. The WTI dates back longer than the more recent Brent Crude Oil Index, wherefore it is generally used across works in energy economics (ref).
% potential GDP ??
	With regards to inflation $\pi$ this work relies on the median consumer price index for the RBC and the NK model. The NK energy model's aim is to analyse inflation under the inclusion of petrol in the economy. Relying on the CPI, which includes petrol prices, would thus be counterproductive as the model should generate inflation resulting from petrol endogenously. In order to accurately reflect the models ability energy price need to be removed from inflation data. In the NK energy model therefore relies on the "Sticky Price Consumer Price Index less Food and Energy". Similar reasoning applies to consumption expenditure which for the NK energy model specifically excludes expenditure for energy and utilities.
	
	\subsection{Preprocessing}
	Economic data is usually considered in per capita terms in order to account for different population dynamics (ref). Population dynamics are not continuously collected and usually a mix of actual measurement and imputation. As a result population data exhibits significant irregularities \cite{pfeifer_guide_2021}. Using such noisy records would distort the data of more carefully collected economic variables. Population data thus requires some preprocessing on its own. Smoothing the data with the Hodrick-Prescott (HP) filter's at a smoothing factor of 10,000 is recommended in order to de-noise the data \cite{edge_judging_2013}. The smoothed time series captures the real population dynamics and can then be used to obtain per capita variables. At this point it is important to mention that the St. Louis FED data is in chained dollars, thus not concerned by inflation of monetary measurements (ref). 	
% talk about one-sided filter
	Having removed population dynamic and inflation the next step is the separation in two key components, trend and cyclicality. DSGE literature generally studies variables in their deviation from their steady state. This requires an identification of the steady state and its evolution over time. Separating the long-term trend of GDP from its cyclical deviations commonly relies on the HP filter. The filter separates trend and deviation. In the case of quarterly variables a smoothing parameter of 1600 is recommended \cite{ravn_adjusting_2002}. As DSGE models are log-linearised they describe deviation in terms of logarithm, which for small values approximates percentage deviations. In order to transform data into log-deviations from their underlying trend the HP filter is fed with log-trasnformed time series. This procedure by construction yields stationary time series of cyclicality. However, to comply with common standards stationarity is formally confirmed by the Augmented-Dickey Fuller test (ref). Having transformed all trending variables have into stationary log-deviation from their natural level only the percentage variables remain, namely inflation and interest rate. They require no such treatment, however need translation from annual to quarterly rates. This has been done according to the following formula $(1 + \frac{\matr{X}}{100})^{1/4} - 1$.
	
	An overview over the transformed variables as well as their descriptive statistics can be found in the (appendix).
	
		\input{{./graphs/data_descriptives.tex}}[H]		
	
	\subsection{Analysis of models}
	The data used for the analysis spans from 1985 to 2020. Moreover, data from 1975 to 1985 have been used to derive parameter prior distribution from historical averages. 
	As can be seen across the transformed GDP series \ref{fig:data} this period clearly spans three major economic crisis, the 2000 dot.com bubble, the 2008 financial crisis and most recently the Covid-19 pandemic. These carry forward to consumption, labour hours and investment where these extreme times can similarly be observed. 
	When turning to the interest rate great differences across the sample period become apparent. The mid-80s still influenced by the Volker-shock have a comparability high interest rate when compared to the more recent low-interest environment starting as of 2009.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.9\textwidth]{data_variables_transformed.png}
			\caption{Bayesian VAR 2 lags}
			\label{fig:data}
		\end{center}
	\end{figure}

	Running a first two-lag Vector Autoregression (VAR) on output, consumption and labour reveals the difficulty of forecasting. Depicted in blue \ref{fig:var_2lag} shows systematic bias across output and consumption forecasts. Similarly labour hours are forecasted to be much more volatile than actually the case. A more comprehensive VAR model will be presented later in (section):
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.9\textwidth]{bvar_forecast1.png}
			\caption{Bayesian VAR 2 lags}
			\label{fig:var_2lag}
		\end{center}
	\end{figure}
	
	
	The initial success of RBC literature was based on the fact that a model was able to replicate data with the same first and second order moments as real data. For this purpose covariance matrices were compared, this work will follow this tradition. Looking into the model covariances large differences between the New-Keynesian models and the RBC become apparent. While 
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=\textwidth]{cov_matrix.png}
			\caption{Covariance matrices}
		\end{center}
	\end{figure}
	
	Compare model covariance matrices, to actual covariance in data
	Show theoretical impulse response functions, differences between NK and RBC
			
	\section{Bayesian Estimation}
	As explained in the above the RBC and NK models are based on structural parameters of the economy. As such they are not subject to the Lucas 1981 critique, as their parameters do not depend on behaviour of agents but describe their preferences \cite{lucas_jr_tobin_1981}. Parameters thus describe the time invariant dynamic system underling the economy. Evaluating the accuracy of such system is thus tied to evaluating its parameters in capturing the true dynamics \cite{herbst_bayesian_2016}. In this aim two main approaches of evaluation have been drawn up by research over time, frequentist or Bayesian methods. 
	The frequentist approach first relied on the simulation method of moments, which latter developed into the general method of moments (GMM) \cite{blanchard_solution_1980}. This approach attempts to minimize the distance between simulated and real data moments, allowing to discriminate between several specification of the same structural model \cite{christiano_current_1992}. However, as the number of parameters grew GMM conditions were no longer sufficient to estimate model fit \cite{guerron-quintana_bayesian_2013}. Instead, maximum-likelihood (ML) estimation was used but likewise unable to estimate the growing number of parameters \cite{guerron-quintana_bayesian_2013}. Bayesian statistics offered remedy in the convenient formulation of conditional likelihood \cite{guerron-quintana_bayesian_2013}. Based on Bayes's law the parameters' posterior distribution can be found through the definition of a prior distribution and the conditional likelihood. 
	Bayes' law suggests that the posterior of parameter vector $\Theta$ can be obtained by multiplying the prior $P(\Theta)$ with the conditional likelihood of the data $Y_t$ given the parameters $\Theta$. 
	\begin{equation} \label{eq:blaw}
		P( \Theta | Y_{T}) = P(\Theta) L(Y_{T} | \Theta)
	\end{equation}
	Based on Bayes law parameters and its dynamic system can be evaluated using the data the system is meant to generate \cite{herbst_bayesian_2016}. For that three ingredients are required, a distribution reflecting prior believes about the parameters, a likelihood function and an iterative procedure allowing to sample from the posterior. The following section will introduce these three concepts. Moreover, it will introduce the forecasting properties that naturally arise from joining the three. 

%	 DSGE literature has developed a rich body of literature covering the case of the canonical DSGE model rather well (ref). The state of research is currently concerned with the estimation of larger models and non-linear and not normally distributed posterior distributions (ref). However, given the simplicity of the above discussed models current techniques are more than sufficient for this thesis purposes.
		
	\subsection{Kalman Filter}
	Bayesian analysis of DSGE models is concerned with the transition from a prior distribution reflecting previous held knowledge to the posterior \cite{herbst_bayesian_2016}. In this an expression of conditional likelihood is needed \eqref{eq:blaw}. In obtaining the likelihood of a linear system one can make use of the fact that the system is itself a data generating process \cite{andrews_kalman_2008}. The likelihood function is obtained through evaluating the probability of a given data series being generated by the system. 
	A method allowing to do so is the Kalman filter, a method originally developed for tracking moving objects across several sensors \cite{kalman_new_1960}. In order to track any object the filter requires the object's position and velocity to be described in a linear system. The filter then uses this system and sensor data to form a believe about the object's position, also referred to as the system's state \cite{andrews_kalman_2008}. Applied to DSGE modelling the system is the above derived model and its state are the magnitude to which the variables deviate from their respective steady state.	
	In forming a believe about the system states the filter takes into account that sensor signals are inherently noisy. The Kalman filter exploits its knowledge about the nature of this measurement noise in using it to reduce the variance of its believe about the current system state \cite{andrews_kalman_2008}. In doing so the Kalman filter imposes the assumption of Gaussian distributed noise \cite{kalman_new_1960}. This assumption makes use of the fact that multiplying Gaussian distributions results in another Gaussian distribution with lower or equal variance. Decreasing a distribution's variance is equivalent to narrowing down the believe about the current state of the tracked object. Applied to DSGE modelling the assumption of Gaussian noise implies that the joint distribution of model shocks must be Gaussian \cite{herbst_bayesian_2016}. This is satisfied for small scale New Keynesian models if the individual shocks are Gaussian \cite{herbst_bayesian_2016}. 	
	The Kalman filter is especially desirable for two properties. For one it translates its believe about a system's state into a likelihood function \cite{kalman_new_1960}. Moreover, it is the optimal forecast for a linear system \cite{kalman_new_1960}. The below will briefly explain how the filter operates as well as its construction of the likelihood function. In implementing the below in code this has drawn from filterpy, a library for Bayesian filtering (ref).
	
	In a generic linear system with transition matrix $\matr{F}$, the state variables $\matr{X}_{k-1}$ is propagated to $\matr{X}_k$. The shock noise matrix $\matr N$ describes the effect of shocks on the transition process. In forming a believe about the current state $\matr{X}_k$ the Kalman filter proceeds in two steps, a forecast and an update step.
	\begin{equation}
		\begin{aligned}
			\matr{X}_k = \matr F \matr{X}_{k-1} + \matr N \matr{\epsilon}_k \\
			\matr{\epsilon}_k \sim N(0, \sigma_{\epsilon})
		\end{aligned}
	\end{equation}
	
	\subsubsection{Kalman forecast step} \label{kalman_forecast_step}
	In the forecast step the Kalman filter propagates system $\matr X_{k}$ according to the transition matrix $\matr F$. The propagation then yields a believe about the state given $\matr{\hat{X}}_{k|k-1}$ state given information from period $k-1$.
	\[
		\matr{\hat{X}}_{k|k-1} = \matr F \matr X_{k-1|k-1}
	\]
	The second key aspect to the Kalman Filter is the system state variable's covariance matrix $\matr P_{k}$, which is also time dependent. As pointed out in the above, this matrix is assumed to be Gaussian, thus semi-definite positive \cite{andrews_kalman_2008}. The covariance matrix $\matr P_{k}$ is propagated in similar fashion. In doing so measurement noise $\matr Q$ is added to reflect the uncertainty around measurements. This results in $\matr{P}_{k| k-1}$ another Gaussian distribution, reflecting the uncertainty the predicted state $matr{\hat{X}}_{k|k-1}$.
	This procedure without the filter step is used to obtain the three period ahead forecasts used in \nameref{forecast_eval}. The filter's forecast is repeatedly applied without taking reference to a measurement. The confidence intervals of this forecast are derived from the multivariate distribution $\matr{P}_{k| k-1}$ representing the certainty of the believe about state $\matr{\hat{X}}_{k|k-1}$. It's diagonal contains the state variable's standard deviation and is used to construct confidence interval around the predicted state. 
	\[
		\matr{P}_{k| k-1} = \matr F \matr{P}_{k-1| k-1} \matr{F}^T + \matr Q
	\]
	\subsubsection{Kalman filter step}
	Once the predicted state $\matr{\hat{X}}$ and its covariance matrix $\matr{P}_{k| k-1}$ have been obtained Kalman filter proceeds to identifying the most likely system state. For this it takes into account the state measurement $\matr{z}_t$ and its measurement noise. 
	Due to measurement noise the system's real state is in-between its predicted state $\matr{X}_{k| k-1}$ and its measurement $\matr{z}_k$. The objective of filtering is to identify the exact point in-between the two \cite{andrews_kalman_2008}. For this purpose the Kalman filter derives the Kalman gain $\matr{K}_{k| k-1}$, a weighting function between $\matr{X}_{k| k-1}$ and $\matr{z}_k$. In building $\matr{K}_{k| k-1}$ the filter takes into account the process noise, the covariance matrix and the accuracy of past iterations.	
	This requires $\matr{y}_k$ a measure of distance between measurement and prediction is derived. In calculating $\matr{y}_k$ the measurement matrix $\matr H$ translats measurement $\matr{z}_t$ into the same units as the predicted state $\matr{X}_{k| k-1}$. For the purposes of this work all unit transformation has been done prior to filtering, wherefore the measurement matrix corresponds to the identity matrix $\matr{I}$.
	\[
		\matr{y}_{k} = \matr{z}_k - \matr H \matr{X}_{k| k-1}
	\]
	Filter uncertainty $\matr{S}_k$ is obtained by combining the covariance $\matr{P}_{k| k-1}$ of the predicted state $\matr{X}_{k| k-1}$ with the process noise $\matr{R}$. In the context of DSGE filtering the process noise is derived from the covariance matrix $\matr{N}$ as defined above \cite{guerron-quintana_bayesian_2013}.
	\[
		\matr{S}_k = \matr H \matr{P}_{k| k-1} \matr{H}^T + \matr{R}
	\]
	The uncertainty term $\matr{S}_{k}$ is then used to calculate the weighting function between measurement and predicted state, the Kalman gain. 
	\[
		\matr{K}_k = \matr{P}_{k| k-1} \matr{H}^T \matr{S}_{k}^{-1}
	\]
	The filtered state, which will be at the beginning of the next iterations, is then obtained from the Kalman gain and distance between measurement and predicted state $\matr{y}_k$. The covariance matrix $\matr{P}_{k|k-1}$ is similarly updated. This yields a new narrower Gaussian distribution $\matr{P}_{k|k}$, summarising the uncertainty around the system's true state.
	\begin{equation}
		\begin{aligned}
			\matr{X}_{k|k} = \matr{X}_{k| k-1} + \matr{K}_k \matr{y}_k
			\matr{P}_{k|k} = (\matr I - \matr{K}_k \matr{H}) \matr{P}_{k|k-1}		
		\end{aligned}
	\end{equation}
	Having introduced the Kalman filter algorithm the question of its initial condition remains. As DSGE models describe deviations from the steady state $\matr{X}_{0|0}$ is set to zero, assuming the system to be in steady state. This approach is generally followed by filtering literature \cite{schorfheide_loss_2000}. 
	
	\subsubsection{Likelihood function} \label{kalman_ll}
	The likelihood function for each iteration is obtained from the assumption of Gaussian system noise \cite{kalman_new_1960}. The likelihood is defined as a multivariate normal distribution, which s generically given the below where $\mu$ is the distribution's mean, $\sigma$ its standard deviation and $x$ the measurement.
	\begin{equation}
		\Lagr = \frac{1}{\sigma \sqrt{2 \pi}} \exp [- \frac{1}{2} \frac{x - \mu}{\sigma}]^2
	\end{equation}
	The below will refer to the Kalman likelihood as the cumulated likelihood across the entire data set with periods $T$. The Kalman likelihood used in the sampler is therefore expressed as the below.
	\begin{equation}
		\Lagr_T = \sum_{k=0}^{T} \left[ \quad \frac{1}{\matr{S}_k \sqrt{2 \pi}} \exp [- \frac{1}{2} \matr{y}_k^{T} \matr{S}_{k}^{-1} \matr{y}_k] \right]
	\end{equation}
		
	\subsection{Metropolis-Hastings Sampler}
	The aim of Bayesian estimation is the identification of the true parameter distribution, the posterior. However, mapping a DSGE model's parameters to its posterior is non-linear in the parameter vector $\Theta$. Therefore the posterior distribution cannot be evaluated analytically \cite{herbst_bayesian_2016}. Instead a numerical approximation mechanism is require, this is referred to as the sampler \cite{guerron-quintana_bayesian_2013}. The Metropolis-Hastings Monte Carlo Markov Chain (MH-MCMC) sampler is the predominant sampling method in linear Bayesian estimation literature \cite{guerron-quintana_bayesian_2013}. The MH-MCMC sampler is applicable to small scale New Keynesian models. It usually delivers good results for small models as their joint parameter distribution is well-behaved and elliptic \cite{herbst_bayesian_2016}. This reduces the posterior identification problem to a global problem. Larger models usually exhibit no-elliptic parameter distributions, requiring samplers which are able to distinguish local and global likelihood maxima \cite{herbst_bayesian_2016}. However, as this thesis is concerned with the canonical model the MH-MCMC sampler is sufficiently accurate.	
	The MH-MCMC sampler suggests new posterior candidates according to a multivariate random walk. The posterior is extend by a suggestion if it provides an improvement in the likelihood as described in \nameref{kalman_ll}. The algorithm generates a stable Markov chain, which is meant to exhibit low autocorrelation and low variance of the MH estimator. Under such conditions the resulting chain is equivalent to the posterior distribution \cite{herbst_bayesian_2016}. The below will outline this thesis implementation of the MH-MCMC sampler for which the code can be found in \nameref{appd}. \\
%and methods of evaluating its convergence to the true posterior distribution

	The algorithm proceeds in three main steps, first a candidate for the posterior $\matr{\hat{\Theta}}_{k|k-1}$ is suggested based on the random-walk law of motion. The random-walk departs from the last accepted posterior candidate and suggests a new candidate by departing from the $\matr{\Theta}_{k-1|k-1}$ according to the zero-mean Gaussian distribution $\epsilon$. The distribution's variance $\matr{\Sigma}$ can be set to either the identity matrix $\matr{I}$ or to contain the parameter prior variances on its diagonal. The latter is recommended if well-defined priors are at play \cite{herbst_bayesian_2016}.
	\begin{equation}
		\begin{aligned}
			\matr{\hat{\Theta}}_{k|k-1} = \matr{\Theta}_{k-1|k-1} + \eta \epsilon \\
			\epsilon = N(0, \matr{\Sigma})
		\end{aligned}
	\end{equation}
	Across the law of motion the \ac{MH-MCMC} sampler employs the gain parameter $\eta$ which essentially scales the distance of the new posterior candidate away from the last accepted candidate $\matr{\Theta}_{k-1|k-1}$. This value has been calibrated to be $\eta \in [0.2, 04.]$ \cite{gelman_weak_1997}. Generally the value of $\eta$ should correspond to an acceptance rate between 20\% and 40\% of suggested draws \cite{herbst_bayesian_2016}. This work puts $\eta$ at 0.25 which yields an acceptance rate of around 30\% for each model.
	A new posterior candidate $\matr{\hat{\Theta}}_{k|k-1}$ is once obtained evaluated through the Kalman likelihood. This yields the likelihood of the candidate given the data $\matr{Y}_T$.
	\[
		 \Lagr(Y| \matr{\hat{\Theta}}_{k|k-1})
	\]
	Once the likelihood is obtained the algorithm proceeds into the two step acceptance procedure \cite{guerron-quintana_bayesian_2013}. First the likelihood of candidate $\hat{\Theta}_{k|k-1}$ is compared to the most recently accepted posterior. According to Bayes' law this allows to determine whether this draw constitutes an improvement in likelihood.	
	\begin{equation}
		\omega_k = \min \quad \left\{ \frac{ \Lagr(Y|\matr{\hat{\Theta}}_{k|k-1}) P(\matr{\hat{\Theta}}_{k|k-1})}{\Lagr(Y| \matr{\Theta}_{k-1|k-1})  P(\matr{\Theta}_{k-1|k-1})}, 1 \right\}
	\end{equation}
	In a second step the likelihood ratio $\omega_k$ is randomly accepted if above uniform random variable $\phi \sim U(0, 1)$. If $\omega_k \leq \phi_k$ the draw is accepted and the candidate is assumed into the posterior. This process is repeated $N$ times generating the a posterior sample. 
% check number of sampler runs
	As an recursive method the MH-MCMC sampler requires an initial condition. This initial condition is usually obtained throughout a so-called burn-in period corresponding to 50\% of total iterations. Moreover, a number of at least 20,000 is recommended for the sampler \cite{herbst_bayesian_2016}.
%	The following will now describe the evaluation steps to insure convergence of the chain. Moreover, once the algorithm has run one needs to evaluate this performance. A method beyond visual inspection is suggested by (ref).
	The MH random walk is highly susceptible to miss-specification of the posterior of the prior distribution is far away from the true posterior \cite{herbst_bayesian_2016}.
			
	\subsection{Priors}
	In response to the Lucas critique \ac{DSGE} models were build around structural time invariant parameters \cite{lucas_jr_tobin_1981}. Consequently, parameters can be expressed as a joint distribution of preferences, which in its entirety is usually impossible to evaluate \cite{del_negro_forming_2008}. Instead parameters are analysed in building individual prior distributions reflecting independent and prior knowledge about them \cite{del_negro_forming_2008}. Here the notion of independence is important in tow ways. Firstly, only if the data used to construct the priors is independent of the likelihood mechanism the resulting posterior is unbiased \cite{del_negro_forming_2008}. This condition is fulfilled if the data for prior formation at least pre-dates the of the likelihood evaluation \cite{herbst_bayesian_2016}. Secondly, assuming independence between parameters is only reasonable within limits, as shocks do not necessarily occur independently \cite{herbst_bayesian_2016}. However, the latter only poses a problem for large multi-shock models. This work analyses the canonical singular shock model and is thus unconcerned by this second issues. The below will therefore focus on identifying priors based on independent information. In doing so the approach of Del Negro \& Schorfheide (2008), dividing priors into three groups for analysis, will be followed \cite{del_negro_forming_2008}. Graph summarising the prior to posterior transition can be found here [\ref{fig:rbcpp}, \ref{fig:nk5pp}, \ref{fig:nk6pp}]. Across the following sections each prior group will be briefly outlined and its posterior transition will be analysed. An overview of all priors distributions can be found in the appendix \ref{appb}. 
	
	\subsubsection{Steady state priors}
	The group of steady state priors can be easily identified from long-run relationship of model variables. These relationships, such as the labour or capital share are also sometimes referred to as the 'great ratios' \cite{kydland_time_1982}. For the models in question the steady state parameters are: 
	\begin{equation}
		\begin{aligned}
			\Theta_{ss}^{RBC} = \{ \alpha, \beta, \delta \} \\
			\Theta_{ss}^{NK} = \{\alpha, \beta, \theta \} \\
			\Theta_{ss}^{NKE} = \{\alpha_m, \alpha_n, \beta, \theta, \chi \} \\
		\end{aligned}
	\end{equation}
	The time discount factor $\beta$ is usually not calibrated, as it results directly from the inverse of the interest rate \cite{guerron-quintana_bayesian_2013}. The pre-1985 data used to estimate prior distributions puts $\beta$ to 0.995.	
	The capital share can be derived from the long-run share in industrialised nations. For smaller models using a uniform distribution covering the possible parameter interval $\alpha \in [0,1[$ is suggested \cite{del_negro_forming_2008}. In the RBC $\alpha$'s posterior converges from a uniform to a centred distribution \ref{fig:rbcpp}. While this is still rather broad and far off the empirical value of 33\% it is indication of convergence towards the true posterior \ref{fig:rbcpp}.
	For both New-Keynesian models a more targeted distribution has been used to ensure steady state existence \cite{guerron-quintana_bayesian_2013}. Consequently the values for $\alpha$ are more left skewed \ref{fig:nk5pp}. Interestingly enough the calibration of the NKE greatly overestimates the share of oil in production which by \cite{blanchard_macroeconomic_2007} is reckoned at below 5\% \ref{fig:nk6pp}. The labour share in production is more reasonable but also off the empirical estimate of 0.7 \cite{blanchard_macroeconomic_2007}.
	The depreciation rate $\delta$ while rather high in the posterior has yet assumed reasonable values in limiting its posterior to below 0.8 \ref{fig:rbcpp}. This indicates some convergence to the real prior, while still being far from empirically estimated values of 2.5\% per quarter \cite{campbell_inspecting_1994}.
	The price setting probability $\theta \in [0,1]$ has reasonable estimates in the NK model \ref{fig:nk5pp}. Its mean is around 0.8 which corresponds to an average price duration of roughly four month, close to the empirical estimate \cite{blanchard_macroeconomic_2007}. This is not the case for the NKE model where $\theta$ exceeds one, which as a probability is impossible. The NKE estimates are much more accurate for the share of petrol in consumption, whose posterior remains close to the empirical mean of 1.7\% \cite{blanchard_macroeconomic_2007}.

	\subsubsection{Exogenous prior}
	Exogenous priors contain the exogenous law of motion and standard deviation of shocks. In a simple model with singular shocks they can be identified from the first and second order moments of the economic variables affected by the shocks \cite{del_negro_forming_2008}. This work thus calibrated the shock standard deviation based on the variance of pre-1985 output cyclicality amounting to 0.0243. Moreover, in setting the autoregressive parameter this work draws from the distributions suggested by \cite{vasconez_what_nodate}. Looking across the posterior all estimates for $\rho$ are bounded at one. While this might appear to be a successful identification this is merely due to the stability requirement of the dynamic system. If $\rho \in \quad ]-1,1[ $ is violated the dynamic system is not stable and thus cannot be solved, leading the sampler to discard any value $|\rho| > 1$. The high variance of the posterior thus indicates problems of identification. However, its skewness towards higher values seems to confirm the prior assumption of relatively time persistent shocks across all models \ref{fig:nk5pp}
	\begin{equation}
		\begin{aligned}
			\Theta_{exog}^{RBC} = \{\rho_A \} \\
			\Theta_{exog}^{NK} = \{\rho_v \} \\
			\Theta_{exog}^{NKE} = \{\rho_s \}
		\end{aligned}
	\end{equation}

	\subsubsection{Endogenous priors}
	All remaining parameters make up the group of endogenous priors. These parameters cannot be measured directly through economic variables' long-term relationships. Instead they rely on micro-evidence from external data sets \cite{del_negro_forming_2008}. 
		\begin{equation}
		\begin{aligned}
			\Theta_{endog}^{RBC} = \{\sigma_C, \sigma_L, \} \\
			\Theta_{endog}^{NK} = \{\sigma_C, \sigma_L, \epsilon, \phi_{y}, \phi_{\pi} \} \\
			\Theta_{endog}^{NKE} = \{\epsilon, \phi_{y}\}
		\end{aligned}
	\end{equation}
		
	The elasticity of substitution $\sigma_C$ and $\sigma_L$ belong in this category. In setting their prior distribution this work relied on the recommendation from \cite{del_negro_forming_2008}. For the RBC the posterior values demonstrate large spreads indicating that the MH sampler did not identify the posterior correctly \ref{fig:rbcpp}. Even larger dispersion can be observed for the NK model \ref{fig:nk5pp} \footnote{ In the \ac{NKE} model the labour and consumption elasticities do not enter into the log-linear model specification due to the log-utility function}. This occurs frequently as the consumption and labour elasticity are parameters with low influence on the transition dynamics \cite{guerron-quintana_bayesian_2013}. Consequently they are dominated by stronger parameters in the likelihood function and the posterior identification fails for these variables.
	The elasticity of substitution $\epsilon$ originates from the Dixit-Stiglitz aggregator and factors into the steady-state mark-up of firms. As such it is one of the driving parameters behind inflation and strong in its influence on the transition dynamics. In setting the prior distribution one can rely on information on the steady-state mark-up $mc^r = \frac{\epsilon}{1-\epsilon}$. Following this a value of $\epsilon$ slightly above 0.5 appears realistic. Consulting the posterior both put the mean around 0.5 \ref{fig:nk5pp} \ref{fig:nk6pp}. While the standard deviation is still large the posterior specification does appear to be within reasonable bounds. 
	
	The central bank's preference parameters $\phi_y$ and $\phi_{\pi}$ also appear within reasonable bounds. As suggested by \cite{gali_monetary_2008} the central bank has a stronger response to inflation that to to the output gap. This has been incorporated into priors and the resulting posterior seems to confirm this assumption in the NK model \ref{fig:nk5pp}. The NKE model relies on output and not the output gap as intertemporal instrument, wherefore $\phi_{\pi}$ appears. However, the posterior estimate appear to confirm the same reasoning \ref{fig:nk6pp}.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.5\textheight]{mod4_rbc_vanilla_posterior_hist.png}
			\caption{RBC model prior posterior comparison.}\label{fig:rbcpp}
			\includegraphics[width=.5\textheight]{mod5_nk_vanilla_lin2_posterior_hist.png}
			\caption{New Keynesian model prior posterior comparison.}\label{fig:nk5pp}
			\includegraphics[width=.5\textheight]{mod6_nk_energy_lin2_posterior_hist.png}
			\caption{New Keynesian Energy model prior posterior comparison.}\label{fig:nk6pp}
		\end{center}
	\end{figure}
	\pagebreak
		
	\section{Forecast evaluation} \label{forecast_eval}
	Having identified the parameter posterior distribution this section use the Kalman forecasting step as outlined in \nameref{kalman_forecast_step} to obtain three quarter ahead forecast for each model. In doing so the mean posterior will be used across parameter specification, as it corresponds to the most likely model.
	The individual model forecast will be analysed in the first three section. A fourth will introduce a \ac{BVAR} model. A last section will establish a comparison between DSGE models and the BVAR.
	In order to accommodate a first period of convergence for the Kalman filter the following will be analysing data after 2003. Moreover, the variables revealed as observables to the filter vary across models in order to reflect the differences in model specification such as investment expenditure, inflation and petrol prices. 
	
	\subsection{Real Business Cycle model}	
	The Kalman filter closely traces output and investment, the real data lies within the 95\% confidence interval for all post 2010 observations. This is also true for the recession in early 2020, indicating a good fit of the model to real data.
	Consumption, while not contained as an observable in the filter, also is relatively closely traced with similar accuracy of the confidence intervals. This points to even better model accuracy, as the dynamic system is able to replicate a non-included variable to a good extend. 	
	The interest rate is less closely traced and predicted to be much more volatile the case in reality. While this indicates problems in the model specification it is important to note that the US Federal Reserve (FED) pursued a notorious low interest rate policy for much of the sample period. As such interest rates were not dictated by changes in capital demand as assumed in the RBC model. It is thus not surprising that the data is not well fit.
	Labour is the only variable continuously outside of the 95\% confidence interval. Moreover, the model even suggests a rise in labour hours during crisis. While in the case of Covid-19 one could argue that peculiar dynamics where at play the 2008 crisis points out that recessionary unemployment is not captured at all. Generally, pre 2010 dynamics are captured less well by the model. Output and consumption data escapes the filter's 95\% confidence interval before and after the 2008 financial crisis. The good fit to more recent data might be circumstantial and related to the expanded period of growth and relative economic stability.
			
	The RBC forecasts seem reasonable in direction and as well as confidence. The 95\% confidence intervals naturally increase with distance to the point of prediction. However, especially at the one period ahead forecast they exhibit reasonable bounds. Moreover, the point forecasts, with exception of labour, closely trace real variables with no systematic bias. Labour as explained in the above appears to be incorrectly specified in the models functional form, yet not impeding the model in its forecasting of other variables. 
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.5\textheight]{mod4_rbc_vanilla_y_I_kalman_filter_00.png}
			\caption{NK model Kalman Filter with variables c, n}\label{fig:rbc_kfil}
			
			\includegraphics[width=.5\textheight]{mod4_rbc_vanilla_y_I_kalman_forecast.png}
			\caption{NK model Kalman Filter with variables y}\label{fig:rbc_kfor}
		\end{center}
	\end{figure}
	
	\subsubsection{New-Keynesian model}
	At first glance it is evident that the New-Keynesian filter traces data less accurately. As with the RBC output is closely traced after 2010 while confidence intervals exclude actual data around the 2008 crisis. However, other than with the RBC filter the NK filter does not manage to track the 2020 crisis as closely, even deems outside its 95\% confidence interval.
	A similar dynamic can be observed with consumption. Again filter fit is much better after 2010 and does not capture the 2020 crisis. Consumption essentially remodels output data, due to the absence of investment the NK model assumes consumption and output to be equal in equilibrium. Thus the consumption time series is less of a performance indicator as in the RBC model.
	Labour on the other hand is more closely traced and differently from the RBC no longer exhibits counter intuitive movements during crisis, modelling recessionary unemployment correctly. 
	The main contribution of the New-Keynesian literature is the inclusion of inflation. Looking at the actual inflationary dynamics and the filtered inflation series the large divergence is obvious. The model puts inflation at much higher rates than actually the case in the economy. The same goes for the real interest rate, which is not surprising due to the close tie between the two $r_t = i_t + \E[][\pi_{t+1}]$. Moreover, the confidence intervals are rather broad on both inflation and interest rate. This while no sign of good fit at leasts shows that the model is uncertain about the actual behaviour of inflation and does not deem it erroneously close to its filter result.
	
	While the filtered data provided some good results the forecasts of the NK model are far from informative. Output and consumption are not contained in the forecasts' 95\% interval. Moreover, the broad confidence intervals on inflation and interest rate conceal the fact that the inflation forecasts actually moves opposite to real data. Only labour is proxied traced to reasonable extend by the confidence interval, while the point forecast is systematically off.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.5\textheight]{mod5_nk_vanilla_lin2_y_kalman_filter.png}
			\caption{NK model Kalman Filter with variables c, n}\label{fig:nk_kfil}
			\includegraphics[width=.5\textheight]{mod5_nk_vanilla_lin2_y_kalman_forecast.png}
			\caption{NK model Kalman Filter with variables y}\label{fig:nk_kfor}
		\end{center}
	\end{figure}
	
	
	\subsubsection{New-Keynesian energy model}
	The filtered data for the New-Keynesian Energy model are the furthest off real data. The dynamics of output and labour are poorly tracked while spanned by erroneously narrow confidence intervals. Including inflation in the filter does not improve this inability to track output or consumption. Neither is the real interest rate, even-though closely tied to inflation, traced by the confidence interval or point forecast.
	The reason for including petrol in the model specification has been to supplement the analysis of inflation by an important factor. Including the petrol price in the filter does not provide the theorised improvement. Instead confidence intervals narrow across all variables with no improvement in point forecasts. 
	
	The poor filter fit is obviously reflected across the forecasts as well. While confidence bands are narrow the forecast often points into opposite direction, excluding the real data for consumption, inflation and the real interest rate. Output and labour are in no way traced reasonably.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.5\textheight]{mod6_nk_energy_lin2_y_pi_s_c_kalman_filter.png}
			\caption{NK model Kalman Filter with variables c, n}\label{fig:nke_fil}
			
			\includegraphics[width=.5\textheight]{mod6_nk_energy_lin2_y_pi_s_kalman_forecast.png}
			\caption{NK model Kalman Filter with variables y}\label{fig:nke_for}
			
			\includegraphics[width=.5\textheight]{mod6_nk_energy_lin2_y_pi_s_s_kalman_filter.png}
			\caption{NK model Kalman Filter with variables y}\label{fig:nke_fil_s}
		\end{center}
	\end{figure}

	\subsubsection{BVAR}
	Using a Vector Autoregression as reference model for comparing DSGE model forecasts has been commonly done across literature \cite{schorfheide_loss_2000, chin_bayesian_2019}. 
	Naturally the BVAR does not provide confidence intervals like frequentist methods of forecasting. Instead model parameters are defined through a sampler, which in this case ran in four iterations. Sampling from these iterations yields the posterior coefficient distribution. Using several draws from the posterior then allows to derive information about the range of possible outcomes. This range can then be interpreted as a confidence interval (ref).  
	A sample from the posterior is illustrated in blue \ref{fig:bvar}, while the mean of all samples is marked in red. The range of outcomes for output suggest bias in the forecast. They are note evenly distributed around the actual data series. Instead the range of outcomes underestimates output and also introduces additional volatility not matched by the actual observations. This is also the case for all other variables, additional volatility or classicality is introduced systematically across all forecasts. The forecast for the real interest rate after over correcting even moves in the opposite direction. 
	The BVAR thus represents on first account not better forecasts than for example the RBC model.
%	This work uses a frequentist VAR to determine the optimal number of lags, which are found to be 4. This corresponds to the choice of four lags in \cite{chin_bayesian_2019}
%	Including priors into bayesian VAR makes the model better at forecasting \cite{chin_bayesian_2019}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.4\textheight]{bvar_forecast.png}
			\caption{NK model Kalman Filter with variables y}\label{fig:bvar}
		\end{center}
	\end{figure}
	
	\subsubsection{Forecast comparison}
	Comparing several models across their forecasting performance can be done in two ways, a frequentist and Bayesian approach.
	The frequentist approach compares models based on a mean squared error (MSE) metric and then tests for systematic differences. Formal testing is provided by the Diebold-Mariano test, testing for the null hypothesis of structural differences in MSE. The methodology is recommended for on step ahead forecasts and needs a large number of observations to construct the test statistics. It is thus not recommended for quarterly data \cite{chin_bayesian_2019}.
	Instead Bayesian model averaging approach compares models in their log-likelihood based on an information criterion. For the purposes of this work the information criterion will be de widely applicable information criterion (WAIC) as suggested in \cite{chin_bayesian_2019}. In Bayesian model averaging the weights of each model is calculated from its posterior model probability \cite{chin_bayesian_2019}. This is traditionally done for structurally similar models in order to discriminate between different priors. However this procedure has also been employed in comparing different model classes, then usually relying on the WAIC \cite{chin_bayesian_2019}.
	If comparing models according to the WAIC the RBC forecasts appears to work best. This is not surprising with regards to the visual inspection of the forecasting performance. However, given the extension of NK models by inflation the opposite could be expected.
	
	\section{Conclusion}
	\subsubsection{Conclusion on MH MCMC}
	The MH MCMC sampler used across this work is one of the simpler likelihood sampling algorithms. In more comprehensive DSGE models it fails to converge, but for the small scale models analysed in this work it is still deemed appropriate (ref). However, as the above illustrates some identification problems especially regarding 'weaker' parameters have become apparent. Despite providing reasonable estimates for most parameters these should also be handled with care. 
	
	Possible explanations for the deterioration of model fit across the NKE could lie in its specification. The model was originally build around domestic output which combined with petrol consumption generates overall consumption. Overall output was then derived off nominal output by including a GDP deflator. This extension was not necessary for closing the model but a way of including the correct output specification to make the model comparable to data. The NKE at its core does thus not describe output as it is specified in the data, wherefore it is no surprise that it does badly in tracing the real data. In that the model does deliver further insight in possible links between petrol and inflation. However, it does so in a way that cannot be verified with observable data.
	
% Bibliography
	\bibliography{20230505_m1_dsge}
	\addcontentsline{toc}{section}{References}

% Appendix
	\section*{Appendix}
	
	\pagebreak
	\subsection*{Appendix A} \label{appa}

%BVAR
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.4\textheight]{BVAR_coeff.png}
			\caption{NK model Kalman Filter with variables y}\label{fig:bvar_coef}
		\end{center}
	\end{figure}

	\pagebreak
	\subsection*{Appendix B} \label{appb}
	% fed data
	
%	\begin{center}
%		\csvautotabular{{./graphs/priors_table.csv}}
%	\end{center}

	
%% tables
%	\small \input{{./graphs/priors_table.tex}}
%	\input{{./graphs/fred_variables.tex}}
%	\input{{./graphs/data_descriptives.tex}}
	
	\pagebreak
	\subsection*{Appendix C} \label{appc}
	\subsubsection*{Log-linearisation}
	
	Euler equations
	
	Expression for $N_t$ from $N_{it}$
	It can be shown that $ log \left( \int_{0}^{1} \left( \frac{P_{it}}{P_t} \right)^{-\frac{\epsilon}{1 - \alpha}} di \right) \approx 0$ which then allows to log linearise the production function to $y_t = a_t + (1 - \alpha) n_t$.
	
	
	RBC all equations\\
	\begin{tabular}{lr}
		\textbf{Equation name} & \textbf{Log-linear expression}\\
		\hline
		Euler equation & $\E_t \left[ \frac{C_{t+1}}{C_t} \right]^\sigma = \beta \left[ (1 - \delta)  + \E_t [R_{t+1}] \right]$ \\
		Capital supply & $K_t = (1 - delta) K_{t-1} + I_t$ \\
		Capital demand & $K_t = \alpha A_t K_t^{\alpha -1} N_t^{1-\alpha}$ \\
		Labour supply & $- \frac{U_{n,t}}{U_{c, t}} = W_t$ \\
		Labour demand & $W_t = (1 - \alpha) A_t K_t^\alpha N_t^{-\alpha}$ \\
		Production function & $Y_t = A_t K_t^\alpha N_t^{1 - \alpha}$ \\
		Equilibrium condition & $Y_t = C_t + I_t$ \\
		Technology law of motion & $\log(A_t) = (1- \rho_A) \log(A^*) \rho_A \log(A_{t-1}) + \epsilon_t^A$ \\
	\end{tabular}\\

	\include{{./graphs/rbc_steady_state.tex}}
	\include{{./graphs/rbc_transition_matrix.tex}}
	
	\pagebreak
	\subsection*{Appendix D} \label{appd}
	Metropolis Hastings MCMC sampler code\\
	Deriving the multivariate covariance matrix $G$ from prior the prior distributions standard deviation
	\lstinputlisting[language=Python, firstline=33, lastline=44]{mh_mcmc_code.py}
	Suggesting new priors based on the random walk law of motion
	\lstinputlisting[language=Python, firstline=85, lastline=99]{mh_mcmc_code.py}
	MH MCMC acceptance procedure
	\lstinputlisting[language=Python, firstline=175, lastline=195]{mh_mcmc_code.py}
	
	\pagebreak
	\subsection{Appendix E} \label{appe}
	\verbfilenobox[\tiny]{{./graphs/mod4_rbc_vanilla.txt}}
	\verbfilenobox[\tiny]{{./graphs/mod5_nk_vanilla_lin2.txt}}
	\verbfilenobox[\tiny]{{./graphs/mod6_nk_energy_lin2.txt}}
	Test

	

	
\end{document}