\documentclass[11pt,a4paper,english]{article} % document type and language

\usepackage{babel}   % multi-language support
\usepackage{float}   % floats
\usepackage{url}     % urls
\usepackage{graphicx}
\graphicspath{{./graphs/}}	% graphics
\usepackage{multirow} % tables
\usepackage{booktabs} % tables

\bibliographystyle{alpha}
\addcontentsline{toc}{chapter}{Bibliography}

\author{Lukas Gahl}
\title{\textbf{\huge Memoire }\\}
\date{} 


\begin{document}
	
	\maketitle
	\pagebreak	
	
	\section{Introduction}
	
	This work is joining 3 strains of literature
	- Literature on the RBC, the origin of all new keynesian models
	- Literature on bayesian estimation of structural macroeconomics models
	- Literature on energy price shocks and their role in the 
		- This refers to the nature of the shock
		- Shocks as theoretical origin on business cycles are at the origine of this research, 
		identifying suitable shocks that explain more thus adds to the literature on business cycles
	
	The Lucas 1976 critique: 
	
	Intertamperol evoluation of macreoconomic variables
	
	Kydland and Prescott (1982) introduced the RBC \\
	- became main model of macroeconomy
	- rational expectation model
	
	The New Keynesian model extends the RBC by inflation
	- 
	
	Rational expectation models build on two corner stones Chair et al
	- Structural parameters which are unaffected by policy changes
	- shocks that have a relevant economic interpretation
	
	Kocherlakota (2007) adjustment principle
	- the better a model fits to data the better it is for policy advise
	- in assessing this fit one needs to be wary of over fitting specific data
	

	
	
	\section{RBC model}
	The theoretical benchmark model a classic RBC 
	The aim of analysing business cycles
	
	The micro foundation, instead of clear behavioural rules agents optimise their behaviour according to utility optimisation
	
	Conceptual ideas:
	The efficiency of business cycles (Gali, 2008)
	in a world of perfect competition and lack of nominal rigidities business cycles might be efficient
	they are the response to changes in productivity, and are actually the result of a "correcting" force towards an efficient equilibrium
	this raises questions on the desirability of policy interventions
	This goes against (Keynes, 1936) who regarded recession as inefficient, due to under-utilisation of ressources
	
	Technology shocks
	Technological shocks for a correctly calibrated model allowed to simulate cycles similar to actual business cycles in first and second order moment (ref). This shed new light on the assumption that technology was solely a driver of long-term growth with neglible impact on business cycles (Gali, 2008, p.3).
	
	RBCs succesfully abstraced from monetary rigidities in their explanation of business cycles
	
	The assumptions of the RBC and their consequences, namely the non-existance of money and monetary rigidities in the economy are greatly contratsted by emperical evidence (Gali, 2008, p 15)
	
		
	
	Assumptions
	- perfect compitions
	- flexible prices
	- technology shocks
	- infinetly lived households and firms
	- identical technology across firms
	
	Agents
	- representative households, where the sum of all households is normed to one
	- Firms, the representative firm normed to unity
	
	Allocation decisions
	HH
	- intertamperol consumption and leasure choice
	- intertemperol consumption savings 
	
	Firm
	- static optimisation of profit
	
	
	the model
	-	Non-linearity arises from multiplicative Cobb-Douglas production and additive law of motion of e.g. capital (Campbell)
	- This raises the need for linear approximations, overview of possible methods:
	- Models become more complex as researchers are trying to use more realistic functions of utility and incorporate heterogeneity (Taylor  Uhlig, 1990)
	- The method of solving and approximating the model has significant impact on simulated data, hence is relevant when relating models to real data (Taylor  Uhlig, 1990)
	
	
	Methods of linearisation
	-	This work builds on models that can be solved analytically
	- All models are solved analytically and then log-linearised with 1st order Taylor approximation following (Cambpell)
	- Based on the log-linearisation the predictions are performed
	- Log-linear quadratic approximation Rebello (1987)
	- Under a deterministic singular solution it solves correctly (Cambell)
	This work follows Campbell and uses the capital stock as the state variable
	As there is no multiple steady states, this work solves the models anlytically
	
	
	
	Criticism
	the correlation of real wage and hours worked does not correspond to reality, this is the lacmus test of RBC models (Christiano Eichenbaum 1992)
	"Robert E. Lucas (1981 p. 226) says that "observed real wages are not constant over the cycle, but neither do they exhibit consistent pro- or countercyclical tendencies. This suggests that any attempt to assign systematic real wage movements a central role in an explanation of business cycles is doomed to failure" \cite{christiano_current_1992}
	
	
	\section{NK}
	
	Additional NK assumptions (Gali)
	- monopolistic competition, inputs are set by private agents according to their own optimisation problem
	- nominal rigidity, price setting is limited in frequency
		- this results in short-run neutrality of monetary policy where changes in interest are note directly matched by changes in expected inflation
		- this is the source of short-run fluctuations
		- However, in the long-run prices adjust 
	- This causes the response of the economy to be inefficient, unlike RBC where cycles are result of efficient adjustments
	
	NK models are suitable to comparing alternative policy regimes without being subject to the Lucas (1976) critique (Gali)
	
	Small scale new keynesian model solved with first order log-approximation \cite{herbst_bayesian_2014}
	
	Define stability condition of the linear state-space, Blanchard-Kahn
	
	
	\section{Petrol}
	High correlation of hours worked and real wage in the RBC model, an fact that is not matched by reality. The aim of RBC research thus must be finding new kinds of shocks, that allow for a more realistic representation of labour supply ()Christiano Eichenbaum 1991).
	
	
	One such 'new' shock is the the inclusion of petrol as an exogenous shock series \cite{kim_role_1992}
	
	
	
	
	\part{Bringing the model to data}
	
	\section{The Data}
	Data sources
	References for data sources
	
	\section{Preprocessing}
	per capita
	log
	HP filter
	- what is natural GDP
	- what is cyclicity
	- why does it yield log-deviations
	
	
	\section{Descriptives}
	Descriptives
	Plot data
	
	\section{Analysis of models}
	Compare model covariance matrices, to actual covariance in data
	
	Show theoretical impulse response functions, differences between NK and RBC
	
	\section{Estimation methods}
		
	As explained in the above the RBC and later NK models are based on structural parameters of the economy. As such they are not subject to the Lucas 1976 critique as their parameters do not  directly depend on the choices of agents (ref). Instead, they represent the deep-rooted dynamics of the dynamic system describing the economy, as such they should exhibit a certain stability over time (ref). \\
	
	This perspective makes an implicit but important assumption: DSGE model corresponds to the real dynamic system describing the economy. A far reaching assumption which has been considered at least partially validated based on the ability of early DSGE models to generate data matching 1st and 2nd order moments of real variables such as output, consumption and investment (ref). 
	In an attempt to tweak this capability  early attempts to parameter to identifying the true parameters of the economy were made. In doing so academics first relied on what \cite{prescott_theory_1986} referred to as great ratios of the economy. Calculating for example the real interest rate allowed to derive the discount factor $\beta$. 
	Other parameters had fewer empirical counterfactuals and where thus not estimated by manually calibrated to sensible values. These inaccuracy gave rise to approaches more deeply rooted in econometric analysis. They were undertaken in the aim of reliably estimating the economy's true structural parameters.\\
	
	Such early attempts of econometric identification relied on standard frequentists statistics such as the maximum-likelihood estimators (MLE) (ref). Another approach pioneered by Blanchard \& Kahn (ref) referred to as simulation method of moments, later progressed into the general method of moments (GMM) attempted to minimize the distance between simulated and real data moments. The GMM allowed to discriminate between several competing specification of the same structural model \cite{christiano_current_1992}. Comparing the estimated parameters of the model with real data values then served as inspection of model fit to data. However, as models grew in size a major short-coming of GMM became apparent. The method of moment conditions were no longer sufficient to estimate model fit across the variety of parameters \cite{guerron-quintana_bayesian_2013}. Likewise, the ML estimator turned out to be limited in its ability to estimated the entirety of parameters, requiring manual calibration of some parameters \cite{guerron-quintana_bayesian_2013}.\\
	
	While frequentists statistics knew to overcome the identification problem in introducing theoretical moments Smith (1993) it was soon challenged by a different perspective. At the heart of all frequentist statistics lies the assumption that a given model corresponds to the true structural process, it is the 'true' model (ref). If this assumption cannot be met all estimation will be biased and thus need to be corrected for. Again remedies where found (ref) Smith (1993) or Dridi, Guay, and Renault (2007) in varying parameters most relevant to overall model moments and keeping less relevant ones constant throughout econometric estimation. Yet it is argued that this approach at least partly failed in overcoming the conditional bias introduced by non-relevant parameters \cite{guerron-quintana_bayesian_2013}.\\
	
% explain Baye's law ?
	Consequently, a new philosophy of linking models to data was identified. Bayesian statistics replaces the assumption of a single true model by the convenient formulation of conditional likelihood making it more appealing to the purposes of DSGE modelling \cite{guerron-quintana_bayesian_2013}. In doing so the Bayesian statistics relies on Bayes law linking the conditional likelihood of an event to the prior distribution.
		
	\[
	P( \Theta | Y_{T}) = L(Y_{T} | \Theta) P(\Theta)
	\]
		
	A given model specification can be linked to a likelihood evaluated against the backdrop of data the system is meant to generate (ref). This allows to assess a model's suitability. The frequentist's idea of a single true parameter is replaced by distributions assigning different probabilities to parameter values. These prior distribution reflect previous knowledge allowing to narrow down the possible space of parameters. Instead of identifying a true parameter Bayesian statistics, aims at evaluating these believes and at decreasing uncertainty around the parameter space i.e. the variance of the distribution. This process translates the prior distribution into a posterior distribution, not necessarily belonging to the same class. 
	This process of evaluation is referred to as sampling and builds on numerical and stochastic calculus to approximate the true posterior distribution (ref). Furthermore, Bayesian statistics naturally provides great forecasting properties. Once uncertainty around the space of the real system has been reduced it allows for make informed forecasts, naturally providing confidence bans. \\
	
	The following section will therefore be concerned with these four concepts, conditional likelihood, prior and posterior distribution, sampling and forecasting. DSGE literature has developed a rich body of literature covering the case of the canonical DSGE model rather well (ref). The state of research is currently concerned with the estimation of larger models and non-linear and not normally distributed posterior distributions (ref). However, given the simplicity of the above discussed models current techniques are more than sufficient for this thesis purposes.
	
	
	\subsubsection{The log-likelihood}
	The Kalman filter serves as means of evaluating the log-likelihood of a linear state space system. The filter is a good remedy, as for a given system the Kalman Filter is the optimal forecast (ref). This of course is conditional on the believe that the underlying system is normally distributed.
	- Linear Gaussian state space system for the Kalman filter, this requires shocks to be normally distributed \cite{herbst_bayesian_2014}
	- For the non-normal case a Kalman-Particle filter is used, which however is not discussed here
	
	\[
	L = \frac{1}{\sqrt{2 \pi S}} \exp [- \frac{1}{2} y^T S^{-1} y]
	\]

	
	
	\subsubsection{Sampler}
	
	There are local and global identification problems \cite{herbst_bayesian_2014}
	
	Mapping a DSGE model to its posterior is non-linear in the parameter vector $\Theta$. Therefore the posterior distribution cannot be evaluated analytically \cite{herbst_bayesian_2014}. Instead a numerical approximation mechanism is require, this is referred to as the sampler \cite{guerron-quintana_bayesian_2013}.
	
	Metropolis Hastings belongs to the Markov Chain Monte Carlo (MCMC) group of alogrithms. 
	It generates a stable Markov chain, which is equivalent to the posterior distribution \cite{herbst_bayesian_2014}
	\\
	
	
	Metropolis Hastings sampling algorithm
	- a multivariate random walk to update the posterior distribution
	- the posterior then is updated based on improvements in the overall model likelihood
	- it is important to note that a draw from the shocks prior, factors into the drawing process
	- procedure also followed by, with 300,000 draws of which 20,000 are considered burn in \cite{chin_bayesian_2019}
	
	MH-MCMC sampler
	- predominant estimation method in linear Bayesian estimation literature \cite{guerron-quintana_bayesian_2013}\\
	- the MH sampler is applicable to small scale New Keynesian models and usually delivers good results on this scale. For larger models a different kind of sampling is required \cite{herbst_bayesian_2014}. As this thesis is concerned with such small scale model it will follow the Metropolis Hastings sampling procedure.
	
	
	The algorithm proceeds as follows \cite{herbst_bayesian_2014}\\
	
	Posterior $\Theta$, prior $p$, suggestions $\theta$
	
	\begin{enumerate}
		\item A new draw $\zeta$ is suggested based on a random draw from the prior distribution of parameters $\Theta$. In order to determine whether this draw is admissible to the posterior $\Gamma_{1:i}$ the following steps are followed.
		\[
			\zeta = \Theta_{i-1} + \eta
		\]
		\[
			\eta \sim N(0, c \hat{\Sigma})
		\]
		
		\cite{herbst_bayesian_2014} recommends to set $\Sigma$ either to the identity matrix or as a diagonal matrix with the posterior variances on its diagonal.
		
		The scaling factor $c$ is under the assumption of Gaussian normal joint posterior distribution usually put between .2 and .4
		
		\item The new draw $\zeta$ is evaluated in likelihood using the DSGE model's likelihood function given the data $Y$, yielding $p(Y | \zeta)$. 
				
		\item In a two stage procedure the algorithm then either accepts or rejects the new draw $\zeta$ as part of the Markov chain of posterior distributions. 
		
		\item First the posterior likelihood of $\zeta$ is calculated based on Bayes theorem. It is then compared to the the posterior likelihood of the last accepted draw, that is now part of the posterior $\Gamma_{i-1}$. 
		\[
			\omega_i = \min \{\frac{L(Y| \zeta, \Gamma) * P(\zeta)}{L(Y| \Gamma_{i-1}, \Gamma) P(\Gamma_{i-1})}, 1 \}
		\]

		\item The second step of the decision algorithm draws from a uniform random variable $\psi \sim U(0, 1)$. The new draw $\zeta$ is accepted if its likelihood ration $\omega_i$ is accepted if $\omega_i \geq \phi$. This step is equivalent to moving to a higher point on the likelihood surface \cite{herbst_bayesian_2014}
		
		\item If accepted $\zeta$ will become $\Gamma_i$
		
		\item The algorithm is then repeated
		
	\end{enumerate}

	How many runs\\
	

	As illustrated in the above MH-MCMC is a recursive method and as such requires an initial value for the posterior $\Gamma_0$. This values is usually obtained throughout a so-called burn in period corresponding to 10 \% of total iterations (ref). The total number of iterations depends on the 
	
	Moreover, once the algorithm has run one needs to evaluate this performance. A method beyond visual inspection is suggested by (ref).\\
	
	
	The MH random walk is highly suceptible to mis specification of the posterior of the prior distribution is far away from the true posterior \cite{herbst_bayesian_2014}
	
	
	
	
	
	
	
	
	
	
	
	
	
	\subsection{Priors}
	
	The assumption for prior selection is that priors are obtained from some past knowledge. If this knowledge is independent of the data used to evluate the liklihood function than the prior holds. This is fullfiled if the data for prior building at least predates the data for likelihood evluation \cite{herbst_bayesian_2014}.
	
	
	Priors are statistical distributions which either reflect believes about the strucutral mechanism or are derived from data, external to the estimation process \cite{del_negro_forming_2008}
	- Priors thus refelect microeconomic evidence, for example regarding labour supply and elasticity
	- However, they are a simplication of the real joint likelihood function of the strucutural model. Such function is difficult and sometimes impossible to evaluate
	
	Priors \cite{del_negro_forming_2008}
	- Priors are usually assumed to be independent for simplicities sake
	- Priors are often calibrated for the model to match the covariance of real data
		
	\cite{del_negro_forming_2008} suggest two kinds of data sources. Pre-sample data, which is not included in the model evaluation process or in-sample data, that is explicitly excluded from the model evaluating likelihood function.
	As the model parameters are thought of as structural, hence relatively time-invariant, more dated data can be used to estimate them. 
	
	Priors are divided into three groups, following \cite{del_negro_forming_2008}
	
	\subsubsection{Steady state priors}
	The 'great ratios' or long-run relationships as \cite{kydland_time_1982} referred to them pin down the steady state
	\[
		\theta_{ss} = \{\alpha, \beta, \delta, \pi^*, \eta_p\} % eta is calvo
	\]
	When determining the steady-state priors $\theta_{ss}$ and assuming independence it is possible that the resulting joint prior distributions assigns high probability to unrealistic steady state values \cite{del_negro_forming_2008}. To circumvent this issue other method
	
	This priors are usually estimated from pre-sample data \cite{herbst_bayesian_2014}
	this work follows this approach and thus 
	
	
	
	
	\subsubsection{Exogenous priors}
	The exogenenous propagation paraemeters 
	these are the exogenenous shock parameters, their autoregressive coefficient and their standard deviation
	\[
		\theta_{exog} = \{\rho_a, \rho_s, \sigma_a, \sigma_s \}
	\]
	
	Exogenous priors can be evaluated from the model specification in keeping the influence of other priors minimal. Del Negro suggests, to set all other parameters to zero where possible. The system of equations then reduces to variables that are depend on the exogenous shocks. This reduced system can then be used to evaluate the likelihood of prior values, using out-of sample data of the later calibration. 
	An evaluation based on the likelihood of the reduced system is not necessary if the variation in observable variables can be directly retraced to individual shocks. In a simple model where shocks do not intersect in their influence on observable variables, e.g. in the case of a singular technology shock the variance of the shock can be directly inferred from the observable variable. In case of intersecting multiple shocks the likelihood of the reduced system needs to be consulted to choose appropriate priors, as their influence on observables cannot be directly inferred. 
	
	In case of exogenous observable shocks, such as energy prices, the above procedure is not required. Other than the level of technology energy price shocks are measurable, wherefore one can rely on the observable first and second order moments (ref).
	
	
	\subsubsection{Endogenous priors}
	The endogenous propagation parameters, including the shock autoregressive coefficients are parameters involved with the propagation of shocks through the system.
	These parameters heavily rely on micro-evidence from external data sets \cite{del_negro_forming_2008}
	All remaining parameters are stacked into the the following
	\[
		\theta_{endog} = \{\sigma_C, \sigma_L, \}
	\]\}
	Priors of the remaining variables are chosen in order to reflect believes or micro evidence. This work draws from the choice of \cite{del_negro_forming_2008} in their endogenous priors, given the similarity in models
	
	
	
	
	
	
		
	Priors are chosen in order to match first and second order moments individually as well as in a joint distribution across all parameters \cite{del_negro_forming_2008}
	
	\input{{./graphs/priors_table.tex}}

	
	
%	"Also, rather than imposing priors on the great ratios, C∗/Y ∗, I∗/K∗, K∗/Y ∗, and G∗/Y ∗, we fix the capital share, α, the depreciation rate, δ, and the share of government expenditure, g∗. This follows well established practices that pre-date Bayesian estimation of NKDSGE models." \cite{guerron-quintana_bayesian_2013}
	
	
	
	\subsubsection{Reference Model BVar}
	\cite{schorfheide_loss_2000}
	
	BVar with normally indpendently distributed lag parameters \cite{chin_bayesian_2019}
	
	This work uses a frequentist VAR to determine the optimal number of lags, which are found to be 4. This corresponds to the choice of four lags in \cite{chin_bayesian_2019}
	
	
	
	
	
	
	
	\subsection{Forecasts}
	Relevant to policy recommendations
	How well can models capture the dynamics of the economy, e.g. as a spectrum of possible outcomes
	
	Including priors into bayesian VAR makes the model better at forecasting \cite{chin_bayesian_2019}
	
	
	To compare models this work follows a bayesian averaging approach as in \cite{chin_bayesian_2019}
	- (Stock Watson 2003) argue that a simple average of models provides the best forecast
	- ()Jacobson and Karlsson (2004) and Wright (2009)) argue in favour of BMA
	
	
	BMA
	- the weights of each model are calculated from its posterior model probability \cite{chin_bayesian_2019}
	- This is traditionally done for structurally similar models in order to discriminate between different priors
	- the posterior probability is thereby obtained from \cite{chin_bayesian_2019}
	\[
		p(M_k | y) = \frac{p(y | M_k) p(M_k)}{\sum_{K}^{k=1} py(y | M_k) p(M_k)}
	\]
	\[
		p(y | M_k) = \int p( y| \theta_k, M_k) p(\theta_k | M_k) d \theta_k
	\]
	where $p(M_k)$ is the prior density, and $p(y | M_k)$ is the model's marginal likelihood
	
	Anderson (2008) suggest to use the predictive likelihood instead of the marginal model likelihood
	
	"Simply speaking, our Bayesian combination forecast (point forecast) is a weighted average of the K individual point forecasts, wherein the weights are determined by their predictive likelihoods" \cite{chin_bayesian_2019}
	
	Comparing forecasting performance
	Diebold-Mariano test for point forecast comparison following \cite{chin_bayesian_2019}
	- MSE based point forecast comparison 
	Amisano–Giacomini test for density forecast performance following  \cite{chin_bayesian_2019}

	
	
	\pagebreak
	\bibliography{20230505_m1_dsge}
	
	
	\part{Appendix}
	
	\section{Appendix A}
	
	\subsection{RBC}
	
	\includegraphics[scale=.3]{mod4_rbc_vanilla_epsilon_A_irfs_quantiles.png}\\
	
	\includegraphics[scale=.3]{mod4_rbc_vanilla_posterior_hist.png}

\end{document}