{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d2a458b-dfc1-42e0-bcf6-00e2bdf10137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR is existant under: C:\\Users\\LukasGrahl\\Documents\\GIT\\memoire1\\data\n"
     ]
    }
   ],
   "source": [
    "%run init_notebookspace.py\n",
    "from settings import DATA_DIR, MODEL_DIR, POST_EST_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39d5c8a0-1a05-493a-ac8c-c336fb6d7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from gEconpy.classes.model import gEconModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from scipy.stats import percentileofscore\n",
    "from copy import deepcopy\n",
    "\n",
    "from scipy.stats import gamma, norm, beta, uniform\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from filterpy.common import Saver\n",
    "\n",
    "\n",
    "from src.plotting import plot_dfs\n",
    "from src.process_data import load_data\n",
    "from src.filtering_sampling import set_up_kalman_filter, kalman_filter, sample_from_priors, solve_updated_mod, get_arr_pdf_from_dist\n",
    "from src.utils import printProgBar\n",
    "\n",
    "from config import plt_config\n",
    "plt.rcParams.update(plt_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9740e1-2261-4f63-88f1-4013724d694d",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "314c06bf-75e1-49ce-ab90-d445f9e9e890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occured 'is_test', file_dict may be incomplete\n"
     ]
    }
   ],
   "source": [
    "from config import fred_dict\n",
    "\n",
    "df = load_data('prepro_data.csv', DATA_DIR, fred_dict)\n",
    "\n",
    "# using real potential GDP instead of GDP\n",
    "df = df.drop(['Ix', 'Zx', 'Y', 'pi_s', 'w'], axis=1).rename(columns={'Y_p': 'Y', 'pi_c': 'pi'})\n",
    "\n",
    "# split train and test\n",
    "train = df[df['is_test'] == False].drop('is_test', axis=1).copy()\n",
    "test = df[df['is_test'] == True].drop('is_test', axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c112c-36b5-470c-b1b5-b4ef8c9a6200",
   "metadata": {},
   "source": [
    "load & solve model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c958cce-5607-44cb-941d-1a06f2df49b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from config import mod4_params, mod4_priors, mod5_params, mod5_priors\n",
    "mods = {'mod4_rbc_vanilla': {'params': mod4_params,\n",
    "                             'priors': mod4_priors},\n",
    "        'mod5_nk_vanilla': {'params': mod5_params,\n",
    "                            'priors': mod5_priors}}\n",
    "\n",
    "# load model\n",
    "for item in mods.keys():\n",
    "    mods[item]['mod'] = gEconModel(os.path.join(MODEL_DIR, f'{item}.gcn'), verbose=False)\n",
    "\n",
    "# solve model\n",
    "for item in mods.keys():\n",
    "    _, mods[item]['mod'] = solve_updated_mod(mods[item]['mod'], verbose=False)\n",
    "    assert _ == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0a9c14-6fbb-488d-923b-ee48011e3241",
   "metadata": {},
   "source": [
    "## Kalman Filter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876063f7-815b-4b26-ae1c-c0a2d3cc78a2",
   "metadata": {},
   "source": [
    "## drawing from priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4ab7a26-860d-4bc6-b0f1-987ed32f7bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mod_name = 'mod4_rbc_vanilla'\n",
    "mod = mods[mod_name]['mod']\n",
    "mod_params = mod.free_param_dict\n",
    "prior_dist = mods[mod_name]['priors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bd1643b-50df-469d-86b4-b3eb15f95452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
      "\n",
      "loop ran for 33.76643278598785 minutes\n",
      "\n",
      "solver rate 0.6822\n",
      "\n",
      "acceptance rate 0.9501612430372325\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10_000\n",
    "verbose = False\n",
    "infinity_mask_val = -100\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "n_param_dim_out = []\n",
    "n_dim_out = []\n",
    "\n",
    "ratio_list = np.array([[1,0,0,0,0]])\n",
    "loglike_list = [-100]\n",
    "counter_accp = 0\n",
    "counter_solved = 0\n",
    "\n",
    "# reset params\n",
    "mod.free_param_dict.update(mod_params)\n",
    "\n",
    "# get params, variables and shocks as lists\n",
    "shock_names = [x.base_name for x in mod.shocks]\n",
    "state_variables = [x.base_name for x in mod.variables]\n",
    "observed_vars = [\"Y\", 'C']\n",
    "model_params = list(mod.free_param_dict.keys())\n",
    "\n",
    "param_posterio_list = {item: [mod.free_param_dict[item]] for item in model_params if item in prior_dist.keys()}\n",
    "shock_prior_list = {item: [0] for item in shock_names}\n",
    "param_prior_list = {item: [] for item in model_params if item in prior_dist.keys()}\n",
    "new_prior_pdf = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, n_runs):\n",
    "    printProgBar(i, n_runs-1, prefix='Progress')\n",
    "    \n",
    "    # sample from priors\n",
    "    new_prior, shocks = sample_from_priors(prior_dist, mod_params, shock_names)\n",
    "    mod.free_param_dict.update(new_prior)\n",
    "    \n",
    "    is_solved, mod = solve_updated_mod(mod, verbose=verbose)\n",
    "    if not is_solved:\n",
    "        counter_solved += 0\n",
    "        continue\n",
    "    else:\n",
    "        counter_solved += 1\n",
    "            \n",
    "    # get Kalman filter initial condition\n",
    "    T, R = mod.T.values, mod.R.values\n",
    "    H, Z, T, R, QN, zs = set_up_kalman_filter(R=R, T=T, observed_data=train[observed_vars].values, observed_vars=observed_vars, \n",
    "                                              shock_names=shock_names, shocks_drawn_prior=shocks, state_variables=state_variables)\n",
    "       \n",
    "    # set up Kalman filter\n",
    "    kfilter = KalmanFilter(len(state_variables), len(observed_vars))\n",
    "    kfilter.F = T\n",
    "    kfilter.Q = QN\n",
    "    kfilter.H = Z\n",
    "    kfilter.R = H\n",
    "\n",
    "    # run Kalman filter\n",
    "    try:\n",
    "        saver = Saver(kfilter)\n",
    "        mu, cov, _, _ = kfilter.batch_filter(zs, saver=saver)\n",
    "        ll = saver.log_likelihood\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        counter_solved -= 1\n",
    "        continue\n",
    "        \n",
    "    # append shocks\n",
    "    for key in shock_prior_list.keys():\n",
    "        shock_prior_list[key].append(shocks[key])\n",
    "    \n",
    "    # append priors\n",
    "    for key in param_prior_list.keys():\n",
    "        param_prior_list[key].append(new_prior[key])\n",
    "    \n",
    "    # catch -math.inf values in log_likelihood\n",
    "    new_loglike = np.sum([infinity_mask_val if val == -math.inf else val for val in ll])\n",
    "    loglike_list.append(new_loglike)\n",
    "    \n",
    "    \n",
    "    #### MH #####\n",
    "    old_loglike = loglike_list[-2]\n",
    "    old_posterior = {item: vals[-1] for item, vals in param_prior_list.items()}\n",
    "    \n",
    "    # MH ratio\n",
    "    ratio = ((new_loglike * get_arr_pdf_from_dist(new_prior, prior_dist)) / (old_loglike * get_arr_pdf_from_dist(old_posterior, prior_dist))).mean()\n",
    "    ω = min([ratio, 1])\n",
    "    random = np.random.uniform(0, 1)\n",
    "        \n",
    "    \n",
    "    # merge draws prior into posterior \n",
    "    if random <= ω:\n",
    "        is_accepted = True\n",
    "        counter_accp += 1.\n",
    "        for key in new_prior.keys():\n",
    "            param_posterio_list[key].append(new_prior[key])\n",
    "            \n",
    "    # leave posterior unaltered and restart\n",
    "    else:\n",
    "        for key in new_prior.keys():\n",
    "            param_posterio_list[key].append(np.nan)\n",
    "        is_accepted = False\n",
    "        counter_accp += 0.\n",
    "        \n",
    "    # save output\n",
    "    new_prior_pdf.append(list(get_arr_pdf_from_dist(new_prior, prior_dist)))    \n",
    "    ratio_list = np.append(ratio_list, [[new_loglike, ratio, ω, random, random <= ω]], axis=0)\n",
    "        \n",
    "n_param_dim_out = np.array(n_param_dim_out)\n",
    "\n",
    "# print stats\n",
    "print('\\nloop ran for', (time.time() - start) / 60, 'minutes')\n",
    "print('\\nsolver rate', counter_solved/n_runs)\n",
    "print('\\nacceptance rate', counter_accp/counter_solved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3662b15b-8b73-4b66-830d-4c63261a7a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create xarray\n",
    "params = list(param_prior_list.keys())\n",
    "xarr = xr.Dataset(\n",
    "    {\n",
    "        'posterior': (['draw', 'parameter'],  np.array([param_posterio_list[item] for item in params]).transpose()[1:]),\n",
    "        'new_prior': (['draw', 'parameter'], np.array([param_prior_list[item] for item in params]).transpose()),\n",
    "        'new_prior_pdf': (['draw', 'parameter'],  np.array(new_prior_pdf)),\n",
    "        'log_like': (['draw'], loglike_list[1:]),\n",
    "        'n_runs': (['uni_dim'], [n_runs]), # number of solved models\n",
    "        'n_runs_acc': (['uni_dim'], [counter_accp]),\n",
    "        'solved_rate': (['uni_dim'], [counter_accp/counter_solved])       \n",
    "        \n",
    "    },\n",
    "    coords={\n",
    "        'draw': (['draw'], list(range(0, counter_solved))),\n",
    "        'parameter': (['parameter'], params),\n",
    "        'uni_dim': (['uni_dim'], [0])\n",
    "    }\n",
    ")\n",
    "\n",
    "# get percentiles\n",
    "arr_nan = deepcopy(xarr.posterior.values)\n",
    "\n",
    "for i in range(0, arr_nan.shape[1]):\n",
    "    arr_no_nan = arr_nan[~np.isnan(arr_nan[:, i]), i]\n",
    "    arr_nan[~np.isnan(arr_nan[:, i]), i] = [percentileofscore(arr_no_nan, x, 'rank') for x in arr_no_nan]\n",
    "xarr = xarr.assign({'posterior_percentiles': (['draw', 'parameter'], arr_nan)})\n",
    "\n",
    "quantile_ind = []\n",
    "for i in [25, 50, 75]:\n",
    "    arr = np.abs(xarr.posterior_percentiles[int(xarr.n_runs_acc/2):].mean(axis=1) - i)\n",
    "    quantile_ind.append(np.where(arr==np.min(arr))[0][0])\n",
    "    \n",
    "xarr = xarr.assign({\n",
    "    'posterior_q1': (['parameter'], xarr.sel(draw=quantile_ind[0]).posterior.values),\n",
    "    'posterior_q2': (['parameter'], xarr.sel(draw=quantile_ind[1]).posterior.values),\n",
    "    'posterior_q3': (['parameter'], xarr.sel(draw=quantile_ind[2]).posterior.values)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e1a98c6-b6ed-4312-9fcd-ec31c479a750",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LukasGrahl\\Documents\\GIT\\memoire1\\data\\posterior_est_out\\mod4_rbc_vanilla_20230327_1229.nc\n"
     ]
    }
   ],
   "source": [
    "# save output\n",
    "from datetime import datetime\n",
    "\n",
    "a = ''.join(str(datetime.now().date()).split('-'))\n",
    "b = ''.join((str(datetime.now().time()).split(':'))[:-1])\n",
    "timestamp = '_'.join([a, b])\n",
    "\n",
    "file_path = os.path.join(POST_EST_DIR, f'{mod_name}_{timestamp}.nc')\n",
    "print(file_path)\n",
    "if not os.path.exists(file_path):\n",
    "    xarr.to_netcdf(file_path)\n",
    "else:\n",
    "    print('File existst already')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a81d6-95ab-441a-9f18-757f29acf9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e16e3-a0b8-45dc-baf1-d75faacccb26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_memoire1",
   "language": "python",
   "name": "env_memoire1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
